{
  "c/c++": {
    "CWD-1118": [
      {
        "benign_code": {
          "context": "src/rgw/driver/posix/notify.h",
          "class": null,
          "func": null,
          "lines": [
            "    std::thread thrd;",
            "    std::mutex map_mutex;  // protects wd_callback_map and wd_remove_map",
            "    wd_callback_map_t wd_callback_map;",
            "\t      continue;",
            "",
            "\t    // Copy watch record data while holding the lock to avoid use-after-free",
            "\t    std::string watch_name;",
            "\t    void* watch_opaque;",
            "\t    {",
            "\t      std::lock_guard lock(map_mutex);",
            "\t      const auto& it = wd_callback_map.find(event->wd);",
            "\t      //std::cout << fmt::format(\"event! {}\", event->name) << std::endl;",
            "\t      if (it == wd_callback_map.end()) [[unlikely]] {",
            "\t\t/* non-destructive race, it happens */",
            "\t\tcontinue;",
            "\t      }",
            "\t      const auto& wr = it->second;",
            "\t      watch_name = wr.name;",
            "\t      watch_opaque = wr.opaque;",
            "\t    }",
            "\t    const auto& wr = it->second;",
            "",
            "\t    if (event->mask & IN_Q_OVERFLOW) [[unlikely]] {",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);",
            "\t      goto restart;",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);",
            "\t    }",
            "      } else {",
            "\tstd::lock_guard lock(map_mutex);",
            "\twd_callback_map.insert(wd_callback_map_t::value_type(wd, WatchRecord(wd, dname, opaque)));",
            "      int r{0};",
            "      std::lock_guard lock(map_mutex);",
            "      const auto& elt = wd_remove_map.find(dname);"
          ]
        },
        "vulnerable_code": {
          "context": "src/rgw/driver/posix/notify.h",
          "class": null,
          "func": null,
          "lines": [
            "\t    event = reinterpret_cast<struct inotify_event*>(ptr);",
            "\t    const auto& it = wd_callback_map.find(event->wd);",
            "\t    //std::cout << fmt::format(\"event! {}\", event->name) << std::endl;",
            "\t    if (it == wd_callback_map.end()) [[unlikely]] {",
            "\t      /* non-destructive race, it happens */",
            "\t      continue;",
            "",
            "\t    }",
            "\t    const auto& wr = it->second;",
            "",
            "\t      evec.emplace_back(Notifiable::Event(Notifiable::EventType::INVALIDATE, std::nullopt));",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);",
            "\t    if (evec.size() > 0) {",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/ceph/ceph/commit/71b1ec22dc1ddd105c4e0b8604e9ae375a0c3e59",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/ceph/ceph/pull/66186"
      },
      {
        "benign_code": {
          "context": "drivers/crypto/crypto_mtls_shim.c",
          "class": null,
          "func": null,
          "lines": [
            "",
            "static K_MUTEX_DEFINE(mtls_sessions_lock);",
            "",
            "#if defined(MBEDTLS_MEMORY_BUFFER_ALLOC_C)",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "\t\treturn -EINVAL;",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "\t\treturn -EINVAL;",
            "",
            "\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "",
            "\tfor (i = 0; i < CRYPTO_MAX_SESSION; i++) {",
            "\t\t\tmtls_sessions[i].in_use = true;",
            "\t\t\tk_mutex_unlock(&mtls_sessions_lock);",
            "\t\t\treturn i;",
            "",
            "\tk_mutex_unlock(&mtls_sessions_lock);",
            "\treturn -1;",
            "\tint ret;",
            "\tint ret = 0;",
            "",
            "",
            "\tmtls_sessions[ctx_idx].mode = mode;",
            "\tctx->drv_sessn_state = &mtls_sessions[ctx_idx];",
            "",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_free(aes_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t}",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_init(aes_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t}",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_ccm_free(ccm_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_gcm_free(gcm_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\treturn -EINVAL;",
            "\t\tret = -EINVAL;",
            "\t}",
            "",
            "\t/* Centralized cleanup of the session slot if an error occurred",
            "\t *  during configuration (ret != 0).",
            "\t */",
            "\tif (ret != 0) {",
            "\t\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\tk_mutex_unlock(&mtls_sessions_lock);",
            "\t}",
            "\treturn ret;",
            "\t}",
            "\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "\tmtls_session->in_use = false;",
            "\tk_mutex_unlock(&mtls_sessions_lock);",
            "",
            "\t}",
            "\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "\tmtls_session->in_use = false;",
            "\tk_mutex_unlock(&mtls_sessions_lock);",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "drivers/crypto/crypto_mtls_shim.c",
          "class": null,
          "func": null,
          "lines": [
            "\tif (ret) {",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "\tif (ret) {",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "",
            "\tapkt->pkt->out_len = apkt->pkt->in_len;",
            "\tapkt->pkt->out_len += ctx->mode_params.ccm_info.tag_len;",
            "",
            "\treturn 0;",
            "",
            "\tapkt->pkt->out_len = apkt->pkt->in_len;",
            "\tapkt->pkt->out_len += ctx->mode_params.gcm_info.tag_len;",
            "",
            "\treturn 0;",
            "\tint ctx_idx;",
            "\tint ret;",
            "\tint ret = 0;",
            "\t\t\tLOG_ERR(\"AES_ECB: failed at setkey (%d)\", ret);",
            "\t\t\tctx->ops.block_crypt_hndlr = NULL;",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_free(aes_ctx);",
            "\t\t\tLOG_ERR(\"AES_CBC: failed at setkey (%d)\", ret);",
            "\t\t\tctx->ops.cbc_crypt_hndlr = NULL;",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_init(aes_ctx);",
            "\t\t\tLOG_ERR(\"AES_CCM: failed at setkey (%d)\", ret);",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_ccm_free(ccm_ctx);",
            "\t\t\tLOG_ERR(\"AES_GCM: failed at setkey (%d)\", ret);",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_gcm_free(gcm_ctx);",
            "\t\tLOG_ERR(\"Unhandled mode\");",
            "\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\treturn -EINVAL;",
            "\t\tret = -EINVAL;",
            "",
            "\tmtls_sessions[ctx_idx].mode = mode;",
            "\tctx->drv_sessn_state = &mtls_sessions[ctx_idx];",
            "",
            "\t/* Centralized cleanup of the session slot if an error occurred"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/zephyrproject-rtos/zephyr/commit/0719c9eca2b106004598f8a2b2f5f8ba77866fc4",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/zephyrproject-rtos/zephyr/pull/98710"
      },
      {
        "benign_code": {
          "context": "src/v/cloud_io/access_time_tracker.h",
          "class": null,
          "func": null,
          "lines": [
            "    chunked_vector<file_list_item> lru_entries() const;",
            "    ss::future<chunked_vector<file_list_item>> lru_entries();",
            "",
            "    ss::semaphore _table_lock{1};",
            "    mutable ss::semaphore _table_lock{1};",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "src/v/cloud_io/access_time_tracker.h",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    chunked_vector<file_list_item> lru_entries() const;",
            "    ss::future<chunked_vector<file_list_item>> lru_entries();",
            "    // When releasing lock, drain _pending_upserts.",
            "    ss::semaphore _table_lock{1};",
            "    mutable ss::semaphore _table_lock{1};"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/redpanda-data/redpanda/commit/9b45ae8251821b87d4505eb381be51777a39ac85",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/redpanda-data/redpanda/pull/28412"
      },
      {
        "benign_code": {
          "context": "src/utils/resource_object_pool.h",
          "class": null,
          "func": null,
          "lines": [
            "    using ConstructFuncType = std::function<std::shared_ptr<T>()>;",
            "    static constexpr uint64_t kSubPoolCount = 16;",
            "",
            "        : allocator_(allocator), pool_size_(init_size) {",
            "        : allocator_(allocator), init_size_(init_size) {",
            "        this->constructor_ = [=]() -> std::shared_ptr<T> { return std::make_shared<T>(args...); };",
            "        this->resize(pool_size_);",
            "        for (int i = 0; i < kSubPoolCount; ++i) {",
            "            pool_[i] = std::make_unique<Deque<std::shared_ptr<T>>>(this->allocator_);",
            "        }",
            "        this->fill(init_size_);",
            "    }",
            "                pool_->pop_front();",
            "            for (int i = 0; i < kSubPoolCount; ++i) {",
            "                pool_[i].reset();",
            "            }",
            "            return this->constructor_();",
            "        while (true) {",
            "            for (int i = 0; i < kSubPoolCount; ++i) {",
            "                if (sub_pool_mutexes_[i].try_lock()) {",
            "                    if (pool_[i]->empty()) {",
            "                        sub_pool_mutexes_[i].unlock();",
            "                        return this->constructor_();",
            "                    }",
            "                    std::shared_ptr<T> obj = pool_[i]->front();",
            "                    pool_[i]->pop_front();",
            "                    sub_pool_mutexes_[i].unlock();",
            "                    obj->Reset();",
            "                    return obj;",
            "                }",
            "            }",
            "        }",
            "        return this->pool_size_;",
            "        while (true) {",
            "            for (int i = 0; i < kSubPoolCount; ++i) {",
            "                if (sub_pool_mutexes_[i].try_lock()) {",
            "                    pool_[i]->emplace_back(obj);",
            "                    sub_pool_mutexes_[i].unlock();",
            "                    return;",
            "                }",
            "            }",
            "        }",
            "    }",
            "            ++count;",
            "    fill(uint64_t size) {",
            "        for (uint64_t i = 0; i < size; ++i) {",
            "            auto sub_pool_idx = i % kSubPoolCount;",
            "            pool_[sub_pool_idx]->emplace_back(this->constructor_());",
            "        }",
            "    std::atomic<uint64_t> pool_size_;",
            "private:",
            "    std::unique_ptr<Deque<std::shared_ptr<T>>> pool_[kSubPoolCount];",
            "    std::mutex sub_pool_mutexes_[kSubPoolCount];",
            "    uint64_t init_size_{0};",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "src/utils/resource_object_pool.h",
          "class": null,
          "func": null,
          "lines": [
            "    explicit ResourceObjectPool(uint64_t init_size, Allocator* allocator, Args... args)",
            "        : allocator_(allocator), pool_size_(init_size) {",
            "        : allocator_(allocator), init_size_(init_size) {",
            "        }",
            "        this->pool_ = std::make_unique<Deque<std::shared_ptr<T>>>(this->allocator_);",
            "        this->resize(pool_size_);",
            "        for (int i = 0; i < kSubPoolCount; ++i) {",
            "        if (owned_allocator_ != nullptr) {",
            "            this->pool_.reset();",
            "        }",
            "    }",
            "",
            "    void",
            "    SetConstructor(ConstructFuncType func) {",
            "        this->constructor_ = func;",
            "        {",
            "            std::lock_guard<std::mutex> lock(mutex_);",
            "            while (not pool_->empty()) {",
            "                pool_->pop_front();",
            "            for (int i = 0; i < kSubPoolCount; ++i) {",
            "        }",
            "        this->resize(pool_size_);",
            "    }",
            "    TakeOne() {",
            "        std::unique_lock<std::mutex> lock(mutex_);",
            "        if (pool_->empty()) {",
            "            lock.unlock();",
            "            return this->constructor_();",
            "        while (true) {",
            "        }",
            "        std::shared_ptr<T> obj = pool_->front();",
            "        pool_->pop_front();",
            "        pool_size_--;",
            "        lock.unlock();",
            "        obj->Reset();",
            "        return obj;",
            "    }",
            "    ReturnOne(std::shared_ptr<T>& obj) {",
            "        std::lock_guard<std::mutex> lock(mutex_);",
            "        pool_->emplace_back(obj);",
            "        pool_size_++;",
            "    }",
            "",
            "    [[nodiscard]] inline uint64_t",
            "    GetSize() const {",
            "        return this->pool_size_;",
            "        while (true) {",
            "    inline void",
            "    resize(uint64_t size) {",
            "        std::lock_guard<std::mutex> lock(mutex_);",
            "        int count = size - pool_->size();",
            "        while (count > 0) {",
            "            pool_->emplace_back(this->constructor_());",
            "            --count;",
            "        }",
            "        while (count < 0) {",
            "            pool_->pop_front();",
            "            ++count;",
            "    fill(uint64_t size) {",
            "",
            "    std::unique_ptr<Deque<std::shared_ptr<T>>> pool_{nullptr};",
            "    std::atomic<uint64_t> pool_size_;",
            "private:",
            "    ConstructFuncType constructor_{nullptr};",
            "    std::mutex mutex_;",
            "    Allocator* allocator_{nullptr};",
            "",
            "private:",
            "    std::shared_ptr<Allocator> owned_allocator_{nullptr};",
            "};",
            "",
            "}  // namespace vsag"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/antgroup/vsag/commit/b5c519c7b1786b6c99110f1eb7a57e77770fec99",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/antgroup/vsag/pull/1219"
      },
      {
        "benign_code": {
          "context": "ydb/core/tx/columnshard/operations/manager.cpp",
          "class": null,
          "func": null,
          "lines": [
            "        NActors::TLogContextBuilder::Build(NKikimrServices::TX_COLUMNSHARD_TX)(\"commit_tx_id\", txId)(\"commit_lock_id\", lock.GetLockId()));",
            "    if (lock.GetWriteOperations().size() > 0) {",
            "        AFL_VERIFY(!lock.IsBroken())(\"error\", \"the tx has writes, it is broken, and we are committing it\")(\"writes_count\", lock.GetWriteOperations().size());",
            "    }",
            "    TVector<TWriteOperation::TPtr> commited;"
          ]
        },
        "vulnerable_code": {
          "context": "ydb/core/tx/columnshard/operations/manager.cpp",
          "class": null,
          "func": null,
          "lines": []
        },
        "source": "github",
        "commit_url": "https://github.com/ydb-platform/ydb/commit/80a142b406ef5ab8208711551cdf50a26381f041",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/ydb-platform/ydb/pull/28651"
      },
      {
        "benign_code": {
          "context": "src/llmq/signing_shares.cpp",
          "class": null,
          "func": null,
          "lines": [
            "    std::vector<CBLSId> idsForRecovery;",
            "    std::shared_ptr<CRecoveredSig> singleMemberRecoveredSig;",
            "    {",
            "            auto rs = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "            singleMemberRecoveredSig = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "                                                      recoveredSig);",
            "",
            "    // Handle single-member quorum case after releasing the lock",
            "    if (singleMemberRecoveredSig) {",
            "        sigman.ProcessRecoveredSig(singleMemberRecoveredSig, m_peerman);",
            "        return; // end of single-quorum processing",
            "    }",
            "",
            "    // now recover it"
          ]
        },
        "vulnerable_code": {
          "context": "src/llmq/signing_shares.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "            auto rs = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "            singleMemberRecoveredSig = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "                                                      recoveredSig);",
            "            sigman.ProcessRecoveredSig(rs, m_peerman);",
            "            return; // end of single-quorum processing",
            "        }"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/dashpay/dash/commit/b440b1676e16318f0bb7badd85cfb18a82bbbbec",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/dashpay/dash/pull/6954"
      },
      {
        "benign_code": {
          "context": "core/templates/rid_owner.h",
          "class": null,
          "func": null,
          "lines": [
            "\t_FORCE_INLINE_ bool owns(const RID &p_rid) const {",
            "\t\tif (p_rid == RID()) {",
            "\t\t\treturn false;",
            "\t\t}",
            "",
            "\t\tif constexpr (THREAD_SAFE) {",
            "\t\t\tmutex.lock();",
            "\t\t\tSYNC_ACQUIRE;",
            "\t\t}",
            "\t\t\t}",
            "\t\tuint32_t ma;",
            "\t\tif constexpr (THREAD_SAFE) {",
            "\t\t\tma = ((std::atomic<uint32_t> *)&max_alloc)->load(std::memory_order_relaxed);",
            "\t\t} else {",
            "\t\t\tma = max_alloc;",
            "\t\t}",
            "",
            "\t\tif (unlikely(idx >= ma)) {",
            "\t\t\treturn false;",
            "\t\tbool owned = (chunks[idx_chunk][idx_element].validator & 0x7FFFFFFF) == validator;",
            "\t\tif constexpr (THREAD_SAFE) {",
            "#ifdef TSAN_ENABLED",
            "\t\t\t__tsan_acquire(&chunks[idx_chunk]); // We know not a race in practice.",
            "\t\t\t__tsan_acquire(&chunks[idx_chunk][idx_element]); // We know not a race in practice.",
            "#endif",
            "\t\t}",
            "",
            "\t\tChunk &c = chunks[idx_chunk][idx_element];",
            "",
            "\t\t\tmutex.unlock();",
            "#ifdef TSAN_ENABLED",
            "\t\t\t__tsan_release(&chunks[idx_chunk]);",
            "\t\t\t__tsan_release(&chunks[idx_chunk][idx_element]);",
            "\t\t\t__tsan_acquire(&c.validator); // We know not a race in practice.",
            "#endif",
            "\t\t}",
            "",
            "\t\tbool owned = (c.validator & 0x7FFFFFFF) == validator;",
            "",
            "\t\tif constexpr (THREAD_SAFE) {",
            "#ifdef TSAN_ENABLED",
            "\t\t\t__tsan_release(&c.validator);",
            "#endif",
            "\t\t}"
          ]
        },
        "vulnerable_code": {
          "context": "core/templates/rid_owner.h",
          "class": null,
          "func": null,
          "lines": [
            "\t\tif constexpr (THREAD_SAFE) {",
            "\t\t\tmutex.lock();",
            "\t\t\tSYNC_ACQUIRE;",
            "\t\tuint32_t idx = uint32_t(id & 0xFFFFFFFF);",
            "\t\tif (unlikely(idx >= max_alloc)) {",
            "\t\t\tif constexpr (THREAD_SAFE) {",
            "\t\t\t\tmutex.unlock();",
            "\t\t\t}",
            "\t\tuint32_t ma;",
            "",
            "\t\tbool owned = (chunks[idx_chunk][idx_element].validator & 0x7FFFFFFF) == validator;",
            "\t\tif constexpr (THREAD_SAFE) {",
            "\t\tif constexpr (THREAD_SAFE) {",
            "\t\t\tmutex.unlock();",
            "#ifdef TSAN_ENABLED"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/godotengine/godot/commit/c9ef31360f6f954f20997c16be74e90c930f33a5",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/godotengine/godot/pull/112657"
      },
      {
        "benign_code": {
          "context": "third_party/amd/lib/TritonAMDGPUToLLVM/LoadStoreOpToLLVM.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    bool needLdsStaging = !tensorTy && !opResult.use_empty();",
            "    std::optional<Value> atomicSharedMemBase =",
            "        op->hasAttr(\"allocation.offset\")",
            "        op->hasAttr(\"allocation.offset\") && needLdsStaging",
            "            ? std::optional<Value>(getSharedMemoryBase("
          ]
        },
        "vulnerable_code": {
          "context": "third_party/amd/lib/TritonAMDGPUToLLVM/LoadStoreOpToLLVM.cpp",
          "class": null,
          "func": null,
          "lines": [
            "    std::optional<Value> atomicSharedMemBase =",
            "        op->hasAttr(\"allocation.offset\")",
            "        op->hasAttr(\"allocation.offset\") && needLdsStaging"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/triton-lang/triton/commit/1070cd530573098dc8375422a1f1918d3815af3f",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/triton-lang/triton/pull/8633"
      },
      {
        "benign_code": {
          "context": "google/cloud/bigtable/internal/query_plan_test.cc",
          "class": null,
          "func": null,
          "lines": [
            "  std::array<StatusOr<PrepareQueryResponse>, kNumThreads> data_responses;",
            "  std::array<promise<StatusOr<PrepareQueryResponse>>, kNumThreads>",
            "      data_responses;",
            "  std::array<future<StatusOr<PrepareQueryResponse>>, kNumThreads>",
            "      data_response_futures;",
            "  for (auto i = 0; i < kNumThreads; ++i) {",
            "    data_responses[i] = promise<StatusOr<PrepareQueryResponse>>();",
            "    data_response_futures[i] = data_responses[i].get_future();",
            "  }",
            "",
            "                    query_plan](StatusOr<PrepareQueryResponse>* thread_data) {",
            "  auto thread_fn = [barrier, query_plan](",
            "                       promise<StatusOr<PrepareQueryResponse>>* thread_data) {",
            "    barrier->Block();",
            "    *thread_data = query_plan->response();",
            "    thread_data->set_value(query_plan->response());",
            "  };",
            "",
            "  for (auto& f : data_response_futures) {",
            "    auto r = f.get();",
            "    ASSERT_STATUS_OK(r);",
            "    EXPECT_EQ(r->prepared_query(), \"refreshed-query-plan\");",
            "  }",
            "",
            "  for (auto& t : threads) {",
            "  std::array<StatusOr<PrepareQueryResponse>, kNumThreads> data_responses;",
            "  std::array<promise<StatusOr<PrepareQueryResponse>>, kNumThreads>",
            "      data_responses;",
            "  std::array<future<StatusOr<PrepareQueryResponse>>, kNumThreads>",
            "      data_response_futures;",
            "  for (auto i = 0; i < kNumThreads; ++i) {",
            "    data_responses[i] = promise<StatusOr<PrepareQueryResponse>>();",
            "    data_response_futures[i] = data_responses[i].get_future();",
            "  }",
            "",
            "                    query_plan](StatusOr<PrepareQueryResponse>* thread_data) {",
            "  auto thread_fn = [barrier, query_plan](",
            "                       promise<StatusOr<PrepareQueryResponse>>* thread_data) {",
            "    barrier->Block();",
            "    while (!thread_data->ok()) {",
            "    auto response = query_plan->response();",
            "    while (!response.ok()) {",
            "      std::this_thread::yield();",
            "      *thread_data = query_plan->response();",
            "      response = query_plan->response();",
            "    }",
            "    thread_data->set_value(response);",
            "  };",
            "",
            "  for (auto& f : data_response_futures) {",
            "    auto r = f.get();",
            "    ASSERT_STATUS_OK(r);",
            "    EXPECT_EQ(r->prepared_query(), \"refreshed-query-plan\");",
            "  }",
            "",
            "  for (auto& t : threads) {",
            "  std::array<StatusOr<PrepareQueryResponse>, kNumThreads> data_responses;",
            "  std::array<promise<StatusOr<PrepareQueryResponse>>, kNumThreads>",
            "      data_responses;",
            "  std::array<future<StatusOr<PrepareQueryResponse>>, kNumThreads>",
            "      data_response_futures;",
            "  for (auto i = 0; i < kNumThreads; ++i) {",
            "    data_responses[i] = promise<StatusOr<PrepareQueryResponse>>();",
            "    data_response_futures[i] = data_responses[i].get_future();",
            "  }",
            "",
            "                    query_plan](StatusOr<PrepareQueryResponse>* thread_data) {",
            "  auto thread_fn = [barrier, query_plan](",
            "                       promise<StatusOr<PrepareQueryResponse>>* thread_data) {",
            "    barrier->Block();",
            "    *thread_data = query_plan->response();",
            "    thread_data->set_value(query_plan->response());",
            "  };",
            "",
            "  for (auto& f : data_response_futures) {",
            "    auto r = f.get();",
            "    ASSERT_STATUS_OK(r);",
            "    EXPECT_EQ(r->prepared_query(), \"refreshed-query-plan\");",
            "  }",
            "",
            "  for (auto& t : threads) {"
          ]
        },
        "vulnerable_code": {
          "context": "google/cloud/bigtable/internal/query_plan_test.cc",
          "class": null,
          "func": null,
          "lines": [
            "  std::vector<std::thread> threads(kNumThreads);",
            "  std::array<StatusOr<PrepareQueryResponse>, kNumThreads> data_responses;",
            "  std::array<promise<StatusOr<PrepareQueryResponse>>, kNumThreads>",
            "  auto barrier = std::make_shared<absl::Barrier>(kNumThreads + 1);",
            "  auto thread_fn = [barrier,",
            "                    query_plan](StatusOr<PrepareQueryResponse>* thread_data) {",
            "  auto thread_fn = [barrier, query_plan](",
            "    barrier->Block();",
            "    *thread_data = query_plan->response();",
            "    thread_data->set_value(query_plan->response());",
            "  for (int i = 0; i < kNumThreads; ++i) {",
            "    data_responses[i] = StatusOr<PrepareQueryResponse>(",
            "        Status(StatusCode::kNotFound, \"not found\"));",
            "    threads.emplace_back(thread_fn, &(data_responses[i]));",
            "  }",
            "",
            "  EXPECT_EQ(calls_to_refresh_fn, 1);",
            "  for (auto const& r : data_responses) {",
            "    ASSERT_STATUS_OK(r);",
            "    EXPECT_EQ(r->prepared_query(), \"refreshed-query-plan\");",
            "  }",
            "",
            "  std::vector<std::thread> threads(kNumThreads);",
            "  std::array<StatusOr<PrepareQueryResponse>, kNumThreads> data_responses;",
            "  std::array<promise<StatusOr<PrepareQueryResponse>>, kNumThreads>",
            "  auto barrier = std::make_shared<absl::Barrier>(kNumThreads + 1);",
            "  auto thread_fn = [barrier,",
            "                    query_plan](StatusOr<PrepareQueryResponse>* thread_data) {",
            "  auto thread_fn = [barrier, query_plan](",
            "    barrier->Block();",
            "    *thread_data = query_plan->response();",
            "    while (!thread_data->ok()) {",
            "    auto response = query_plan->response();",
            "      std::this_thread::yield();",
            "      *thread_data = query_plan->response();",
            "      response = query_plan->response();",
            "  for (int i = 0; i < kNumThreads; ++i) {",
            "    data_responses[i] = StatusOr<PrepareQueryResponse>(",
            "        Status(StatusCode::kNotFound, \"not found\"));",
            "    threads.emplace_back(thread_fn, &(data_responses[i]));",
            "  EXPECT_EQ(calls_to_refresh_fn, 4);",
            "  for (auto const& r : data_responses) {",
            "    ASSERT_STATUS_OK(r);",
            "    EXPECT_EQ(r->prepared_query(), \"refreshed-query-plan\");",
            "  }",
            "",
            "  std::vector<std::thread> threads(kNumThreads);",
            "  std::array<StatusOr<PrepareQueryResponse>, kNumThreads> data_responses;",
            "  std::array<promise<StatusOr<PrepareQueryResponse>>, kNumThreads>",
            "  auto barrier = std::make_shared<absl::Barrier>(kNumThreads + 1);",
            "  auto thread_fn = [barrier,",
            "                    query_plan](StatusOr<PrepareQueryResponse>* thread_data) {",
            "  auto thread_fn = [barrier, query_plan](",
            "    barrier->Block();",
            "    *thread_data = query_plan->response();",
            "    thread_data->set_value(query_plan->response());",
            "  for (int i = 0; i < kNumThreads; ++i) {",
            "    data_responses[i] = StatusOr<PrepareQueryResponse>(",
            "        Status(StatusCode::kNotFound, \"not found\"));",
            "    threads.emplace_back(thread_fn, &(data_responses[i]));",
            "  }",
            "",
            "  EXPECT_EQ(calls_to_refresh_fn, kNumFailingThreads + 1);",
            "  for (auto const& r : data_responses) {",
            "    ASSERT_STATUS_OK(r);",
            "    EXPECT_EQ(r->prepared_query(), \"refreshed-query-plan\");",
            "  }",
            ""
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/googleapis/google-cloud-cpp/commit/361f9b8120d0d3b1ddd9eb37fb6450cd2ec6e649",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/googleapis/google-cloud-cpp/pull/15741"
      },
      {
        "benign_code": {
          "context": "Modules/_lzmamodule.c",
          "class": null,
          "func": null,
          "lines": [
            "    PyThread_type_lock lock;",
            "    PyMutex mutex;",
            "} Compressor;",
            "    PyThread_type_lock lock;",
            "    PyMutex mutex;",
            "} Decompressor;",
            "    ACQUIRE_LOCK(self);",
            "    PyMutex_Lock(&self->mutex);",
            "    if (self->flushed) {",
            "    RELEASE_LOCK(self);",
            "    PyMutex_Unlock(&self->mutex);",
            "    return result;",
            "    ACQUIRE_LOCK(self);",
            "    PyMutex_Lock(&self->mutex);",
            "    if (self->flushed) {",
            "    RELEASE_LOCK(self);",
            "    PyMutex_Unlock(&self->mutex);",
            "    return result;",
            "    }",
            "    self->mutex = (PyMutex){0};",
            "",
            "    Compressor *self = Compressor_CAST(op);",
            "    assert(!PyMutex_IsLocked(&self->mutex));",
            "    lzma_end(&self->lzs);",
            "    ACQUIRE_LOCK(self);",
            "    PyMutex_Lock(&self->mutex);",
            "    if (self->eof)",
            "    RELEASE_LOCK(self);",
            "    PyMutex_Unlock(&self->mutex);",
            "    return result;",
            "    }",
            "    self->mutex = (PyMutex){0};",
            "",
            "    Decompressor *self = Decompressor_CAST(op);",
            "    assert(!PyMutex_IsLocked(&self->mutex));",
            "",
            "    if(self->input_buffer != NULL)"
          ]
        },
        "vulnerable_code": {
          "context": "Modules/_lzmamodule.c",
          "class": null,
          "func": null,
          "lines": [
            "",
            "#define ACQUIRE_LOCK(obj) do { \\",
            "    if (!PyThread_acquire_lock((obj)->lock, 0)) { \\",
            "        Py_BEGIN_ALLOW_THREADS \\",
            "        PyThread_acquire_lock((obj)->lock, 1); \\",
            "        Py_END_ALLOW_THREADS \\",
            "    } } while (0)",
            "#define RELEASE_LOCK(obj) PyThread_release_lock((obj)->lock)",
            "",
            "    int flushed;",
            "    PyThread_type_lock lock;",
            "    PyMutex mutex;",
            "    size_t input_buffer_size;",
            "    PyThread_type_lock lock;",
            "    PyMutex mutex;",
            "",
            "    ACQUIRE_LOCK(self);",
            "    PyMutex_Lock(&self->mutex);",
            "    }",
            "    RELEASE_LOCK(self);",
            "    PyMutex_Unlock(&self->mutex);",
            "",
            "    ACQUIRE_LOCK(self);",
            "    PyMutex_Lock(&self->mutex);",
            "    }",
            "    RELEASE_LOCK(self);",
            "    PyMutex_Unlock(&self->mutex);",
            "",
            "    self->lock = PyThread_allocate_lock();",
            "    if (self->lock == NULL) {",
            "        Py_DECREF(self);",
            "        PyErr_SetString(PyExc_MemoryError, \"Unable to allocate lock\");",
            "        return NULL;",
            "    }",
            "    self->mutex = (PyMutex){0};",
            "    lzma_end(&self->lzs);",
            "    if (self->lock != NULL) {",
            "        PyThread_free_lock(self->lock);",
            "    }",
            "    PyTypeObject *tp = Py_TYPE(self);",
            "",
            "    ACQUIRE_LOCK(self);",
            "    PyMutex_Lock(&self->mutex);",
            "        result = decompress(self, data->buf, data->len, max_length);",
            "    RELEASE_LOCK(self);",
            "    PyMutex_Unlock(&self->mutex);",
            "",
            "    self->lock = PyThread_allocate_lock();",
            "    if (self->lock == NULL) {",
            "        Py_DECREF(self);",
            "        PyErr_SetString(PyExc_MemoryError, \"Unable to allocate lock\");",
            "        return NULL;",
            "    }",
            "    self->mutex = (PyMutex){0};",
            "    Py_CLEAR(self->unused_data);",
            "    if (self->lock != NULL) {",
            "        PyThread_free_lock(self->lock);",
            "    }",
            "    PyTypeObject *tp = Py_TYPE(self);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/python/cpython/commit/c13b59204af562bfb022eb8f6a5c03eb82659531",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/python/cpython/pull/140711"
      },
      {
        "benign_code": {
          "context": "src/v/cluster/partition_balancer_backend.cc",
          "class": null,
          "func": null,
          "lines": [
            "          [this] {",
            "              if (_tick_in_progress) {",
            "                  vlog(",
            "                    clusterlog.debug,",
            "                    \"skipping tick, tick already in progress\");",
            "                  return ss::now();",
            "              }",
            "",
            "              _tick_in_progress = ss::abort_source{};",
            "              return do_tick().finally([this] {"
          ]
        },
        "vulnerable_code": {
          "context": "src/v/cluster/partition_balancer_backend.cc",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    _tick_in_progress = ss::abort_source{};",
            "",
            "    const bool force_refresh_this_tick"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/redpanda-data/redpanda/commit/7905c386be30a5b945dcd2092b37a8580ea82507",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/redpanda-data/redpanda/pull/28460"
      },
      {
        "benign_code": {
          "context": "ydb/library/yql/providers/common/http_gateway/yql_http_gateway.h",
          "class": null,
          "func": null,
          "lines": [
            "        TCountedContent(TString&& data, const std::shared_ptr<std::atomic_size_t>& counter, const ::NMonitoring::TDynamicCounters::TCounterPtr& inflightCounter);",
            "        TCountedContent(TString&& data, const std::shared_ptr<std::atomic_size_t>& counter, const ::NMonitoring::TDynamicCounters::TCounterPtr& inflightCounter, std::weak_ptr<CURLM> handle, size_t threshold);",
            "        ~TCountedContent();",
            "    private:",
            "        void BeforeRelease();",
            "",
            "        const std::shared_ptr<std::atomic_size_t> Counter;",
            "        const ::NMonitoring::TDynamicCounters::TCounterPtr InflightCounter;",
            "        std::weak_ptr<CURLM> Handle;",
            "        const size_t Threshold;",
            "    };"
          ]
        },
        "vulnerable_code": {
          "context": "ydb/library/yql/providers/common/http_gateway/yql_http_gateway.h",
          "class": null,
          "func": null,
          "lines": [
            "    public:",
            "        TCountedContent(TString&& data, const std::shared_ptr<std::atomic_size_t>& counter, const ::NMonitoring::TDynamicCounters::TCounterPtr& inflightCounter);",
            "        TCountedContent(TString&& data, const std::shared_ptr<std::atomic_size_t>& counter, const ::NMonitoring::TDynamicCounters::TCounterPtr& inflightCounter, std::weak_ptr<CURLM> handle, size_t threshold);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/ydb-platform/ydb/commit/c734e91d988d1ff6b16883836dafee23d535a6af",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/ydb-platform/ydb/pull/28390"
      },
      {
        "benign_code": {
          "context": "lldb/source/Breakpoint/BreakpointLocationCollection.cpp",
          "class": null,
          "func": null,
          "lines": [
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "  BreakpointLocationSP old_bp_loc =",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "  collection::iterator pos = GetIDPairIterator(bp_id, bp_loc_id); // Predicate",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "  BreakpointLocationSP stop_sp;",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "  BreakpointLocationSP stop_sp;",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "  collection::iterator pos, begin = m_break_loc_collection.begin(),",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "  collection::const_iterator pos, begin = m_break_loc_collection.begin(),",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "  collection::iterator pos, begin = m_break_loc_collection.begin(),",
            "      std::lock_guard<std::mutex> rhs_guard(rhs.m_collection_mutex, std::adopt_lock);",
            "      std::lock_guard<std::recursive_mutex> lhs_guard(m_collection_mutex,",
            "                                                      std::adopt_lock);",
            "      std::lock_guard<std::recursive_mutex> rhs_guard(rhs.m_collection_mutex,",
            "                                                      std::adopt_lock);",
            "      m_break_loc_collection = rhs.m_break_loc_collection;"
          ]
        },
        "vulnerable_code": {
          "context": "lldb/source/Breakpoint/BreakpointLocationCollection.cpp",
          "class": null,
          "func": null,
          "lines": [
            "void BreakpointLocationCollection::Add(const BreakpointLocationSP &bp_loc) {",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "                                          lldb::break_id_t bp_loc_id) {",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "BreakpointLocationSP BreakpointLocationCollection::GetByIndex(size_t i) {",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "BreakpointLocationCollection::GetByIndex(size_t i) const {",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "bool BreakpointLocationCollection::ValidForThisThread(Thread &thread) {",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "bool BreakpointLocationCollection::IsInternal() const {",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "    Stream *s, lldb::DescriptionLevel level) {",
            "  std::lock_guard<std::mutex> guard(m_collection_mutex);",
            "  std::lock_guard<std::recursive_mutex> guard(m_collection_mutex);",
            "      std::lock(m_collection_mutex, rhs.m_collection_mutex);",
            "      std::lock_guard<std::mutex> lhs_guard(m_collection_mutex, std::adopt_lock);",
            "      std::lock_guard<std::mutex> rhs_guard(rhs.m_collection_mutex, std::adopt_lock);",
            "      std::lock_guard<std::recursive_mutex> lhs_guard(m_collection_mutex,"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/llvm/llvm-project/commit/97d50b5c8a7b9269953888d2fcf21e4d4a16dc25",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/llvm/llvm-project/pull/166876"
      },
      {
        "benign_code": {
          "context": "src/net_processing.cpp",
          "class": null,
          "func": null,
          "lines": [
            "    // Also reset pindexLastCommonBlock after a snapshot was loaded, so that blocks after the snapshot will be prioritised for download.",
            "    // Determine the forking point between the peer's chain and our chain:",
            "    // pindexLastCommonBlock is required to be an ancestor of pindexBestKnownBlock, and will be used as a starting point.",
            "    // It is being set to the fork point between the peer's best known block and the current tip, unless it is already set to",
            "    // an ancestor with more work than the fork point.",
            "    auto fork_point = LastCommonAncestor(state->pindexBestKnownBlock, m_chainman.ActiveTip());",
            "    if (state->pindexLastCommonBlock == nullptr ||",
            "        state->pindexLastCommonBlock = m_chainman.ActiveChain()[std::min(state->pindexBestKnownBlock->nHeight, m_chainman.ActiveChain().Height())];",
            "        fork_point->nChainWork > state->pindexLastCommonBlock->nChainWork ||",
            "        state->pindexBestKnownBlock->GetAncestor(state->pindexLastCommonBlock->nHeight) != state->pindexLastCommonBlock) {",
            "        state->pindexLastCommonBlock = fork_point;",
            "    }"
          ]
        },
        "vulnerable_code": {
          "context": "src/net_processing.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    // Bootstrap quickly by guessing a parent of our best tip is the forking point.",
            "    // Guessing wrong in either direction is not a problem.",
            "    // Also reset pindexLastCommonBlock after a snapshot was loaded, so that blocks after the snapshot will be prioritised for download.",
            "    // Determine the forking point between the peer's chain and our chain:",
            "    if (state->pindexLastCommonBlock == nullptr ||",
            "        (snap_base && state->pindexLastCommonBlock->nHeight < snap_base->nHeight)) {",
            "        state->pindexLastCommonBlock = m_chainman.ActiveChain()[std::min(state->pindexBestKnownBlock->nHeight, m_chainman.ActiveChain().Height())];",
            "        fork_point->nChainWork > state->pindexLastCommonBlock->nChainWork ||",
            "    }",
            "",
            "    // If the peer reorganized, our previous pindexLastCommonBlock may not be an ancestor",
            "    // of its current tip anymore. Go back enough to fix that.",
            "    state->pindexLastCommonBlock = LastCommonAncestor(state->pindexLastCommonBlock, state->pindexBestKnownBlock);",
            "    if (state->pindexLastCommonBlock == state->pindexBestKnownBlock)"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/bitcoin/bitcoin/commit/1fe851a4781a6af6264f1ba9d93c3281b59a6cac",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/bitcoin/bitcoin/pull/32180"
      },
      {
        "benign_code": {
          "context": "gst/nnstreamer/tensor_repo/tensor_repo.c",
          "class": null,
          "func": null,
          "lines": [
            "  g_mutex_init (&data->lock);",
            "",
            "  g_mutex_lock (&data->lock);",
            "  data->eos = FALSE;",
            "  data->buffer = NULL;",
            "  data->sink_changed = FALSE;",
            "  data->pushed = FALSE;",
            "  g_mutex_unlock (&data->lock);",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "gst/nnstreamer/tensor_repo/tensor_repo.c",
          "class": null,
          "func": null,
          "lines": [
            "",
            "  data->eos = FALSE;",
            "  data->buffer = NULL;",
            "  g_cond_init (&data->cond_push);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/nnstreamer/nnstreamer/commit/3ab58006f9a8ed7fc0940dfcfd4fa3d1a092a9f5",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/nnstreamer/nnstreamer/pull/2709"
      }
    ],
    "CWD-1120": [
      {
        "benign_code": {
          "context": "drivers/crc/crc_renesas_ra.c",
          "class": null,
          "func": null,
          "lines": [
            "\tif (err != FSP_SUCCESS) {",
            "\t\tdata->flag_crc_updated = false;",
            "\t\tctx->state = CRC_STATE_IDLE;",
            "\t\tif (err != FSP_SUCCESS) {",
            "\t\t\tdata->flag_crc_updated = false;",
            "\t\t\tctx->state = CRC_STATE_IDLE;",
            "\t\tif (err != FSP_SUCCESS) {",
            "\t\t\tdata->flag_crc_updated = false;",
            "\t\t\tctx->state = CRC_STATE_IDLE;"
          ]
        },
        "vulnerable_code": {
          "context": "drivers/crc/crc_renesas_ra.c",
          "class": null,
          "func": null,
          "lines": [
            "\t\tctx->state = CRC_STATE_IDLE;",
            "\t\tcrc_unlock(dev);",
            "\t\treturn -EINVAL;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/zephyrproject-rtos/zephyr/commit/a08c691e052804ebfae47c2d293b5f65b1cbbfae",
        "CWE": "CWE-368",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/zephyrproject-rtos/zephyr/pull/98622"
      }
    ],
    "CWD-1122": [
      {
        "benign_code": {
          "context": "drivers/crypto/crypto_mtls_shim.c",
          "class": null,
          "func": null,
          "lines": [
            "",
            "static K_MUTEX_DEFINE(mtls_sessions_lock);",
            "",
            "#if defined(MBEDTLS_MEMORY_BUFFER_ALLOC_C)",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "\t\treturn -EINVAL;",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "\t\treturn -EINVAL;",
            "",
            "\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "",
            "\tfor (i = 0; i < CRYPTO_MAX_SESSION; i++) {",
            "\t\t\tmtls_sessions[i].in_use = true;",
            "\t\t\tk_mutex_unlock(&mtls_sessions_lock);",
            "\t\t\treturn i;",
            "",
            "\tk_mutex_unlock(&mtls_sessions_lock);",
            "\treturn -1;",
            "\tint ret;",
            "\tint ret = 0;",
            "",
            "",
            "\tmtls_sessions[ctx_idx].mode = mode;",
            "\tctx->drv_sessn_state = &mtls_sessions[ctx_idx];",
            "",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_free(aes_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t}",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_init(aes_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t}",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_ccm_free(ccm_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_gcm_free(gcm_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\treturn -EINVAL;",
            "\t\tret = -EINVAL;",
            "\t}",
            "",
            "\t/* Centralized cleanup of the session slot if an error occurred",
            "\t *  during configuration (ret != 0).",
            "\t */",
            "\tif (ret != 0) {",
            "\t\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\tk_mutex_unlock(&mtls_sessions_lock);",
            "\t}",
            "\treturn ret;",
            "\t}",
            "\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "\tmtls_session->in_use = false;",
            "\tk_mutex_unlock(&mtls_sessions_lock);",
            "",
            "\t}",
            "\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "\tmtls_session->in_use = false;",
            "\tk_mutex_unlock(&mtls_sessions_lock);",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "drivers/crypto/crypto_mtls_shim.c",
          "class": null,
          "func": null,
          "lines": [
            "\tif (ret) {",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "\tif (ret) {",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "",
            "\tapkt->pkt->out_len = apkt->pkt->in_len;",
            "\tapkt->pkt->out_len += ctx->mode_params.ccm_info.tag_len;",
            "",
            "\treturn 0;",
            "",
            "\tapkt->pkt->out_len = apkt->pkt->in_len;",
            "\tapkt->pkt->out_len += ctx->mode_params.gcm_info.tag_len;",
            "",
            "\treturn 0;",
            "\tint ctx_idx;",
            "\tint ret;",
            "\tint ret = 0;",
            "\t\t\tLOG_ERR(\"AES_ECB: failed at setkey (%d)\", ret);",
            "\t\t\tctx->ops.block_crypt_hndlr = NULL;",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_free(aes_ctx);",
            "\t\t\tLOG_ERR(\"AES_CBC: failed at setkey (%d)\", ret);",
            "\t\t\tctx->ops.cbc_crypt_hndlr = NULL;",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_init(aes_ctx);",
            "\t\t\tLOG_ERR(\"AES_CCM: failed at setkey (%d)\", ret);",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_ccm_free(ccm_ctx);",
            "\t\t\tLOG_ERR(\"AES_GCM: failed at setkey (%d)\", ret);",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_gcm_free(gcm_ctx);",
            "\t\tLOG_ERR(\"Unhandled mode\");",
            "\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\treturn -EINVAL;",
            "\t\tret = -EINVAL;",
            "",
            "\tmtls_sessions[ctx_idx].mode = mode;",
            "\tctx->drv_sessn_state = &mtls_sessions[ctx_idx];",
            "",
            "\t/* Centralized cleanup of the session slot if an error occurred"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/zephyrproject-rtos/zephyr/commit/0719c9eca2b106004598f8a2b2f5f8ba77866fc4",
        "CWE": "CWE-412",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/zephyrproject-rtos/zephyr/pull/98710"
      }
    ],
    "CWD-1123": [
      {
        "benign_code": {
          "context": "src/llmq/signing_shares.cpp",
          "class": null,
          "func": null,
          "lines": [
            "    std::vector<CBLSId> idsForRecovery;",
            "    std::shared_ptr<CRecoveredSig> singleMemberRecoveredSig;",
            "    {",
            "            auto rs = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "            singleMemberRecoveredSig = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "                                                      recoveredSig);",
            "",
            "    // Handle single-member quorum case after releasing the lock",
            "    if (singleMemberRecoveredSig) {",
            "        sigman.ProcessRecoveredSig(singleMemberRecoveredSig, m_peerman);",
            "        return; // end of single-quorum processing",
            "    }",
            "",
            "    // now recover it"
          ]
        },
        "vulnerable_code": {
          "context": "src/llmq/signing_shares.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "            auto rs = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "            singleMemberRecoveredSig = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "                                                      recoveredSig);",
            "            sigman.ProcessRecoveredSig(rs, m_peerman);",
            "            return; // end of single-quorum processing",
            "        }"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/dashpay/dash/commit/b440b1676e16318f0bb7badd85cfb18a82bbbbec",
        "CWE": "CWE-413",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/dashpay/dash/pull/6954"
      }
    ],
    "CWD-1124": [
      {
        "benign_code": {
          "context": "src/rgw/driver/posix/notify.h",
          "class": null,
          "func": null,
          "lines": [
            "    std::thread thrd;",
            "    std::mutex map_mutex;  // protects wd_callback_map and wd_remove_map",
            "    wd_callback_map_t wd_callback_map;",
            "\t      continue;",
            "",
            "\t    // Copy watch record data while holding the lock to avoid use-after-free",
            "\t    std::string watch_name;",
            "\t    void* watch_opaque;",
            "\t    {",
            "\t      std::lock_guard lock(map_mutex);",
            "\t      const auto& it = wd_callback_map.find(event->wd);",
            "\t      //std::cout << fmt::format(\"event! {}\", event->name) << std::endl;",
            "\t      if (it == wd_callback_map.end()) [[unlikely]] {",
            "\t\t/* non-destructive race, it happens */",
            "\t\tcontinue;",
            "\t      }",
            "\t      const auto& wr = it->second;",
            "\t      watch_name = wr.name;",
            "\t      watch_opaque = wr.opaque;",
            "\t    }",
            "\t    const auto& wr = it->second;",
            "",
            "\t    if (event->mask & IN_Q_OVERFLOW) [[unlikely]] {",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);",
            "\t      goto restart;",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);",
            "\t    }",
            "      } else {",
            "\tstd::lock_guard lock(map_mutex);",
            "\twd_callback_map.insert(wd_callback_map_t::value_type(wd, WatchRecord(wd, dname, opaque)));",
            "      int r{0};",
            "      std::lock_guard lock(map_mutex);",
            "      const auto& elt = wd_remove_map.find(dname);"
          ]
        },
        "vulnerable_code": {
          "context": "src/rgw/driver/posix/notify.h",
          "class": null,
          "func": null,
          "lines": [
            "\t    event = reinterpret_cast<struct inotify_event*>(ptr);",
            "\t    const auto& it = wd_callback_map.find(event->wd);",
            "\t    //std::cout << fmt::format(\"event! {}\", event->name) << std::endl;",
            "\t    if (it == wd_callback_map.end()) [[unlikely]] {",
            "\t      /* non-destructive race, it happens */",
            "\t      continue;",
            "",
            "\t    }",
            "\t    const auto& wr = it->second;",
            "",
            "\t      evec.emplace_back(Notifiable::Event(Notifiable::EventType::INVALIDATE, std::nullopt));",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);",
            "\t    if (evec.size() > 0) {",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/ceph/ceph/commit/71b1ec22dc1ddd105c4e0b8604e9ae375a0c3e59",
        "CWE": "CWE-414",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/ceph/ceph/pull/66186"
      },
      {
        "benign_code": {
          "context": "src/v/cloud_io/cache_service.cc",
          "class": null,
          "func": null,
          "lines": [
            "    auto tracker_lru_entries = _access_time_tracker.lru_entries();",
            "    auto tracker_lru_entries = co_await _access_time_tracker.lru_entries();",
            "    vlog("
          ]
        },
        "vulnerable_code": {
          "context": "src/v/cloud_io/cache_service.cc",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    auto tracker_lru_entries = _access_time_tracker.lru_entries();",
            "    auto tracker_lru_entries = co_await _access_time_tracker.lru_entries();"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/redpanda-data/redpanda/commit/9b45ae8251821b87d4505eb381be51777a39ac85",
        "CWE": "CWE-414",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/redpanda-data/redpanda/pull/28412"
      },
      {
        "benign_code": {
          "context": "ydb/core/tx/columnshard/operations/manager.cpp",
          "class": null,
          "func": null,
          "lines": [
            "        NActors::TLogContextBuilder::Build(NKikimrServices::TX_COLUMNSHARD_TX)(\"commit_tx_id\", txId)(\"commit_lock_id\", lock.GetLockId()));",
            "    if (lock.GetWriteOperations().size() > 0) {",
            "        AFL_VERIFY(!lock.IsBroken())(\"error\", \"the tx has writes, it is broken, and we are committing it\")(\"writes_count\", lock.GetWriteOperations().size());",
            "    }",
            "    TVector<TWriteOperation::TPtr> commited;"
          ]
        },
        "vulnerable_code": {
          "context": "ydb/core/tx/columnshard/operations/manager.cpp",
          "class": null,
          "func": null,
          "lines": []
        },
        "source": "github",
        "commit_url": "https://github.com/ydb-platform/ydb/commit/80a142b406ef5ab8208711551cdf50a26381f041",
        "CWE": "CWE-414",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/ydb-platform/ydb/pull/28651"
      }
    ],
    "CWD-1125": [
      {
        "benign_code": {
          "context": "src/chainlock/chainlock.cpp",
          "class": null,
          "func": null,
          "lines": [
            "            // no need to process/relay older CLSIGs",
            "            // no need to process older/same CLSIGs",
            "            return {};",
            "        LOCK(cs);",
            "        // newer chainlock could be processed via another thread while we were not holding the lock, re-verify",
            "        if (!bestChainLock.IsNull() && clsig.getHeight() <= bestChainLock.getHeight()) {",
            "            // no need to process older/same CLSIGs",
            "            return {};",
            "        }",
            "        bestChainLockHash = hash;"
          ]
        },
        "vulnerable_code": {
          "context": "src/chainlock/chainlock.cpp",
          "class": null,
          "func": null,
          "lines": [
            "        if (!bestChainLock.IsNull() && clsig.getHeight() <= bestChainLock.getHeight()) {",
            "            // no need to process/relay older CLSIGs",
            "            // no need to process older/same CLSIGs"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/dashpay/dash/commit/be4a71cb23d77cf46d24b1fd6c83f54fd345698b",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/dashpay/dash/pull/6924"
      },
      {
        "benign_code": {
          "context": "src/skadi/sample.cc",
          "class": null,
          "func": null,
          "lines": [
            "    item.init(f, format_t::RAW);",
            "    mutex.lock();",
            "    if (item.get_data() == nullptr) {",
            "      auto f = data_source + get_hgt_file_name(index);",
            "      item.init(f, format_t::RAW);",
            "    }",
            "    mutex.unlock();",
            "  }",
            "    }",
            "    tile = cache_->source(index);",
            "    if (!tile) {"
          ]
        },
        "vulnerable_code": {
          "context": "src/skadi/sample.cc",
          "class": null,
          "func": null,
          "lines": [
            "  if (item.get_data() == nullptr) {",
            "    auto f = data_source + get_hgt_file_name(index);",
            "    item.init(f, format_t::RAW);",
            "    mutex.lock();",
            "  if (index != tile.get_index()) {",
            "    {",
            "      std::lock_guard<std::mutex> _(cache_lck);",
            "      tile = cache_->source(index);",
            "    }",
            "    tile = cache_->source(index);",
            "",
            "  std::lock_guard<std::mutex> _(cache_lck);",
            "  return cache_->insert(data->first, fpath.string(), data->second);",
            "void sample::add_single_tile(const std::string& path) {",
            "  std::lock_guard<std::mutex> _(cache_lck);",
            "  cache_->insert(0, path, format_t::RAW);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/valhalla/valhalla/commit/da06f3dea81d1863c67e12ae315cd9854f93ecce",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/valhalla/valhalla/pull/5601"
      },
      {
        "benign_code": {
          "context": "src/string_pool/string_pool.c",
          "class": null,
          "func": null,
          "lines": [
            "\tpthread_rwlock_t rwlock;   // read-write lock to protect hash table",
            "\tpthread_mutex_t lock;      // mutex protecting access to hash table",
            "\tuint64_t total_ref_count;  // total number of active references",
            "\tASSERT(pool != NULL);",
            "\tStringPool pool = rm_malloc (sizeof(struct OpaqueStringPool)) ;",
            "\tASSERT (pool != NULL) ;",
            "",
            "\tpool->ht = HashTableCreate(&_dictType);",
            "\tpool->ht = HashTableCreate (&_dictType) ;",
            "\tpool->total_ref_count = 0;",
            "\tASSERT(res == 0);",
            "\tint res = pthread_mutex_init (&pool->lock, NULL) ;",
            "\tASSERT (res == 0) ;",
            "",
            "\tpthread_rwlock_rdlock(&pool->rwlock);",
            "\t// first, try to find the string",
            "\tpthread_mutex_lock (&pool->lock) ;",
            "",
            "\tde = HashTableFind(ht, (void*)str);",
            "\tde = HashTableFind (ht, (void*)str) ;",
            "",
            "        pthread_rwlock_unlock(&pool->rwlock);",
            "\t\tpthread_mutex_unlock (&pool->lock) ;",
            "",
            "        char *stored_str = (char*)HashTableGetKey(de);",
            "        char *stored_str = (char*)HashTableGetKey (de) ;",
            "        return stored_str;",
            "\tde = HashTableAddRaw(ht, (void*)str, &existing);",
            "\tde = HashTableAddRaw (ht, (void*)str, &existing) ;",
            "\tASSERT (de != NULL) ;",
            "",
            "\t}",
            "\t// new entry: duplicate key and insert",
            "\tHashTableSetKey (ht, de, rm_strdup(str)) ;",
            "",
            "\t// set initial ref-count to 1",
            "\tcount = (uint32_t*)HashTableEntryMetadata(de);",
            "\t*count = 1;",
            "",
            "\tpthread_rwlock_unlock(&pool->rwlock);",
            "\t// release lock",
            "\tpthread_mutex_unlock (&pool->lock) ;",
            "",
            "\tASSERT(str != NULL);",
            "\tASSERT (str != NULL) ;",
            "",
            "\tASSERT(pool != NULL);",
            "\tStringPool pool = Globals_Get_StringPool () ;",
            "\tASSERT (pool != NULL) ;",
            "",
            "\t\t\tstr, file, line);",
            "\tRedisModule_Log (NULL, \"debug\", \"StringPool_return: \\\"%s\\\" from %s:%d\\n\",",
            "\t\t\tstr, file, line) ;",
            "\t#endif",
            "    pthread_rwlock_rdlock(&pool->rwlock);",
            "\t// access entry",
            "\tpthread_mutex_lock (&pool->lock) ;",
            "",
            "\tde = HashTableFind(ht, str);",
            "\tde = HashTableFind (ht, str) ;",
            "\tASSERT(de != NULL);",
            "\t\tpthread_rwlock_unlock(&pool->rwlock);",
            "\tif (old_count == 1) {",
            "\t\tint res = HashTableDelete (ht, (const void *)str) ;",
            "\t\tASSERT(res == DICT_OK);",
            "\t}",
            "",
            "\t// release lock",
            "\tpthread_mutex_unlock (&pool->lock) ;",
            "",
            "\t// update total reference count",
            "\tpthread_rwlock_unlock(&pool->rwlock);",
            "\t// get hash table entry count under lock",
            "\tpthread_mutex_lock (&pool->lock) ;",
            "\tn_entries = HashTableElemCount (pool->ht) ;",
            "\tpthread_mutex_unlock (&pool->lock) ;",
            "",
            "\tHashTableRelease(p->ht);",
            "\tHashTableRelease (p->ht) ;",
            "",
            "\tASSERT(res == 0);",
            "\tint res = pthread_mutex_destroy (&p->lock) ;",
            "\tASSERT (res == 0) ;",
            "",
            "\trm_free(*pool);",
            "\trm_free (*pool) ;",
            "\t*pool = NULL;"
          ]
        },
        "vulnerable_code": {
          "context": "src/string_pool/string_pool.c",
          "class": null,
          "func": null,
          "lines": [
            "\tdict *ht;                  // hash table mapping string -> reference count",
            "\tpthread_rwlock_t rwlock;   // read-write lock to protect hash table",
            "\tpthread_mutex_t lock;      // mutex protecting access to hash table",
            "StringPool StringPool_create(void) {",
            "\tStringPool pool = rm_malloc(sizeof(struct OpaqueStringPool));",
            "\tASSERT(pool != NULL);",
            "\tStringPool pool = rm_malloc (sizeof(struct OpaqueStringPool)) ;",
            "",
            "\tpool->ht = HashTableCreate(&_dictType);",
            "\tpool->ht = HashTableCreate (&_dictType) ;",
            "",
            "\t// initialize read-write lock with writer preference if supported",
            "",
            "\tpthread_rwlockattr_t attr;",
            "\tint res = pthread_rwlockattr_init(&attr);",
            "\tASSERT(res == 0);",
            "",
            "#ifdef PTHREAD_RWLOCK_PREFER_WRITER_NONRECURSIVE_NP",
            "\tint pref = PTHREAD_RWLOCK_PREFER_WRITER_NONRECURSIVE_NP;",
            "\tres = pthread_rwlockattr_setkind_np(&attr, pref);",
            "\tASSERT(res == 0);",
            "#endif",
            "",
            "\tres = pthread_rwlock_init(&pool->rwlock, &attr);",
            "\tASSERT(res == 0);",
            "",
            "\tres = pthread_rwlockattr_destroy(&attr);",
            "\tASSERT(res == 0);",
            "\tint res = pthread_mutex_init (&pool->lock, NULL) ;",
            "",
            "\t// first, try to find the string under a read lock",
            "\tpthread_rwlock_rdlock(&pool->rwlock);",
            "\t// first, try to find the string",
            "",
            "\tde = HashTableFind(ht, (void*)str);",
            "\tde = HashTableFind (ht, (void*)str) ;",
            "",
            "        pthread_rwlock_unlock(&pool->rwlock);",
            "\t\tpthread_mutex_unlock (&pool->lock) ;",
            "",
            "        char *stored_str = (char*)HashTableGetKey(de);",
            "        char *stored_str = (char*)HashTableGetKey (de) ;",
            "    }",
            "    pthread_rwlock_unlock(&pool->rwlock);",
            "",
            "\t// entry not found, insert under write lock",
            "\tpthread_rwlock_wrlock(&pool->rwlock);",
            "",
            "\t// double-check: another thread might have inserted while we waited",
            "\tdictEntry *existing = NULL;  // existing dict entry",
            "\tde = HashTableAddRaw(ht, (void*)str, &existing);",
            "\tde = HashTableAddRaw (ht, (void*)str, &existing) ;",
            "\tuint32_t *count = NULL;",
            "\tif(de != NULL) {",
            "\t\t// new entry: duplicate key and insert",
            "\t\tHashTableSetKey(ht, de, rm_strdup(str));",
            "\t\t// set initial ref-count to 1",
            "\t\tcount = (uint32_t*)HashTableEntryMetadata(de);",
            "\t\t*count = 1;",
            "\t} else {",
            "\t\t// another thread inserted it before us",
            "\t\tde = existing;",
            "\t\tcount = (uint32_t*)HashTableEntryMetadata(de);",
            "\t\t__atomic_fetch_add(count, 1, __ATOMIC_RELAXED);",
            "\t}",
            "\t// new entry: duplicate key and insert",
            "",
            "\t// release write lock",
            "\tpthread_rwlock_unlock(&pool->rwlock);",
            "\t// release lock",
            "\t// validate arguments",
            "\tASSERT(str != NULL);",
            "\tASSERT (str != NULL) ;",
            "",
            "\tStringPool pool = Globals_Get_StringPool();",
            "\tASSERT(pool != NULL);",
            "\tStringPool pool = Globals_Get_StringPool () ;",
            "\t#ifdef DEBUG_STRINGPOOL",
            "\tRedisModule_Log(NULL, \"debug\", \"StringPool_return: \\\"%s\\\" from %s:%d\\n\",",
            "\t\t\tstr, file, line);",
            "\tRedisModule_Log (NULL, \"debug\", \"StringPool_return: \\\"%s\\\" from %s:%d\\n\",",
            "",
            "\t// access entry under read lock",
            "    pthread_rwlock_rdlock(&pool->rwlock);",
            "\t// access entry",
            "",
            "\tde = HashTableFind(ht, str);",
            "\tde = HashTableFind (ht, str) ;",
            "",
            "    pthread_rwlock_unlock(&pool->rwlock);",
            "",
            "\t// decrease reference count",
            "\t// if this was the last reference, delete the entry",
            "\tif(old_count == 1) {",
            "\t\t// acquire write lock for potential deletion",
            "\t\tpthread_rwlock_wrlock(&pool->rwlock);",
            "",
            "\t\t// re-check under write lock in case another thread modified the count",
            "\t\tde = HashTableFind(ht, str);",
            "\t\tif(de != NULL) {",
            "\t\t\tcount = (uint32_t*)HashTableEntryMetadata(de);",
            "            if(*count == 0) {",
            "                // safe to delete",
            "                int res = HashTableDelete(ht, (const void *)str);",
            "                ASSERT(res == DICT_OK);",
            "            }",
            "\t\t\t// if count != 0, another thread incremented it, so don't delete",
            "\t\t}",
            "\t\t// if de == NULL, another thread already deleted it",
            "",
            "\t\t// release write lock",
            "\t\tpthread_rwlock_unlock(&pool->rwlock);",
            "\tif (old_count == 1) {",
            "",
            "\t// get hash table entry count under read lock",
            "\tpthread_rwlock_rdlock(&pool->rwlock);",
            "\tn_entries = HashTableElemCount(pool->ht);",
            "\tpthread_rwlock_unlock(&pool->rwlock);",
            "\t// get hash table entry count under lock",
            "\t// free hashtable",
            "\tHashTableRelease(p->ht);",
            "\tHashTableRelease (p->ht) ;",
            "",
            "\tint res = pthread_rwlock_destroy(&p->rwlock);",
            "\tASSERT(res == 0);",
            "\tint res = pthread_mutex_destroy (&p->lock) ;",
            "",
            "\trm_free(*pool);",
            "\trm_free (*pool) ;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/FalkorDB/FalkorDB/commit/83490c86fe1c5a98a46bbc4d58edd48736a6b6e8",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/FalkorDB/FalkorDB/pull/1196"
      }
    ],
    "CWD-1126": [
      {
        "benign_code": {
          "context": "src/llmq/signing_shares.cpp",
          "class": null,
          "func": null,
          "lines": [
            "    std::vector<CBLSId> idsForRecovery;",
            "    std::shared_ptr<CRecoveredSig> singleMemberRecoveredSig;",
            "    {",
            "            auto rs = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "            singleMemberRecoveredSig = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "                                                      recoveredSig);",
            "",
            "    // Handle single-member quorum case after releasing the lock",
            "    if (singleMemberRecoveredSig) {",
            "        sigman.ProcessRecoveredSig(singleMemberRecoveredSig, m_peerman);",
            "        return; // end of single-quorum processing",
            "    }",
            "",
            "    // now recover it"
          ]
        },
        "vulnerable_code": {
          "context": "src/llmq/signing_shares.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "            auto rs = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "            singleMemberRecoveredSig = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "                                                      recoveredSig);",
            "            sigman.ProcessRecoveredSig(rs, m_peerman);",
            "            return; // end of single-quorum processing",
            "        }"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/dashpay/dash/commit/b440b1676e16318f0bb7badd85cfb18a82bbbbec",
        "CWE": "CWE-764",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/dashpay/dash/pull/6954"
      },
      {
        "benign_code": {
          "context": "src/OrbitBase/include/OrbitBase/Executor.h",
          "class": null,
          "func": null,
          "lines": [
            "      if (argument.has_error()) {",
            "        // Here in SetResult other continuations might be called to propogate the result,",
            "        // and some of these continuations might result in calling of ScheduleAfter() which locks",
            "        // the same executor mutex. To avoid possible deadlocks, execute SetResult() here",
            "        // in the non-locked context.",
            "        promise.SetResult(outcome::failure(argument.error()));",
            "",
            "        absl::ReaderMutexLock lock{&executor_handle.data_->mutex_};",
            "        if (executor_handle.data_->executor_ == nullptr) return;",
            "",
            "        executor_handle.data_->executor_->ScheduleImpl(CreateAction(",
            "      };",
            "",
            "      absl::ReaderMutexLock lock{&executor_handle.data_->mutex_};",
            "      if (executor_handle.data_->executor_ == nullptr) return;",
            "",
            "      executor_handle.data_->executor_->ScheduleImpl("
          ]
        },
        "vulnerable_code": {
          "context": "src/OrbitBase/include/OrbitBase/Executor.h",
          "class": null,
          "func": null,
          "lines": [
            "                         promise = std::move(promise)](const Result<T, E>& argument) mutable {",
            "      absl::ReaderMutexLock lock{&executor_handle.data_->mutex_};",
            "      if (executor_handle.data_->executor_ == nullptr) return;",
            "",
            "      // If the future returns a non-success ErrorMessageOr-type, we will short-circuit and won't"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/google/orbit/commit/0b63b7c42d45fe59883b9e72ac978c5d4479c4d9",
        "CWE": "CWE-764",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/google/orbit/pull/4821"
      }
    ],
    "CWD-1128": [
      {
        "benign_code": {
          "context": "sql/handler.cc",
          "class": null,
          "func": null,
          "lines": [
            "  if (mdl_backup.ticket)",
            "  // reset the pointer to the ticket when it's stack instantiated",
            "  if (thd->backup_commit_lock == &mdl_backup)",
            "  {",
            "    thd->mdl_context.release_lock(mdl_backup.ticket);",
            "     */",
            "    if (mdl_backup.ticket)",
            "      thd->mdl_context.release_lock(mdl_backup.ticket);",
            "    thd->backup_commit_lock= 0;"
          ]
        },
        "vulnerable_code": {
          "context": "sql/handler.cc",
          "class": null,
          "func": null,
          "lines": [
            "end:",
            "  if (mdl_backup.ticket)",
            "  // reset the pointer to the ticket when it's stack instantiated",
            "      not needed.",
            "    */",
            "    thd->mdl_context.release_lock(mdl_backup.ticket);",
            "     */"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/MariaDB/server/commit/a1bc50e18af64cbcf16f109c5e0053bd4bfd4229",
        "CWE": "CWE-832",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/MariaDB/server/pull/4354"
      }
    ],
    "CWD-1129": [
      {
        "benign_code": {
          "context": "drivers/crypto/crypto_mtls_shim.c",
          "class": null,
          "func": null,
          "lines": [
            "",
            "static K_MUTEX_DEFINE(mtls_sessions_lock);",
            "",
            "#if defined(MBEDTLS_MEMORY_BUFFER_ALLOC_C)",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "\t\treturn -EINVAL;",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "\t\treturn -EINVAL;",
            "",
            "\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "",
            "\tfor (i = 0; i < CRYPTO_MAX_SESSION; i++) {",
            "\t\t\tmtls_sessions[i].in_use = true;",
            "\t\t\tk_mutex_unlock(&mtls_sessions_lock);",
            "\t\t\treturn i;",
            "",
            "\tk_mutex_unlock(&mtls_sessions_lock);",
            "\treturn -1;",
            "\tint ret;",
            "\tint ret = 0;",
            "",
            "",
            "\tmtls_sessions[ctx_idx].mode = mode;",
            "\tctx->drv_sessn_state = &mtls_sessions[ctx_idx];",
            "",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_free(aes_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t}",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_init(aes_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t}",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_ccm_free(ccm_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_gcm_free(gcm_ctx);",
            "\t\t\tret = -EINVAL;",
            "\t\t\tbreak;",
            "\t\t}",
            "\t\treturn -EINVAL;",
            "\t\tret = -EINVAL;",
            "\t}",
            "",
            "\t/* Centralized cleanup of the session slot if an error occurred",
            "\t *  during configuration (ret != 0).",
            "\t */",
            "\tif (ret != 0) {",
            "\t\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\tk_mutex_unlock(&mtls_sessions_lock);",
            "\t}",
            "\treturn ret;",
            "\t}",
            "\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "\tmtls_session->in_use = false;",
            "\tk_mutex_unlock(&mtls_sessions_lock);",
            "",
            "\t}",
            "\tk_mutex_lock(&mtls_sessions_lock, K_FOREVER);",
            "\tmtls_session->in_use = false;",
            "\tk_mutex_unlock(&mtls_sessions_lock);",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "drivers/crypto/crypto_mtls_shim.c",
          "class": null,
          "func": null,
          "lines": [
            "\tif (ret) {",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "\tif (ret) {",
            "\t\tLOG_ERR(\"Could not encrypt (%d)\", ret);",
            "\t\tLOG_ERR(\"Could not decrypt (%d)\", ret);",
            "",
            "\tapkt->pkt->out_len = apkt->pkt->in_len;",
            "\tapkt->pkt->out_len += ctx->mode_params.ccm_info.tag_len;",
            "",
            "\treturn 0;",
            "",
            "\tapkt->pkt->out_len = apkt->pkt->in_len;",
            "\tapkt->pkt->out_len += ctx->mode_params.gcm_info.tag_len;",
            "",
            "\treturn 0;",
            "\tint ctx_idx;",
            "\tint ret;",
            "\tint ret = 0;",
            "\t\t\tLOG_ERR(\"AES_ECB: failed at setkey (%d)\", ret);",
            "\t\t\tctx->ops.block_crypt_hndlr = NULL;",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_free(aes_ctx);",
            "\t\t\tLOG_ERR(\"AES_CBC: failed at setkey (%d)\", ret);",
            "\t\t\tctx->ops.cbc_crypt_hndlr = NULL;",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_aes_init(aes_ctx);",
            "\t\t\tLOG_ERR(\"AES_CCM: failed at setkey (%d)\", ret);",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_ccm_free(ccm_ctx);",
            "\t\t\tLOG_ERR(\"AES_GCM: failed at setkey (%d)\", ret);",
            "\t\t\tmtls_sessions[ctx_idx].in_use = false;",
            "",
            "\t\t\treturn -EINVAL;",
            "\t\t\tmbedtls_gcm_free(gcm_ctx);",
            "\t\tLOG_ERR(\"Unhandled mode\");",
            "\t\tmtls_sessions[ctx_idx].in_use = false;",
            "\t\treturn -EINVAL;",
            "\t\tret = -EINVAL;",
            "",
            "\tmtls_sessions[ctx_idx].mode = mode;",
            "\tctx->drv_sessn_state = &mtls_sessions[ctx_idx];",
            "",
            "\t/* Centralized cleanup of the session slot if an error occurred"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/zephyrproject-rtos/zephyr/commit/0719c9eca2b106004598f8a2b2f5f8ba77866fc4",
        "CWE": "CWE-833",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/zephyrproject-rtos/zephyr/pull/98710"
      },
      {
        "benign_code": {
          "context": "src/llmq/signing_shares.cpp",
          "class": null,
          "func": null,
          "lines": [
            "    std::vector<CBLSId> idsForRecovery;",
            "    std::shared_ptr<CRecoveredSig> singleMemberRecoveredSig;",
            "    {",
            "            auto rs = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "            singleMemberRecoveredSig = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "                                                      recoveredSig);",
            "",
            "    // Handle single-member quorum case after releasing the lock",
            "    if (singleMemberRecoveredSig) {",
            "        sigman.ProcessRecoveredSig(singleMemberRecoveredSig, m_peerman);",
            "        return; // end of single-quorum processing",
            "    }",
            "",
            "    // now recover it"
          ]
        },
        "vulnerable_code": {
          "context": "src/llmq/signing_shares.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "            auto rs = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "            singleMemberRecoveredSig = std::make_shared<CRecoveredSig>(quorum.params.type, quorum.qc->quorumHash, id, msgHash,",
            "                                                      recoveredSig);",
            "            sigman.ProcessRecoveredSig(rs, m_peerman);",
            "            return; // end of single-quorum processing",
            "        }"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/dashpay/dash/commit/b440b1676e16318f0bb7badd85cfb18a82bbbbec",
        "CWE": "CWE-833",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/dashpay/dash/pull/6954"
      },
      {
        "benign_code": {
          "context": "subsys/net/ip/net_if.c",
          "class": null,
          "func": null,
          "lines": [
            "\t}",
            "",
            "\tif (IS_ENABLED(CONFIG_NET_NATIVE_TCP)) {",
            "\t\tnet_if_unlock(iface);",
            "\t\tnet_tcp_close_all_for_iface(iface);",
            "\t\tnet_if_lock(iface);",
            "\t}",
            "}"
          ]
        },
        "vulnerable_code": {
          "context": "subsys/net/ip/net_if.c",
          "class": null,
          "func": null,
          "lines": [
            "{",
            "\tnet_tcp_close_all_for_iface(iface);",
            "\tnet_if_flag_clear(iface, NET_IF_RUNNING);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/zephyrproject-rtos/zephyr/commit/9e6256e85334fe8b46df600c19127fb20ad80750",
        "CWE": "CWE-833",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/zephyrproject-rtos/zephyr/pull/99268"
      },
      {
        "benign_code": {
          "context": "src/storage/file_manager.c",
          "class": null,
          "func": null,
          "lines": [
            "#endif /* !NDEBUG */",
            "  pthread_mutex_unlock (&tran_entry->mutex);",
            "}"
          ]
        },
        "vulnerable_code": {
          "context": "src/storage/file_manager.c",
          "class": null,
          "func": null,
          "lines": [
            "  assert (tran_entry->owner_mutex == thread_get_current_entry_index ());",
            "  pthread_mutex_unlock (&tran_entry->mutex);",
            "#if !defined (NDEBUG)"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/CUBRID/cubrid/commit/6cb666866f04e663f8e863fb69f409f463b9a289",
        "CWE": "CWE-833",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/CUBRID/cubrid/pull/6316"
      }
    ],
    "CWD-1131": [
      {
        "benign_code": {
          "context": "communication/src/AsyncTlsConnection.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "  std::unique_ptr<ISecretsManagerImpl> secrets_manager;",
            "  if (tlsTcpImpl_.config_.secretData) {",
            "    }",
            "    secrets_manager.reset(new SecretsManagerEnc(tlsTcpImpl_.config_.secretData.value()));",
            "  } else {",
            "    }",
            "    secrets_manager.reset(new SecretsManagerPlain());",
            "  }",
            "  auto decBuf = secrets_manager_->decryptFile(pkpath);",
            "  auto decBuf = secrets_manager->decryptFile(pkpath);",
            "  if (!decBuf) {"
          ]
        },
        "vulnerable_code": {
          "context": "communication/src/AsyncTlsConnection.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "inline static std::unique_ptr<ISecretsManagerImpl> secrets_manager_;",
            "",
            "const std::string AsyncTlsConnection::decryptPK(const boost::filesystem::path& path) {",
            "    pkpath = (path / fs::path(\"pk.pem.enc\")).string();",
            "    if (secrets_manager_ == nullptr) {",
            "      secrets_manager_.reset(new SecretsManagerEnc(tlsTcpImpl_.config_.secretData.value()));",
            "    }",
            "    secrets_manager.reset(new SecretsManagerEnc(tlsTcpImpl_.config_.secretData.value()));",
            "    pkpath = (path / fs::path(\"pk.pem\")).string();",
            "    if (secrets_manager_ == nullptr) {",
            "      secrets_manager_.reset(new SecretsManagerPlain());",
            "    }",
            "    secrets_manager.reset(new SecretsManagerPlain());",
            "",
            "  auto decBuf = secrets_manager_->decryptFile(pkpath);",
            "  auto decBuf = secrets_manager->decryptFile(pkpath);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/vmware/concord-bft/commit/974c96122fbbbf8879d54b62f0bd49e1817f04e1",
        "CWE": "CWE-543",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/vmware/concord-bft/pull/1389"
      }
    ],
    "CWD-1132": [
      {
        "benign_code": {
          "context": "src/rgw/driver/posix/notify.h",
          "class": null,
          "func": null,
          "lines": [
            "    std::thread thrd;",
            "    std::mutex map_mutex;  // protects wd_callback_map and wd_remove_map",
            "    wd_callback_map_t wd_callback_map;",
            "\t      continue;",
            "",
            "\t    // Copy watch record data while holding the lock to avoid use-after-free",
            "\t    std::string watch_name;",
            "\t    void* watch_opaque;",
            "\t    {",
            "\t      std::lock_guard lock(map_mutex);",
            "\t      const auto& it = wd_callback_map.find(event->wd);",
            "\t      //std::cout << fmt::format(\"event! {}\", event->name) << std::endl;",
            "\t      if (it == wd_callback_map.end()) [[unlikely]] {",
            "\t\t/* non-destructive race, it happens */",
            "\t\tcontinue;",
            "\t      }",
            "\t      const auto& wr = it->second;",
            "\t      watch_name = wr.name;",
            "\t      watch_opaque = wr.opaque;",
            "\t    }",
            "\t    const auto& wr = it->second;",
            "",
            "\t    if (event->mask & IN_Q_OVERFLOW) [[unlikely]] {",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);",
            "\t      goto restart;",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);",
            "\t    }",
            "      } else {",
            "\tstd::lock_guard lock(map_mutex);",
            "\twd_callback_map.insert(wd_callback_map_t::value_type(wd, WatchRecord(wd, dname, opaque)));",
            "      int r{0};",
            "      std::lock_guard lock(map_mutex);",
            "      const auto& elt = wd_remove_map.find(dname);"
          ]
        },
        "vulnerable_code": {
          "context": "src/rgw/driver/posix/notify.h",
          "class": null,
          "func": null,
          "lines": [
            "\t    event = reinterpret_cast<struct inotify_event*>(ptr);",
            "\t    const auto& it = wd_callback_map.find(event->wd);",
            "\t    //std::cout << fmt::format(\"event! {}\", event->name) << std::endl;",
            "\t    if (it == wd_callback_map.end()) [[unlikely]] {",
            "\t      /* non-destructive race, it happens */",
            "\t      continue;",
            "",
            "\t    }",
            "\t    const auto& wr = it->second;",
            "",
            "\t      evec.emplace_back(Notifiable::Event(Notifiable::EventType::INVALIDATE, std::nullopt));",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);",
            "\t    if (evec.size() > 0) {",
            "\t      n->notify(wr.name, wr.opaque, evec);",
            "\t      n->notify(watch_name, watch_opaque, evec);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/ceph/ceph/commit/71b1ec22dc1ddd105c4e0b8604e9ae375a0c3e59",
        "CWE": "CWE-567",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/ceph/ceph/pull/66186"
      },
      {
        "benign_code": {
          "context": "be/src/util/hash_util.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "#ifdef __SSE4_2__",
            "#include <nmmintrin.h>",
            "#endif",
            "",
            "#include \"util/cpu_info.h\"",
            "#include \"util/murmur_hash3.h\"",
            "#include \"util/xxh3.h\"",
            "",
            "#ifdef __SSE4_2__",
            "// Forward declarations for SSE4.2 implementations (only used within this file)",
            "static uint32_t crc_hash_sse42(const void* data, int32_t bytes, uint32_t hash);",
            "static uint64_t crc_hash64_sse42(const void* data, int32_t bytes, uint64_t hash);",
            "#endif",
            "",
            "// Function pointer types for hash functions to avoid runtime CPU checks",
            "using Hash32Func = uint32_t (*)(const void*, int32_t, uint32_t);",
            "using Hash64Func = uint64_t (*)(const void*, int32_t, uint64_t);",
            "",
            "// Function pointers that will be initialized at program startup based on CPU capabilities",
            "static Hash32Func g_hash32_func = nullptr;",
            "static Hash64Func g_hash64_func = nullptr;",
            "static Hash32Func g_crc32_func = nullptr;",
            "static Hash64Func g_crc64_func = nullptr;",
            "",
            "// Initialize function pointers at program startup based on CPU capabilities.",
            "// This avoids runtime CPU checks in the hot path (hash(), hash64(), crc_hash(), crc_hash64() functions).",
            "// The constructor attribute ensures this runs before main(), but after CpuInfo::init()",
            "// is called in Daemon::init(). If CpuInfo hasn't been initialized yet, we initialize it here.",
            "__attribute__((constructor)) void select_hash_functions() {",
            "    // Ensure CpuInfo is initialized",
            "    CpuInfo::init();",
            "",
            "    g_hash32_func = HashUtil::fnv_hash;",
            "    g_hash64_func = HashUtil::hash64_fallback;",
            "    g_crc32_func = HashUtil::zlib_crc_hash;",
            "    g_crc64_func = [](const void* data, int32_t bytes, uint64_t hash) -> uint64_t {",
            "        // For 64-bit fallback, use zlib_crc_hash on both halves",
            "        uint32_t h1 = hash >> 32;",
            "        uint32_t h2 = (hash << 32) >> 32;",
            "        h1 = HashUtil::zlib_crc_hash(data, bytes, h1);",
            "        h2 = HashUtil::zlib_crc_hash(data, bytes, h2);",
            "        return ((uint64_t)h1 << 32) | h2;",
            "    };",
            "",
            "#ifdef __SSE4_2__",
            "    if (CpuInfo::is_supported(CpuInfo::SSE4_2)) {",
            "        g_hash32_func = crc_hash_sse42;",
            "        g_hash64_func = crc_hash64_sse42;",
            "        g_crc32_func = crc_hash_sse42;",
            "        g_crc64_func = crc_hash64_sse42;",
            "    }",
            "#endif",
            "}",
            "",
            "uint64_t HashUtil::xx_hash3_64(const void* key, int32_t len, uint64_t seed) {",
            "",
            "uint64_t HashUtil::hash64_fallback(const void* data, int32_t bytes, uint64_t seed) {",
            "    uint64_t hash = 0;",
            "    murmur_hash3_x64_64(data, bytes, seed, &hash);",
            "    return hash;",
            "}",
            "",
            "#ifdef __SSE4_2__",
            "static uint32_t crc_hash_sse42(const void* data, int32_t bytes, uint32_t hash) {",
            "    uint32_t words = bytes / sizeof(uint32_t);",
            "    bytes = bytes % sizeof(uint32_t);",
            "",
            "    const uint32_t* p = reinterpret_cast<const uint32_t*>(data);",
            "",
            "    while (words--) {",
            "        hash = _mm_crc32_u32(hash, *p);",
            "        ++p;",
            "    }",
            "",
            "    const uint8_t* s = reinterpret_cast<const uint8_t*>(p);",
            "",
            "    while (bytes--) {",
            "        hash = _mm_crc32_u8(hash, *s);",
            "        ++s;",
            "    }",
            "",
            "    // The lower half of the CRC hash has poor uniformity, so swap the halves",
            "    // for anyone who only uses the first several bits of the hash.",
            "    hash = (hash << 16) | (hash >> 16);",
            "    return hash;",
            "}",
            "",
            "static uint64_t crc_hash64_sse42(const void* data, int32_t bytes, uint64_t hash) {",
            "    uint32_t words = bytes / sizeof(uint32_t);",
            "    bytes = bytes % sizeof(uint32_t);",
            "",
            "    uint32_t h1 = hash >> 32;",
            "    uint32_t h2 = (hash << 32) >> 32;",
            "",
            "    const uint32_t* p = reinterpret_cast<const uint32_t*>(data);",
            "    while (words--) {",
            "        (words & 1) ? (h1 = _mm_crc32_u32(h1, *p)) : (h2 = _mm_crc32_u32(h2, *p));",
            "        ++p;",
            "    }",
            "",
            "    const uint8_t* s = reinterpret_cast<const uint8_t*>(p);",
            "    while (bytes--) {",
            "        (bytes & 1) ? (h1 = _mm_crc32_u8(h1, *s)) : (h2 = _mm_crc32_u8(h2, *s));",
            "        ++s;",
            "    }",
            "",
            "    h1 = (h1 << 16) | (h1 >> 16);",
            "    h2 = (h2 << 16) | (h2 >> 16);",
            "    ((uint32_t*)(&hash))[0] = h1;",
            "    ((uint32_t*)(&hash))[1] = h2;",
            "    return hash;",
            "}",
            "#endif",
            "",
            "uint32_t HashUtil::hash(const void* data, int32_t bytes, uint32_t seed) {",
            "    return g_hash32_func(data, bytes, seed);",
            "}",
            "",
            "uint64_t HashUtil::hash64(const void* data, int32_t bytes, uint64_t seed) {",
            "    return g_hash64_func(data, bytes, seed);",
            "}",
            "",
            "uint32_t HashUtil::crc_hash(const void* data, int32_t bytes, uint32_t hash) {",
            "    return g_crc32_func(data, bytes, hash);",
            "}",
            "",
            "uint64_t HashUtil::crc_hash64(const void* data, int32_t bytes, uint64_t hash) {",
            "    return g_crc64_func(data, bytes, hash);",
            "}",
            "",
            "} // namespace starrocks"
          ]
        },
        "vulnerable_code": {
          "context": "be/src/util/hash_util.cpp",
          "class": null,
          "func": null,
          "lines": []
        },
        "source": "github",
        "commit_url": "https://github.com/StarRocks/starrocks/commit/5acf3410df86e278ebac1dc12ca0d44bd5f2a4c1",
        "CWE": "CWE-567",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/StarRocks/starrocks/pull/64985"
      },
      {
        "benign_code": {
          "context": "cpp/tensorrt_llm/batch_manager/dataTransceiver.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    void sendResponse(std::vector<size_t> const& blockHashes, std::map<RequestIdType, Response>::iterator it)",
            "    {",
            "        auto reqId = mCurrentRequest.value();",
            "        auto count = --mRemainSendCount[reqId];",
            "        TLLM_CHECK(count >= 0);",
            "        if (count == 0)",
            "        {",
            "            mRemainSendCount.erase(reqId);",
            "",
            "            // TODO(zhengd): pass the hashes directly instead of update llmRequest",
            "            auto llmRequest = it->second.mRequest;",
            "            llmRequest->setRequestedBlockHashes(std::move(blockHashes));",
            "",
            "            if (common::getEnvParallelCacheSend())",
            "            {",
            "                // TODO: Use a thread pool and check for thread safety.",
            "                std::thread(&DataResponder::Impl::sendAndRemoveResponse, this, it->first, std::move(it->second))",
            "                    .detach();",
            "            }",
            "            else",
            "            {",
            "                DataResponder::Impl::sendAndRemoveResponse(it->first, std::move(it->second));",
            "            }",
            "            removeResponse(it);",
            "        }",
            "        mCurrentRequest = std::nullopt;",
            "    }",
            "",
            "    void response() noexcept",
            "                    if (count == 0)",
            "                    sendResponse(blockHashes, it);",
            "                }",
            "                else",
            "                {",
            "                    auto it = getCurrentResponse();",
            "                    while (it == mReadyResponses.end())",
            "                    {",
            "                        else",
            "                        std::unique_lock lk(mCondMutex);",
            "                        mResponderCv.wait(lk, [this]() { return (mAnyReady || mTerminate); });",
            "                        if (mTerminate)",
            "                        {",
            "                            DataResponder::Impl::sendAndRemoveResponse(it->first, std::move(it->second));",
            "                            break;",
            "                        }",
            "                        removeResponse(it);",
            "                        it = getCurrentResponse();",
            "                    }",
            "                    mResponderCv.wait(lk, [this]() { return (mAnyReady || mTerminate); });",
            "                    sendResponse(blockHashes, it);",
            "                }"
          ]
        },
        "vulnerable_code": {
          "context": "cpp/tensorrt_llm/batch_manager/dataTransceiver.cpp",
          "class": null,
          "func": null,
          "lines": [
            "                {",
            "                    auto reqId = mCurrentRequest.value();",
            "                    auto count = --mRemainSendCount[reqId];",
            "                    TLLM_CHECK(count >= 0);",
            "                    if (count == 0)",
            "                    sendResponse(blockHashes, it);",
            "                    {",
            "                        mRemainSendCount.erase(reqId);",
            "",
            "                        // TODO(zhengd): pass the hashes directly instead of update llmRequest",
            "                        auto llmRequest = it->second.mRequest;",
            "                        llmRequest->setRequestedBlockHashes(std::move(blockHashes));",
            "",
            "                        if (common::getEnvParallelCacheSend())",
            "                        {",
            "                            // TODO: Use a thread pool and check for thread safety.",
            "                            std::thread(",
            "                                &DataResponder::Impl::sendAndRemoveResponse, this, it->first, std::move(it->second))",
            "                                .detach();",
            "                        }",
            "                        else",
            "                        std::unique_lock lk(mCondMutex);",
            "                        {",
            "                            DataResponder::Impl::sendAndRemoveResponse(it->first, std::move(it->second));",
            "                            break;",
            "                        }",
            "                        removeResponse(it);",
            "                        it = getCurrentResponse();",
            "                    }",
            "                    mCurrentRequest = std::nullopt;",
            "                }",
            "                else",
            "                {",
            "                    TLLM_CHECK_WITH_INFO(!mCurrentRequest.has_value(),",
            "                        \"This executor does not have a prepared KV cache for request ID: %zu, and the \"",
            "                        \"mReadyResponses size is: %zu. mpi rank :%d     \",",
            "                        mCurrentRequest.value(), mReadyResponses.size(), mpi::MpiComm::world().getRank());",
            "                    std::unique_lock lk(mCondMutex);",
            "                    mResponderCv.wait(lk, [this]() { return (mAnyReady || mTerminate); });",
            "                    sendResponse(blockHashes, it);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/NVIDIA/TensorRT-LLM/commit/bae9560e62728877ac16d20c9eedd741a8473d19",
        "CWE": "CWE-567",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7455"
      },
      {
        "benign_code": {
          "context": "cpp/tensorrt_llm/batch_manager/dataTransceiver.cpp",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    void sendResponse(std::vector<size_t> const& blockHashes, std::map<RequestIdType, Response>::iterator it)",
            "    {",
            "        auto reqId = mCurrentRequest.value();",
            "        auto count = --mRemainSendCount[reqId];",
            "        TLLM_CHECK(count >= 0);",
            "        if (count == 0)",
            "        {",
            "            mRemainSendCount.erase(reqId);",
            "",
            "            // TODO(zhengd): pass the hashes directly instead of update llmRequest",
            "            auto llmRequest = it->second.mRequest;",
            "            llmRequest->setRequestedBlockHashes(std::move(blockHashes));",
            "",
            "            if (common::getEnvParallelCacheSend())",
            "            {",
            "                // TODO: Use a thread pool and check for thread safety.",
            "                std::thread(&DataResponder::Impl::sendAndRemoveResponse, this, it->first, std::move(it->second))",
            "                    .detach();",
            "            }",
            "            else",
            "            {",
            "                DataResponder::Impl::sendAndRemoveResponse(it->first, std::move(it->second));",
            "            }",
            "            removeResponse(it);",
            "        }",
            "        mCurrentRequest = std::nullopt;",
            "    }",
            "",
            "    void response() noexcept",
            "                    if (count == 0)",
            "                    sendResponse(blockHashes, it);",
            "                }",
            "                else",
            "                {",
            "                    auto it = getCurrentResponse();",
            "                    while (it == mReadyResponses.end())",
            "                    {",
            "                        else",
            "                        std::unique_lock lk(mCondMutex);",
            "                        mResponderCv.wait(lk, [this]() { return (mAnyReady || mTerminate); });",
            "                        if (mTerminate)",
            "                        {",
            "                            DataResponder::Impl::sendAndRemoveResponse(it->first, std::move(it->second));",
            "                            break;",
            "                        }",
            "                        removeResponse(it);",
            "                        it = getCurrentResponse();",
            "                    }",
            "                    mResponderCv.wait(lk, [this]() { return (mAnyReady || mTerminate); });",
            "                    sendResponse(blockHashes, it);",
            "                }"
          ]
        },
        "vulnerable_code": {
          "context": "cpp/tensorrt_llm/batch_manager/dataTransceiver.cpp",
          "class": null,
          "func": null,
          "lines": [
            "                {",
            "                    auto reqId = mCurrentRequest.value();",
            "                    auto count = --mRemainSendCount[reqId];",
            "                    TLLM_CHECK(count >= 0);",
            "                    if (count == 0)",
            "                    sendResponse(blockHashes, it);",
            "                    {",
            "                        mRemainSendCount.erase(reqId);",
            "",
            "                        // TODO(zhengd): pass the hashes directly instead of update llmRequest",
            "                        auto llmRequest = it->second.mRequest;",
            "                        llmRequest->setRequestedBlockHashes(std::move(blockHashes));",
            "",
            "                        if (common::getEnvParallelCacheSend())",
            "                        {",
            "                            // TODO: Use a thread pool and check for thread safety.",
            "                            std::thread(",
            "                                &DataResponder::Impl::sendAndRemoveResponse, this, it->first, std::move(it->second))",
            "                                .detach();",
            "                        }",
            "                        else",
            "                        std::unique_lock lk(mCondMutex);",
            "                        {",
            "                            DataResponder::Impl::sendAndRemoveResponse(it->first, std::move(it->second));",
            "                            break;",
            "                        }",
            "                        removeResponse(it);",
            "                        it = getCurrentResponse();",
            "                    }",
            "                    mCurrentRequest = std::nullopt;",
            "                }",
            "                else",
            "                {",
            "                    TLLM_CHECK_WITH_INFO(!mCurrentRequest.has_value(),",
            "                        \"This executor does not have a prepared KV cache for request ID: %zu, and the \"",
            "                        \"mReadyResponses size is: %zu. mpi rank :%d     \",",
            "                        mCurrentRequest.value(), mReadyResponses.size(), mpi::MpiComm::world().getRank());",
            "                    std::unique_lock lk(mCondMutex);",
            "                    mResponderCv.wait(lk, [this]() { return (mAnyReady || mTerminate); });",
            "                    sendResponse(blockHashes, it);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/NVIDIA/TensorRT-LLM/commit/91c4af3f01d40ee330a6013c33a87487a0e8d78f",
        "CWE": "CWE-567",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/NVIDIA/TensorRT-LLM/pull/7099"
      }
    ],
    "CWD-1133": [
      {
        "benign_code": {
          "context": "tsl/platform/net.h",
          "class": null,
          "func": null,
          "lines": [
            "",
            "// Relinquish a claim on the given port which was previously returned by",
            "// PickUnusedPort[OrDie](). This allows PickUnusedPort[OrDie]() to return",
            "// the given port to another caller in the future. Since the number of",
            "// ports the portserver will give to a process is limited (typically 200),",
            "// recycling ports after they are no longer needed can help avoid",
            "// exhausting them. 'port' must be a positive number that was previously",
            "// returned by PickUnusedPort[OrDie](), and not yet recycled, otherwise an",
            "// abort may occur.",
            "void RecycleUnusedPort(int port);",
            "",
            "}  // namespace internal"
          ]
        },
        "vulnerable_code": {
          "context": "tsl/platform/net.h",
          "class": null,
          "func": null,
          "lines": []
        },
        "source": "github",
        "commit_url": "https://github.com/google/tsl/commit/3d6ce0ef41b28e039b0942d588d923f3ab6983bf",
        "CWE": "CWE-1096",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/google/tsl/pull/3408"
      }
    ],
    "CWD-1137": [
      {
        "benign_code": {
          "context": "ractor.c",
          "class": null,
          "func": null,
          "lines": [
            "        VALUE type = RB_BUILTIN_TYPE(obj);",
            "        size_t slot_size = rb_gc_obj_slot_size(obj);",
            "        type |= wb_protected_types[type] ? FL_WB_PROTECTED : 0;",
            "        NEWOBJ_OF(moved, struct RBasic, 0, type, rb_gc_obj_slot_size(obj), 0);",
            "        NEWOBJ_OF(moved, struct RBasic, 0, type, slot_size, 0);",
            "        MEMZERO(&moved[1], char, slot_size - sizeof(*moved));",
            "        data->replacement = (VALUE)moved;"
          ]
        },
        "vulnerable_code": {
          "context": "ractor.c",
          "class": null,
          "func": null,
          "lines": [
            "        type |= wb_protected_types[type] ? FL_WB_PROTECTED : 0;",
            "        NEWOBJ_OF(moved, struct RBasic, 0, type, rb_gc_obj_slot_size(obj), 0);",
            "        NEWOBJ_OF(moved, struct RBasic, 0, type, slot_size, 0);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/ruby/ruby/commit/52ea222027c7315a5d66f0d7b4ab73c1cc0c7344",
        "CWE": "CWE-663",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/ruby/ruby/pull/14947"
      },
      {
        "benign_code": {
          "context": "src/shared_modules/utils/timeHelper.h",
          "class": null,
          "func": null,
          "lines": [
            "",
            "#ifdef WIN32",
            "",
            "static struct tm* gmtime_r(const time_t* timep, struct tm* result)",
            "{",
            "    errno = gmtime_s(result, timep);",
            "    return errno == 0 ? result : nullptr;",
            "}",
            "",
            "static struct tm* localtime_r(const time_t* timep, struct tm* result)",
            "{",
            "    errno = localtime_s(result, timep);",
            "    return errno == 0 ? result : nullptr;",
            "}",
            "",
            "#endif",
            "",
            "namespace Utils",
            "        std::stringstream ss;",
            "        struct tm buf;",
            "",
            "        // gmtime: result expressed as a UTC time",
            "        tm* localTime {utc ? gmtime(&time) : localtime(&time)};",
            "        tm* localTime {utc ? gmtime_r(&time, &buf) : localtime_r(&time, &buf)};",
            "",
            "        if (localTime == nullptr)",
            "        {",
            "            return \"1970/01/01 00:00:00\";",
            "        }",
            "",
            "        // Final timestamp: \"YYYY/MM/DD hh:mm:ss\"",
            "        std::stringstream ss;",
            "        struct tm buf;",
            "",
            "        // gmtime: result expressed as a UTC time",
            "        tm const* localTime {utc ? gmtime(&time) : localtime(&time)};",
            "        tm const* localTime {utc ? gmtime_r(&time, &buf) : localtime_r(&time, &buf)};",
            "",
            "        if (localTime == nullptr)",
            "        {",
            "            return \"1970/01/01 00:00:00\";",
            "        }",
            "",
            "        // Date",
            "        ss << std::put_time(gmtime(&itt), \"%FT%T\");",
            "        struct tm buf;",
            "        tm* localTime = gmtime_r(&itt, &buf);",
            "",
            "        if (localTime == nullptr)",
            "        {",
            "            return \"1970/01/01 00:00:00\";",
            "        }",
            "",
            "        ss << std::put_time(localTime, \"%FT%T\");",
            "",
            "        output << std::put_time(gmtime(&time), \"%FT%T\");",
            "        struct tm* localTime = gmtime_r(&time, &tm);",
            "",
            "        if (localTime == nullptr)",
            "        {",
            "            return \"\";",
            "        }",
            "",
            "        output << std::put_time(localTime, \"%FT%T\");",
            "",
            "        output << std::put_time(gmtime(&time), \"%FT%T\");",
            "        struct tm buf;",
            "        tm* localTime = gmtime_r(&time, &buf);",
            "",
            "        if (localTime == nullptr)",
            "        {",
            "            return \"\";",
            "        }",
            "",
            "        output << std::put_time(localTime, \"%FT%T\");",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "src/shared_modules/utils/timeHelper.h",
          "class": null,
          "func": null,
          "lines": [
            "        // gmtime: result expressed as a UTC time",
            "        tm* localTime {utc ? gmtime(&time) : localtime(&time)};",
            "        tm* localTime {utc ? gmtime_r(&time, &buf) : localtime_r(&time, &buf)};",
            "        // gmtime: result expressed as a UTC time",
            "        tm const* localTime {utc ? gmtime(&time) : localtime(&time)};",
            "        tm const* localTime {utc ? gmtime_r(&time, &buf) : localtime_r(&time, &buf)};",
            "        std::ostringstream ss;",
            "        ss << std::put_time(gmtime(&itt), \"%FT%T\");",
            "        struct tm buf;",
            "        std::ostringstream output;",
            "        output << std::put_time(gmtime(&time), \"%FT%T\");",
            "        struct tm* localTime = gmtime_r(&time, &tm);",
            "        std::ostringstream output;",
            "        output << std::put_time(gmtime(&time), \"%FT%T\");",
            "        struct tm buf;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/wazuh/wazuh/commit/af53b88b58b99ba775cb41cbec760e4ed38d288f",
        "CWE": "CWE-663",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/wazuh/wazuh/pull/25469"
      }
    ],
    "CWD-1622": [
      {
        "benign_code": {
          "context": "be/src/exprs/agg/percentile_approx.h",
          "class": null,
          "func": null,
          "lines": [
            "public:",
            "    // SplitAggregateRule pass the const args to merge phase aggregator for performance.",
            "    // Compression parameter is always the last constant parameter (if provided)",
            "    double get_compression_factor(FunctionContext* ctx) const override {",
            "                compression = DEFAULT_COMPRESSION_FACTOR;",
            "        int num_args = ctx->get_num_args();",
            "        if (num_args > 3) {",
            "            for (int i = num_args - 1; i >= 0; i--) {",
            "                auto const_col = ctx->get_constant_column(i);",
            "                if (const_col != nullptr) {",
            "                    // Found the last constant column, this should be compression",
            "                    compression = ColumnHelper::get_const_value<TYPE_DOUBLE>(const_col);",
            "                    if (compression < MIN_COMPRESSION || compression > MAX_COMPRESSION) {",
            "                        LOG(WARNING) << \"Compression factor out of range. Using default compression factor: \"",
            "                                     << DEFAULT_COMPRESSION_FACTOR;",
            "                        compression = DEFAULT_COMPRESSION_FACTOR;",
            "                    }",
            "                    break;",
            "                }",
            "            }"
          ]
        },
        "vulnerable_code": {
          "context": "be/src/exprs/agg/percentile_approx.h",
          "class": null,
          "func": null,
          "lines": [
            "        double compression = DEFAULT_COMPRESSION_FACTOR;",
            "        if (ctx->get_num_args() > 3) {",
            "            compression = ColumnHelper::get_const_value<TYPE_DOUBLE>(ctx->get_constant_column(3));",
            "            if (compression < MIN_COMPRESSION || compression > MAX_COMPRESSION) {",
            "                LOG(WARNING) << \"Compression factor out of range. Using default compression factor: \"",
            "                             << DEFAULT_COMPRESSION_FACTOR;",
            "                compression = DEFAULT_COMPRESSION_FACTOR;",
            "        int num_args = ctx->get_num_args();"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/StarRocks/starrocks/commit/daca8fb995b1f024018cec1e6953a36512f39061",
        "CWE": "CWE-1286",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/StarRocks/starrocks/pull/64838"
      }
    ]
  },
  "java": {
    "CWD-1118": [
      {
        "benign_code": {
          "context": "remoting/remoting-triple/src/main/java/com/alipay/sofa/rpc/transport/triple/ReferenceCountManagedChannel.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "import com.alipay.sofa.rpc.log.Logger;",
            "import com.alipay.sofa.rpc.log.LoggerFactory;",
            "import io.grpc.CallOptions;",
            "    private final AtomicInteger referenceCount = new AtomicInteger(0);",
            "    private final static Logger  LOGGER         = LoggerFactory.getLogger(ReferenceCountManagedChannel.class);",
            "",
            "    private ManagedChannel      grpcChannel;",
            "    private final AtomicInteger  referenceCount = new AtomicInteger(0);",
            "",
            "    private final ManagedChannel grpcChannel;",
            "",
            "            return grpcChannel.shutdown();",
            "        int remainReferenceCount = referenceCount.decrementAndGet();",
            "        try {",
            "            if (remainReferenceCount <= 0) {",
            "                ManagedChannel shutdown = grpcChannel.shutdown();",
            "                shutdown.awaitTermination(5, TimeUnit.SECONDS);",
            "                return shutdown;",
            "            }",
            "        } catch (InterruptedException e) {",
            "            LOGGER.warn(\"Triple channel shut down interrupted.\");",
            "        } finally {",
            "            LOGGER.info(\"ReferenceCountManagedChannel {} shutdown remain referenceCount: {}\", this,",
            "                remainReferenceCount);",
            "        }"
          ]
        },
        "vulnerable_code": {
          "context": "remoting/remoting-triple/src/main/java/com/alipay/sofa/rpc/transport/triple/ReferenceCountManagedChannel.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    private final AtomicInteger referenceCount = new AtomicInteger(0);",
            "    private final static Logger  LOGGER         = LoggerFactory.getLogger(ReferenceCountManagedChannel.class);",
            "",
            "    private ManagedChannel      grpcChannel;",
            "    private final AtomicInteger  referenceCount = new AtomicInteger(0);",
            "    public ManagedChannel shutdown() {",
            "        if (referenceCount.decrementAndGet() <= 0) {",
            "            return grpcChannel.shutdown();",
            "        int remainReferenceCount = referenceCount.decrementAndGet();"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/sofastack/sofa-rpc/commit/7e6ca00645547e85dc72f390c26c180ce89c42ae",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/sofastack/sofa-rpc/pull/1521"
      },
      {
        "benign_code": {
          "context": "fe/fe-core/src/main/java/com/starrocks/clone/TabletChecker.java",
          "class": null,
          "func": null,
          "lines": [
            "            if (db == null) {",
            "            if (db == null || dbEntry.getValue().isEmpty()) {",
            "                iter.remove();",
            "                    OlapTable tbl = (OlapTable) GlobalStateMgr.getCurrentState().getLocalMetastore().getTable(db.getId(), tblId);",
            "            for (Map.Entry<Long, Set<PrioPart>> tblEntry : dbEntry.getValue().entrySet()) {",
            "                long tblId = tblEntry.getKey();",
            "                locker.lockTableWithIntensiveDbLock(db.getId(), tblId, LockType.READ);",
            "                try {",
            "                    Table tbl = GlobalStateMgr.getCurrentState().getLocalMetastore().getTable(db.getId(), tblId);",
            "                    if (tbl == null) {",
            "                    if (parts.isEmpty()) {",
            "                    boolean hasAny = tblEntry.getValue().stream()",
            "                            .anyMatch(p -> (tbl.getPhysicalPartition(p.partId) != null && !p.isTimeout()));",
            "                    if (!hasAny) {",
            "                        deletedUrgentTable.add(Pair.create(dbId, tblId));",
            "                    }",
            "                } finally {",
            "                    locker.unLockTableWithIntensiveDbLock(db.getId(), tblId, LockType.READ);",
            "                }",
            "            }",
            "            // NOTE: One-cycle delay in cleanup",
            "            // If all the tables in this db are added into `deletedUrgentTable`, the dbEntry will be empty",
            "            // after the tables are removed from copiedUrgentTable and will be removed in next cycle.",
            "        }",
            "",
            "    /**",
            "     * Get repair tablet information for the specified table.",
            "     *",
            "     * @param dbName database name",
            "     * @param tblName table name",
            "     * @param partitions partition names (null for all partitions)",
            "     * @return repair tablet info containing tablet IDs to repair",
            "     * @throws DdlException if:",
            "     *   - database does not exist",
            "     *   - table does not exist or is not OLAP table",
            "     *   - table was dropped and recreated between initial lookup and lock acquisition (retry recommended)",
            "     *   - partition does not exist",
            "     */",
            "    public static RepairTabletInfo getRepairTabletInfo(String dbName, String tblName, List<String> partitions)",
            "        }",
            "        Table table = db.getTable(tblName);",
            "        if (table == null) {",
            "            throw new DdlException(\"Table \" + tblName + \" does not exist\");",
            "        }",
            "",
            "        long tblId;",
            "        long tblId = table.getId();",
            "        List<Long> partIds = Lists.newArrayList();",
            "        locker.lockDatabase(db.getId(), LockType.READ);",
            "        locker.lockTableWithIntensiveDbLock(dbId, tblId, LockType.READ);",
            "        try {",
            "            tblId = tbl.getId();",
            "            if (tbl.getId() != tblId) {",
            "                throw new DdlException(\"Table \" + tblName + \" was recreated during the operation, please retry\");",
            "            }",
            "            OlapTable olapTable = (OlapTable) tbl;",
            "            locker.unLockDatabase(db.getId(), LockType.READ);",
            "            locker.unLockTableWithIntensiveDbLock(dbId, tblId, LockType.READ);",
            "        }"
          ]
        },
        "vulnerable_code": {
          "context": "fe/fe-core/src/main/java/com/starrocks/clone/TabletChecker.java",
          "class": null,
          "func": null,
          "lines": [
            "            Database db = GlobalStateMgr.getCurrentState().getLocalMetastore().getDb(dbId);",
            "            if (db == null) {",
            "            if (db == null || dbEntry.getValue().isEmpty()) {",
            "            Locker locker = new Locker();",
            "            locker.lockDatabase(db.getId(), LockType.READ);",
            "            try {",
            "                for (Map.Entry<Long, Set<PrioPart>> tblEntry : dbEntry.getValue().entrySet()) {",
            "                    long tblId = tblEntry.getKey();",
            "                    OlapTable tbl = (OlapTable) GlobalStateMgr.getCurrentState().getLocalMetastore().getTable(db.getId(), tblId);",
            "            for (Map.Entry<Long, Set<PrioPart>> tblEntry : dbEntry.getValue().entrySet()) {",
            "",
            "                    Set<PrioPart> parts = tblEntry.getValue();",
            "                    parts = parts.stream().filter(p -> (tbl.getPhysicalPartition(p.partId) != null && !p.isTimeout())).collect(",
            "                            Collectors.toSet());",
            "                    if (parts.isEmpty()) {",
            "                    boolean hasAny = tblEntry.getValue().stream()",
            "                }",
            "",
            "                if (dbEntry.getValue().isEmpty()) {",
            "                    iter.remove();",
            "                }",
            "            } finally {",
            "                locker.unLockDatabase(db.getId(), LockType.READ);",
            "            }",
            "        long dbId = db.getId();",
            "        long tblId;",
            "        long tblId = table.getId();",
            "        Locker locker = new Locker();",
            "        locker.lockDatabase(db.getId(), LockType.READ);",
            "        locker.lockTableWithIntensiveDbLock(dbId, tblId, LockType.READ);",
            "            }",
            "",
            "            tblId = tbl.getId();",
            "            if (tbl.getId() != tblId) {",
            "        } finally {",
            "            locker.unLockDatabase(db.getId(), LockType.READ);",
            "            locker.unLockTableWithIntensiveDbLock(dbId, tblId, LockType.READ);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/StarRocks/starrocks/commit/250a93cb813f5ee4ca18949d9bf9c6074136a006",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/StarRocks/starrocks/pull/65312"
      },
      {
        "benign_code": {
          "context": "agents-common/src/main/java/org/apache/ranger/plugin/service/RangerBasePlugin.java",
          "class": null,
          "func": null,
          "lines": [
            "    private       boolean                     synchronousPolicyRefresh;",
            "    private final RangerPluginConfig        pluginConfig;",
            "    private final RangerPluginContext       pluginContext;",
            "    private final Map<String, LogHistory>   logHistoryList = new Hashtable<>();",
            "    private final int                       logInterval    = 30000; // 30 seconds",
            "    private final DownloadTrigger           accessTrigger  = new DownloadTrigger();",
            "    private final List<RangerChainedPlugin> chainedPlugins;",
            "    private final boolean                   dedupStrings;",
            "",
            "    private volatile RangerPolicyEngine  policyEngine;",
            "    private volatile RangerAuthContext   currentAuthContext;",
            "    private volatile RangerRoles         roles;",
            "    private volatile Map<String, String> serviceConfigs;",
            "",
            "    private PolicyRefresher             refresher;",
            "    private RangerAccessResultProcessor resultProcessor;",
            "    private boolean                     isUserStoreEnricherAddedImplcitly;",
            "    private boolean                     synchronousPolicyRefresh;",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "agents-common/src/main/java/org/apache/ranger/plugin/service/RangerBasePlugin.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    private final RangerPluginConfig          pluginConfig;",
            "    private final RangerPluginContext         pluginContext;",
            "    private final Map<String, LogHistory>     logHistoryList = new Hashtable<>();",
            "    private final int                         logInterval    = 30000; // 30 seconds",
            "    private final DownloadTrigger             accessTrigger  = new DownloadTrigger();",
            "    private final List<RangerChainedPlugin>   chainedPlugins;",
            "    private final boolean                     dedupStrings;",
            "    private       PolicyRefresher             refresher;",
            "    private       RangerPolicyEngine          policyEngine;",
            "    private       RangerAuthContext           currentAuthContext;",
            "    private       RangerAccessResultProcessor resultProcessor;",
            "    private       RangerRoles                 roles;",
            "    private       boolean                     isUserStoreEnricherAddedImplcitly;",
            "    private       Map<String, String>         serviceConfigs;",
            "    private       boolean                     synchronousPolicyRefresh;",
            "    private final RangerPluginConfig        pluginConfig;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/apache/ranger/commit/4fb301152ed2ba72596419ebdda90f8a5912e787",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/apache/ranger/pull/729"
      },
      {
        "benign_code": {
          "context": "tests/org.eclipse.ui.tests/Eclipse UI Tests/org/eclipse/ui/tests/activities/DynamicTest.java",
          "class": null,
          "func": null,
          "lines": [
            "import java.util.Set;",
            "import java.util.concurrent.atomic.AtomicBoolean;",
            "",
            "\t\tfinal boolean[] registryChanged = new boolean[] { false, false };",
            "\t\tfinal AtomicBoolean activityChanged = new AtomicBoolean(false);",
            "\t\tfinal AtomicBoolean categoryChanged = new AtomicBoolean(false);",
            "\t\tactivity.addActivityListener(activityEvent -> {",
            "\t\t\tregistryChanged[0] = true;",
            "\t\t\tactivityChanged.set(true);",
            "\t\t});",
            "\t\t\tregistryChanged[1] = true;",
            "\t\t\tcategoryChanged.set(true);",
            "",
            "\t\t\t\t() -> registryChanged[0] && registryChanged[1]);",
            "\t\t\t\t() -> activityChanged.get() && categoryChanged.get());",
            "",
            "\t\tassertTrue(\"Category Listener not called\", registryChanged[1]);",
            "\t\tassertTrue(\"Activity Listener not called\", activityChanged.get());",
            "\t\tassertTrue(\"Category Listener not called\", categoryChanged.get());",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "tests/org.eclipse.ui.tests/Eclipse UI Tests/org/eclipse/ui/tests/activities/DynamicTest.java",
          "class": null,
          "func": null,
          "lines": [
            "\t\t// fired",
            "\t\tfinal boolean[] registryChanged = new boolean[] { false, false };",
            "\t\tfinal AtomicBoolean activityChanged = new AtomicBoolean(false);",
            "\t\t\tSystem.err.println(\"activityChanged\");",
            "\t\t\tregistryChanged[0] = true;",
            "\t\t\tactivityChanged.set(true);",
            "\t\t\tSystem.err.println(\"categoryChanged\");",
            "\t\t\tregistryChanged[1] = true;",
            "\t\t\tcategoryChanged.set(true);",
            "\t\tDisplayHelper.waitForCondition(PlatformUI.getWorkbench().getDisplay(), 20000,",
            "\t\t\t\t() -> registryChanged[0] && registryChanged[1]);",
            "\t\t\t\t() -> activityChanged.get() && categoryChanged.get());",
            "",
            "\t\tassertTrue(\"Activity Listener not called\", registryChanged[0]);",
            "\t\tassertTrue(\"Category Listener not called\", registryChanged[1]);",
            "\t\tassertTrue(\"Activity Listener not called\", activityChanged.get());"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/eclipse-platform/eclipse.platform.ui/commit/dd5d18efda4811c70049f295391534d0b32f73e0",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/eclipse-platform/eclipse.platform.ui/pull/3531"
      },
      {
        "benign_code": {
          "context": "MekHQ/src/mekhq/campaign/io/CampaignXmlParser.java",
          "class": null,
          "func": null,
          "lines": [
            "            }",
            "",
            "            List<String> reports = unit.checkForOverCrewing();",
            "            for (String report : reports) {",
            "                campaign.addReport(report);",
            "            }",
            "        });"
          ]
        },
        "vulnerable_code": {
          "context": "MekHQ/src/mekhq/campaign/io/CampaignXmlParser.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "            List<String> reports = unit.checkForOverCrewing();",
            "            for (String report : reports) {",
            "                campaign.addReport(report);",
            "            }",
            "",
            "            // reset the pilot and entity, to reflect newly assigned personnel"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/MegaMek/mekhq/commit/5a5f3066da1797bc5989b5a730a4063805ffaa5d",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/MegaMek/mekhq/pull/7921"
      }
    ],
    "CWD-1123": [
      {
        "benign_code": {
          "context": "remoting/remoting-triple/src/main/java/com/alipay/sofa/rpc/transport/triple/ReferenceCountManagedChannel.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "import com.alipay.sofa.rpc.log.Logger;",
            "import com.alipay.sofa.rpc.log.LoggerFactory;",
            "import io.grpc.CallOptions;",
            "    private final AtomicInteger referenceCount = new AtomicInteger(0);",
            "    private final static Logger  LOGGER         = LoggerFactory.getLogger(ReferenceCountManagedChannel.class);",
            "",
            "    private ManagedChannel      grpcChannel;",
            "    private final AtomicInteger  referenceCount = new AtomicInteger(0);",
            "",
            "    private final ManagedChannel grpcChannel;",
            "",
            "            return grpcChannel.shutdown();",
            "        int remainReferenceCount = referenceCount.decrementAndGet();",
            "        try {",
            "            if (remainReferenceCount <= 0) {",
            "                ManagedChannel shutdown = grpcChannel.shutdown();",
            "                shutdown.awaitTermination(5, TimeUnit.SECONDS);",
            "                return shutdown;",
            "            }",
            "        } catch (InterruptedException e) {",
            "            LOGGER.warn(\"Triple channel shut down interrupted.\");",
            "        } finally {",
            "            LOGGER.info(\"ReferenceCountManagedChannel {} shutdown remain referenceCount: {}\", this,",
            "                remainReferenceCount);",
            "        }"
          ]
        },
        "vulnerable_code": {
          "context": "remoting/remoting-triple/src/main/java/com/alipay/sofa/rpc/transport/triple/ReferenceCountManagedChannel.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    private final AtomicInteger referenceCount = new AtomicInteger(0);",
            "    private final static Logger  LOGGER         = LoggerFactory.getLogger(ReferenceCountManagedChannel.class);",
            "",
            "    private ManagedChannel      grpcChannel;",
            "    private final AtomicInteger  referenceCount = new AtomicInteger(0);",
            "    public ManagedChannel shutdown() {",
            "        if (referenceCount.decrementAndGet() <= 0) {",
            "            return grpcChannel.shutdown();",
            "        int remainReferenceCount = referenceCount.decrementAndGet();"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/sofastack/sofa-rpc/commit/7e6ca00645547e85dc72f390c26c180ce89c42ae",
        "CWE": "CWE-413",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/sofastack/sofa-rpc/pull/1521"
      },
      {
        "benign_code": {
          "context": "netty/src/main/java/io/grpc/netty/NettyClientHandler.java",
          "class": null,
          "func": null,
          "lines": [
            "                stream.setHttp2Stream(http2Stream);",
            "                promise.setSuccess();",
            "              } else {",
            "                // Otherwise, the stream has been cancelled and Netty is sending a",
            "                // RST_STREAM frame which causes it to purge pending writes from the",
            "                // flow-controller and delete the http2Stream. The stream listener has already",
            "                // been notified of cancellation so there is nothing to do.",
            "                //",
            "                // This process has been observed to fail in some circumstances, leaving listeners",
            "                // unanswered. Ensure that some exception has been delivered consistent with the",
            "                // implied RST_STREAM result above.",
            "                Status status = Status.INTERNAL.withDescription(\"unknown stream for connection\");",
            "                promise.setFailure(status.asRuntimeException());",
            "              }"
          ]
        },
        "vulnerable_code": {
          "context": "netty/src/main/java/io/grpc/netty/NettyClientHandler.java",
          "class": null,
          "func": null,
          "lines": [
            "              }",
            "              // Otherwise, the stream has been cancelled and Netty is sending a",
            "              // RST_STREAM frame which causes it to purge pending writes from the",
            "              // flow-controller and delete the http2Stream. The stream listener has already",
            "              // been notified of cancellation so there is nothing to do.",
            "",
            "              // Just forward on the success status to the original promise.",
            "              promise.setSuccess();",
            "            } else {"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/grpc/grpc-java/commit/a37d3eb349e048b953633027ed011cda8b68c603",
        "CWE": "CWE-413",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/grpc/grpc-java/pull/12207"
      }
    ],
    "CWD-1124": [
      {
        "benign_code": {
          "context": "common/src/main/java/dev/ftb/mods/ftbquests/command/FTBQuestsCommands.java",
          "class": null,
          "func": null,
          "lines": [
            "\t\t\t\t.then(Commands.literal(\"editing_mode\")",
            "\t\t\t\t\t\t.requires(FTBQuestsCommands::isSSPOrEditor)",
            "\t\t\t\t\t\t.executes(c -> editingMode(c.getSource(), c.getSource().getPlayerOrException(), null))",
            "\t\t\t\t.then(Commands.literal(\"locked\")",
            "\t\t\t\t\t\t.requires(FTBQuestsCommands::hasEditorPermission)",
            "\t\t\t\t\t\t.executes(c -> locked(c.getSource(), c.getSource().getPlayerOrException(), null))",
            "\t\t\t\t.then(Commands.literal(\"delete_empty_reward_tables\")",
            "\t\t\t\t\t\t.requires(FTBQuestsCommands::hasEditorPermission)",
            "\t\t\t\t\t\t.executes(context -> deleteEmptyRewardTables(context.getSource()))",
            "\t\t\t\t.then(Commands.literal(\"generate_chapter_with_all_items_in_game\")",
            "\t\t\t\t\t\t.requires(FTBQuestsCommands::hasEditorPermission)",
            "\t\t\t\t\t\t.executes(context -> generateAllItemChapter(context.getSource()))",
            "\t\t\t\t.then(Commands.literal(\"block_rewards\")",
            "\t\t\t\t\t\t.requires(FTBQuestsCommands::hasEditorPermission)",
            "\t\t\t\t\t\t.executes(c -> toggleRewardBlocking(c.getSource(), c.getSource().getPlayerOrException(), null))",
            "",
            "\tprivate static boolean isSSPOrEditor(CommandSourceStack s) {",
            "\t\t// s.getServer() *can* be null here, whatever the IDE thinks!",
            "\t\t//noinspection ConstantValue",
            "\t\treturn s.getServer() != null && s.getServer().isSingleplayer() || hasEditorPermission(s);",
            "\t}",
            "",
            "\tprivate static boolean hasEditorPermission(CommandSourceStack stack) {"
          ]
        },
        "vulnerable_code": {
          "context": "common/src/main/java/dev/ftb/mods/ftbquests/command/FTBQuestsCommands.java",
          "class": null,
          "func": null,
          "lines": [
            "\t\tdispatcher.register(Commands.literal(\"ftbquests\")",
            "\t\t\t\t// s.getServer() *can* be null here, whatever the IDE thinks!",
            "\t\t\t\t.requires(s -> s.getServer() != null && s.getServer().isSingleplayer() || hasEditorPermission(s))",
            "\t\t\t\t.then(Commands.literal(\"editing_mode\")"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/FTBTeam/FTB-Quests/commit/006a35c9d3595ff0d7d1f5b6fecd33ed04bec63a",
        "CWE": "CWE-414",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/FTBTeam/FTB-Quests/pull/798"
      },
      {
        "benign_code": {
          "context": "fe/fe-core/src/main/java/com/starrocks/clone/TabletChecker.java",
          "class": null,
          "func": null,
          "lines": [
            "            if (db == null) {",
            "            if (db == null || dbEntry.getValue().isEmpty()) {",
            "                iter.remove();",
            "                    OlapTable tbl = (OlapTable) GlobalStateMgr.getCurrentState().getLocalMetastore().getTable(db.getId(), tblId);",
            "            for (Map.Entry<Long, Set<PrioPart>> tblEntry : dbEntry.getValue().entrySet()) {",
            "                long tblId = tblEntry.getKey();",
            "                locker.lockTableWithIntensiveDbLock(db.getId(), tblId, LockType.READ);",
            "                try {",
            "                    Table tbl = GlobalStateMgr.getCurrentState().getLocalMetastore().getTable(db.getId(), tblId);",
            "                    if (tbl == null) {",
            "                    if (parts.isEmpty()) {",
            "                    boolean hasAny = tblEntry.getValue().stream()",
            "                            .anyMatch(p -> (tbl.getPhysicalPartition(p.partId) != null && !p.isTimeout()));",
            "                    if (!hasAny) {",
            "                        deletedUrgentTable.add(Pair.create(dbId, tblId));",
            "                    }",
            "                } finally {",
            "                    locker.unLockTableWithIntensiveDbLock(db.getId(), tblId, LockType.READ);",
            "                }",
            "            }",
            "            // NOTE: One-cycle delay in cleanup",
            "            // If all the tables in this db are added into `deletedUrgentTable`, the dbEntry will be empty",
            "            // after the tables are removed from copiedUrgentTable and will be removed in next cycle.",
            "        }",
            "",
            "    /**",
            "     * Get repair tablet information for the specified table.",
            "     *",
            "     * @param dbName database name",
            "     * @param tblName table name",
            "     * @param partitions partition names (null for all partitions)",
            "     * @return repair tablet info containing tablet IDs to repair",
            "     * @throws DdlException if:",
            "     *   - database does not exist",
            "     *   - table does not exist or is not OLAP table",
            "     *   - table was dropped and recreated between initial lookup and lock acquisition (retry recommended)",
            "     *   - partition does not exist",
            "     */",
            "    public static RepairTabletInfo getRepairTabletInfo(String dbName, String tblName, List<String> partitions)",
            "        }",
            "        Table table = db.getTable(tblName);",
            "        if (table == null) {",
            "            throw new DdlException(\"Table \" + tblName + \" does not exist\");",
            "        }",
            "",
            "        long tblId;",
            "        long tblId = table.getId();",
            "        List<Long> partIds = Lists.newArrayList();",
            "        locker.lockDatabase(db.getId(), LockType.READ);",
            "        locker.lockTableWithIntensiveDbLock(dbId, tblId, LockType.READ);",
            "        try {",
            "            tblId = tbl.getId();",
            "            if (tbl.getId() != tblId) {",
            "                throw new DdlException(\"Table \" + tblName + \" was recreated during the operation, please retry\");",
            "            }",
            "            OlapTable olapTable = (OlapTable) tbl;",
            "            locker.unLockDatabase(db.getId(), LockType.READ);",
            "            locker.unLockTableWithIntensiveDbLock(dbId, tblId, LockType.READ);",
            "        }"
          ]
        },
        "vulnerable_code": {
          "context": "fe/fe-core/src/main/java/com/starrocks/clone/TabletChecker.java",
          "class": null,
          "func": null,
          "lines": [
            "            Database db = GlobalStateMgr.getCurrentState().getLocalMetastore().getDb(dbId);",
            "            if (db == null) {",
            "            if (db == null || dbEntry.getValue().isEmpty()) {",
            "            Locker locker = new Locker();",
            "            locker.lockDatabase(db.getId(), LockType.READ);",
            "            try {",
            "                for (Map.Entry<Long, Set<PrioPart>> tblEntry : dbEntry.getValue().entrySet()) {",
            "                    long tblId = tblEntry.getKey();",
            "                    OlapTable tbl = (OlapTable) GlobalStateMgr.getCurrentState().getLocalMetastore().getTable(db.getId(), tblId);",
            "            for (Map.Entry<Long, Set<PrioPart>> tblEntry : dbEntry.getValue().entrySet()) {",
            "",
            "                    Set<PrioPart> parts = tblEntry.getValue();",
            "                    parts = parts.stream().filter(p -> (tbl.getPhysicalPartition(p.partId) != null && !p.isTimeout())).collect(",
            "                            Collectors.toSet());",
            "                    if (parts.isEmpty()) {",
            "                    boolean hasAny = tblEntry.getValue().stream()",
            "                }",
            "",
            "                if (dbEntry.getValue().isEmpty()) {",
            "                    iter.remove();",
            "                }",
            "            } finally {",
            "                locker.unLockDatabase(db.getId(), LockType.READ);",
            "            }",
            "        long dbId = db.getId();",
            "        long tblId;",
            "        long tblId = table.getId();",
            "        Locker locker = new Locker();",
            "        locker.lockDatabase(db.getId(), LockType.READ);",
            "        locker.lockTableWithIntensiveDbLock(dbId, tblId, LockType.READ);",
            "            }",
            "",
            "            tblId = tbl.getId();",
            "            if (tbl.getId() != tblId) {",
            "        } finally {",
            "            locker.unLockDatabase(db.getId(), LockType.READ);",
            "            locker.unLockTableWithIntensiveDbLock(dbId, tblId, LockType.READ);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/StarRocks/starrocks/commit/250a93cb813f5ee4ca18949d9bf9c6074136a006",
        "CWE": "CWE-414",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/StarRocks/starrocks/pull/65312"
      }
    ],
    "CWD-1125": [
      {
        "benign_code": {
          "context": "platforms/core-runtime/service-registry-impl/src/main/java/org/gradle/internal/service/DefaultServiceRegistry.java",
          "class": null,
          "func": null,
          "lines": [
            "import java.util.concurrent.atomic.AtomicReference;",
            "import java.util.function.Supplier;",
            "",
            "            if (instance instanceof AnnotatedServiceLifecycleHandler && !isAssignableFromAnyType(AnnotatedServiceLifecycleHandler.class, serviceProvider.getDeclaredServiceTypes())) {",
            "        public void instanceRealized(List<Class<?>> declaredServiceTypes, Supplier<String> displayName, Object instance) {",
            "            if (instance instanceof AnnotatedServiceLifecycleHandler && !isAssignableFromAnyType(AnnotatedServiceLifecycleHandler.class, declaredServiceTypes)) {",
            "                throw new IllegalStateException(String.format(\"%s implements %s but is not declared as a service of this type. This service is declared as having %s.\",",
            "                    serviceProvider.getDisplayName(), AnnotatedServiceLifecycleHandler.class.getSimpleName(), format(\"type\", declaredServiceTypes)));",
            "                    displayName.get(), AnnotatedServiceLifecycleHandler.class.getSimpleName(), format(\"type\", declaredServiceTypes)));",
            "            }",
            "                            serviceProvider.getDisplayName(), format(annotation), format(\"type\", declaredServiceTypes)));",
            "                            displayName.get(), format(annotation), format(\"type\", declaredServiceTypes)));",
            "                    }",
            "",
            "        protected void instanceRealized(Object instance) {",
            "            owner.ownServices.instanceRealized(getDeclaredServiceTypes(), this::getDisplayName, instance);",
            "        }",
            "",
            "        protected void setInstance(Object instance) {",
            "            instanceRealized(instance);",
            "            // Only expose the instance after we're done with initialization.",
            "            this.instance = instance;",
            "        public List<Class<?>> getDeclaredServiceTypes() {",
            "        List<Class<?>> getDeclaredServiceTypes() {",
            "            return serviceTypesAsClasses;",
            "",
            "        private String getDisplayNameImpl(Object serviceInstance) {",
            "            return format(\"Service\", serviceTypes) + \" with implementation \" + format(serviceInstance.getClass());",
            "        }",
            "",
            "        @Override",
            "        protected void instanceRealized(Object instance) {",
            "            owner.ownServices.instanceRealized(getDeclaredServiceTypes(), () -> getDisplayNameImpl(instance), instance);",
            "        }",
            "",
            "        @Override",
            "            return format(\"Service\", serviceTypes) + \" with implementation \" + format(getInstance().getClass());",
            "            return getDisplayNameImpl(getInstance());",
            "        }"
          ]
        },
        "vulnerable_code": {
          "context": "platforms/core-runtime/service-registry-impl/src/main/java/org/gradle/internal/service/DefaultServiceRegistry.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "        public void instanceRealized(ManagedObjectServiceProvider serviceProvider, Object instance) {",
            "            List<Class<?>> declaredServiceTypes = serviceProvider.getDeclaredServiceTypes();",
            "            if (instance instanceof AnnotatedServiceLifecycleHandler && !isAssignableFromAnyType(AnnotatedServiceLifecycleHandler.class, serviceProvider.getDeclaredServiceTypes())) {",
            "        public void instanceRealized(List<Class<?>> declaredServiceTypes, Supplier<String> displayName, Object instance) {",
            "                throw new IllegalStateException(String.format(\"%s implements %s but is not declared as a service of this type. This service is declared as having %s.\",",
            "                    serviceProvider.getDisplayName(), AnnotatedServiceLifecycleHandler.class.getSimpleName(), format(\"type\", declaredServiceTypes)));",
            "                    displayName.get(), AnnotatedServiceLifecycleHandler.class.getSimpleName(), format(\"type\", declaredServiceTypes)));",
            "                        throw new IllegalStateException(String.format(\"%s is annotated with @%s but is not declared as a service with this annotation. This service is declared as having %s.\",",
            "                            serviceProvider.getDisplayName(), format(annotation), format(\"type\", declaredServiceTypes)));",
            "                            displayName.get(), format(annotation), format(\"type\", declaredServiceTypes)));",
            "            this.instance = instance;",
            "            owner.ownServices.instanceRealized(this, instance);",
            "        }",
            "        @Override",
            "        public List<Class<?>> getDeclaredServiceTypes() {",
            "        List<Class<?>> getDeclaredServiceTypes() {",
            "        public String getDisplayName() {",
            "            return format(\"Service\", serviceTypes) + \" with implementation \" + format(getInstance().getClass());",
            "            return getDisplayNameImpl(getInstance());"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/gradle/gradle/commit/da722291e6ac30129d23d368bead8710a9cd3ca5",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/gradle/gradle/pull/35629"
      },
      {
        "benign_code": {
          "context": "language-textmate/src/main/java/org/eclipse/tm4e/core/internal/grammar/Grammar.java",
          "class": null,
          "func": null,
          "lines": [
            "\tprivate RuleId _rootId;",
            "\tprivate volatile RuleId _rootId;",
            "\tprivate int _lastRuleId = 0;",
            "\t\t}",
            "",
            "        if (this._rootId == null) {",
            "            synchronized (this) {",
            "                if (this._rootId == null) {",
            "                    this._rootId = RuleFactory.getCompiledRuleId(",
            "                            this._grammar.getRepository().getSelf(),",
            "                            this,",
            "                            this._grammar.getRepository());",
            "                    // This ensures ids are deterministic, and thus equal in renderer and webworker.",
            "                    this.getInjections();",
            "                }",
            "            }",
            "        }",
            "",
            "        var rootId = this._rootId;",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "language-textmate/src/main/java/org/eclipse/tm4e/core/internal/grammar/Grammar.java",
          "class": null,
          "func": null,
          "lines": [
            "\t@Nullable",
            "\tprivate RuleId _rootId;",
            "\tprivate volatile RuleId _rootId;",
            "\t\t\t@Nullable final Duration timeLimit) {",
            "\t\tvar rootId = this._rootId;",
            "\t\tif (rootId == null) {",
            "\t\t\trootId = this._rootId = RuleFactory.getCompiledRuleId(",
            "\t\t\t\t\tthis._grammar.getRepository().getSelf(),",
            "\t\t\t\t\tthis,",
            "\t\t\t\t\tthis._grammar.getRepository());",
            "\t\t\t// This ensures ids are deterministic, and thus equal in renderer and webworker.",
            "\t\t\tthis.getInjections();",
            "\t\t}",
            ""
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/Rosemoe/sora-editor/commit/acc2abc58ccc4c5a803b2a97ed258c35f9d43111",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/Rosemoe/sora-editor/pull/736"
      },
      {
        "benign_code": {
          "context": "core/src/main/java/com/alibaba/nacos/core/remote/grpc/GrpcConnection.java",
          "class": null,
          "func": null,
          "lines": [
            "    private static TpsControlManager tpsControlManager;",
            "    private static volatile TpsControlManager tpsControlManager;",
            "    ",
            "                    synchronized (GrpcConnection.class.getClass()) {",
            "                    synchronized (GrpcConnection.class) {",
            "                        if (tpsControlManager == null) {"
          ]
        },
        "vulnerable_code": {
          "context": "core/src/main/java/com/alibaba/nacos/core/remote/grpc/GrpcConnection.java",
          "class": null,
          "func": null,
          "lines": [
            "    ",
            "    private static TpsControlManager tpsControlManager;",
            "    private static volatile TpsControlManager tpsControlManager;",
            "                if (tpsControlManager == null) {",
            "                    synchronized (GrpcConnection.class.getClass()) {",
            "                    synchronized (GrpcConnection.class) {"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/alibaba/nacos/commit/d964cd78706761e2d465d2dcee4e115068592413",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/alibaba/nacos/pull/13882"
      },
      {
        "benign_code": {
          "context": "languagetool-core/src/main/java/org/languagetool/Language.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "  private final Object patternRuleLock = new Object();",
            "  private final Object disambiguatorLock = new Object();",
            "  private final Object sentenceTokenizerLock = new Object();",
            "  private final Object wordTokenizerLock = new Object();",
            "",
            "  private final UnifierConfiguration unifierConfig = new UnifierConfiguration();",
            "  private Disambiguator disambiguator;",
            "  private volatile List<AbstractPatternRule> patternRules;",
            "  private volatile Disambiguator disambiguator;",
            "  private Tagger tagger;",
            "  private Tokenizer wordTokenizer;",
            "  private volatile SentenceTokenizer sentenceTokenizer;",
            "  private volatile Tokenizer wordTokenizer;",
            "  private Chunker chunker;",
            "  public synchronized Disambiguator getDisambiguator() {",
            "  public Disambiguator getDisambiguator() {",
            "    if (disambiguator == null) {",
            "      disambiguator = createDefaultDisambiguator();",
            "      synchronized (disambiguatorLock) {",
            "        if (disambiguator == null) {",
            "          disambiguator = createDefaultDisambiguator();",
            "        }",
            "      }",
            "    }",
            "  public synchronized SentenceTokenizer getSentenceTokenizer() {",
            "  public SentenceTokenizer getSentenceTokenizer() {",
            "    if (sentenceTokenizer == null) {",
            "      sentenceTokenizer = createDefaultSentenceTokenizer();",
            "      synchronized (sentenceTokenizerLock) {",
            "        if (sentenceTokenizer == null) {",
            "          sentenceTokenizer = createDefaultSentenceTokenizer();",
            "        }",
            "      }",
            "    }",
            "  public synchronized Tokenizer getWordTokenizer() {",
            "  public Tokenizer getWordTokenizer() {",
            "    if (wordTokenizer == null) {",
            "      wordTokenizer = createDefaultWordTokenizer();",
            "      synchronized (wordTokenizerLock) {",
            "        if (wordTokenizer == null) {",
            "          wordTokenizer = createDefaultWordTokenizer();",
            "        }",
            "      }",
            "    }",
            "  protected synchronized List<AbstractPatternRule> getPatternRules() throws IOException {",
            "  protected List<AbstractPatternRule> getPatternRules() throws IOException {",
            "    // use lazy loading to speed up server use case and start of stand-alone LT, where all the languages get initialized:",
            "              }",
            "      synchronized (patternRuleLock) {",
            "        if (patternRules == null) {",
            "          patternRules = initializePatternRules();",
            "        }",
            "      }",
            "    }",
            "    return patternRules;",
            "  }",
            "",
            "  private List<AbstractPatternRule> initializePatternRules() throws IOException {",
            "    List<AbstractPatternRule> rules = new ArrayList<>();",
            "    PatternRuleLoader ruleLoader = new PatternRuleLoader();",
            "    for (String fileName : getRuleFileNames()) {",
            "      InputStream is = null;",
            "      try {",
            "        is = JLanguageTool.getDataBroker().getAsStream(fileName);",
            "        boolean ignore = false;",
            "        if (is == null) {                     // files loaded via the dialog",
            "          try {",
            "            is = new FileInputStream(fileName);",
            "          } catch (FileNotFoundException e) {",
            "            if (fileName.contains(\"-test-\")) {",
            "              // ignore, used for testing",
            "              ignore = true;",
            "            } else {",
            "              throw e;",
            "            }",
            "          }",
            "        }",
            "        if (!ignore) {",
            "          rules.addAll(ruleLoader.getRules(is, fileName, this));",
            "        }",
            "      } finally {",
            "        if (is != null) {",
            "          is.close();",
            "        }",
            "    return patternRules;",
            "    return Collections.unmodifiableList(rules);",
            "  }"
          ]
        },
        "vulnerable_code": {
          "context": "languagetool-core/src/main/java/org/languagetool/Language.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "  private List<AbstractPatternRule> patternRules;",
            "",
            "  private Disambiguator disambiguator;",
            "  private volatile List<AbstractPatternRule> patternRules;",
            "  private Tagger tagger;",
            "  private SentenceTokenizer sentenceTokenizer;",
            "  private Tokenizer wordTokenizer;",
            "  private volatile SentenceTokenizer sentenceTokenizer;",
            "   */",
            "  public synchronized Disambiguator getDisambiguator() {",
            "  public Disambiguator getDisambiguator() {",
            "    if (disambiguator == null) {",
            "      disambiguator = createDefaultDisambiguator();",
            "      synchronized (disambiguatorLock) {",
            "   */",
            "  public synchronized SentenceTokenizer getSentenceTokenizer() {",
            "  public SentenceTokenizer getSentenceTokenizer() {",
            "    if (sentenceTokenizer == null) {",
            "      sentenceTokenizer = createDefaultSentenceTokenizer();",
            "      synchronized (sentenceTokenizerLock) {",
            "   */",
            "  public synchronized Tokenizer getWordTokenizer() {",
            "  public Tokenizer getWordTokenizer() {",
            "    if (wordTokenizer == null) {",
            "      wordTokenizer = createDefaultWordTokenizer();",
            "      synchronized (wordTokenizerLock) {",
            "  @SuppressWarnings(\"resource\")",
            "  protected synchronized List<AbstractPatternRule> getPatternRules() throws IOException {",
            "  protected List<AbstractPatternRule> getPatternRules() throws IOException {",
            "    if (patternRules == null) {",
            "      List<AbstractPatternRule> rules = new ArrayList<>();",
            "      PatternRuleLoader ruleLoader = new PatternRuleLoader();",
            "      for (String fileName : getRuleFileNames()) {",
            "        InputStream is = null;",
            "        try {",
            "          is = JLanguageTool.getDataBroker().getAsStream(fileName);",
            "          boolean ignore = false;",
            "          if (is == null) {                     // files loaded via the dialog",
            "            try {",
            "              is = new FileInputStream(fileName);",
            "            } catch (FileNotFoundException e) {",
            "              if (fileName.contains(\"-test-\")) {",
            "                // ignore, used for testing",
            "                ignore = true;",
            "              } else {",
            "                throw e;",
            "              }",
            "      synchronized (patternRuleLock) {",
            "          }",
            "          if (!ignore) {",
            "            rules.addAll(ruleLoader.getRules(is, fileName, this));",
            "            patternRules = Collections.unmodifiableList(rules);",
            "          }",
            "        } finally {",
            "          if (is != null) {",
            "            is.close();",
            "          }",
            "        }",
            "    }",
            "    return patternRules;",
            "    return Collections.unmodifiableList(rules);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/languagetool-org/languagetool/commit/e989acfa9dfa45059ee269c48f9447cb98d7259f",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/languagetool-org/languagetool/pull/11550"
      },
      {
        "benign_code": {
          "context": "sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/CachingFactory.java",
          "class": null,
          "func": null,
          "lines": [
            "import org.checkerframework.checker.initialization.qual.UnknownInitialization;",
            "import org.checkerframework.checker.nullness.qual.MonotonicNonNull;",
            "import org.checkerframework.checker.nullness.qual.NonNull;",
            "  private transient @Nullable ConcurrentHashMap<TypeDescriptor<?>, CreatedT> cache = null;",
            "  private transient volatile @MonotonicNonNull ConcurrentHashMap<TypeDescriptor<?>, CreatedT>",
            "      cache = null;",
            "",
            "      cache = new ConcurrentHashMap<>();",
            "    ConcurrentHashMap<TypeDescriptor<?>, CreatedT> value = cache;",
            "    if (value == null) {",
            "      synchronized (this) {",
            "        value = cache;",
            "        if (value == null) {",
            "          cache = value = new ConcurrentHashMap<>();",
            "        }",
            "      }",
            "    }",
            "    return cache;",
            "    return value;",
            "  }"
          ]
        },
        "vulnerable_code": {
          "context": "sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/CachingFactory.java",
          "class": null,
          "func": null,
          "lines": [
            "public class CachingFactory<CreatedT extends @NonNull Object> implements Factory<CreatedT> {",
            "  private transient @Nullable ConcurrentHashMap<TypeDescriptor<?>, CreatedT> cache = null;",
            "  private transient volatile @MonotonicNonNull ConcurrentHashMap<TypeDescriptor<?>, CreatedT>",
            "  private ConcurrentHashMap<TypeDescriptor<?>, CreatedT> getCache() {",
            "    if (cache == null) {",
            "      cache = new ConcurrentHashMap<>();",
            "    ConcurrentHashMap<TypeDescriptor<?>, CreatedT> value = cache;",
            "    }",
            "    return cache;",
            "    return value;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/apache/beam/commit/5593f405cbef0fbad506c2a8726548628d53e120",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/apache/beam/pull/35752"
      },
      {
        "benign_code": {
          "context": "rhino/src/main/java/org/mozilla/javascript/ClassCache.java",
          "class": null,
          "func": null,
          "lines": [
            "    private transient Map<Class<?>, Object> interfaceAdapterCache;",
            "    private transient volatile Map<CacheKey, JavaMembers> classTable;",
            "    private transient volatile Map<JavaAdapter.JavaAdapterSignature, Class<?>> classAdapterCache;",
            "    private transient volatile Map<Class<?>, Object> interfaceAdapterCache;",
            "    private int generatedClassSerial;",
            "            classTable = new ConcurrentHashMap<>(16, 0.75f, 1);",
            "            synchronized (this) {",
            "                if (classTable == null) {",
            "                    classTable = new ConcurrentHashMap<>();",
            "                }",
            "            }",
            "        }",
            "            classAdapterCache = new ConcurrentHashMap<>(16, 0.75f, 1);",
            "            synchronized (this) {",
            "                if (classAdapterCache == null) {",
            "                    classAdapterCache = new ConcurrentHashMap<>();",
            "                }",
            "            }",
            "        }",
            "                interfaceAdapterCache = new ConcurrentHashMap<>(16, 0.75f, 1);",
            "                synchronized (this) {",
            "                    if (interfaceAdapterCache == null) {",
            "                        interfaceAdapterCache = new ConcurrentHashMap<>();",
            "                    }",
            "                }",
            "            }"
          ]
        },
        "vulnerable_code": {
          "context": "rhino/src/main/java/org/mozilla/javascript/ClassCache.java",
          "class": null,
          "func": null,
          "lines": [
            "    private volatile boolean cachingIsEnabled = true;",
            "    private transient Map<CacheKey, JavaMembers> classTable;",
            "    private transient Map<JavaAdapter.JavaAdapterSignature, Class<?>> classAdapterCache;",
            "    private transient Map<Class<?>, Object> interfaceAdapterCache;",
            "    private transient volatile Map<CacheKey, JavaMembers> classTable;",
            "        if (classTable == null) {",
            "            // Use 1 as concurrency level here and for other concurrent hash maps",
            "            // as we don't expect high levels of sustained concurrent writes.",
            "            classTable = new ConcurrentHashMap<>(16, 0.75f, 1);",
            "            synchronized (this) {",
            "        if (classAdapterCache == null) {",
            "            classAdapterCache = new ConcurrentHashMap<>(16, 0.75f, 1);",
            "            synchronized (this) {",
            "            if (interfaceAdapterCache == null) {",
            "                interfaceAdapterCache = new ConcurrentHashMap<>(16, 0.75f, 1);",
            "                synchronized (this) {"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/mozilla/rhino/commit/c2e45bfb5b31b4d60c276620d67ce6f049252357",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/mozilla/rhino/pull/1883"
      },
      {
        "benign_code": {
          "context": "seatunnel-engine/seatunnel-engine-storage/checkpoint-storage-plugins/checkpoint-storage-hdfs/src/main/java/org/apache/seatunnel/engine/checkpoint/storage/hdfs/common/HdfsFileStorageInstance.java",
          "class": null,
          "func": null,
          "lines": [
            "    private static HdfsStorage HDFS_STORAGE;",
            "    private static volatile HdfsStorage HDFS_STORAGE;",
            "    private static final Object LOCK = new Object();"
          ]
        },
        "vulnerable_code": {
          "context": "seatunnel-engine/seatunnel-engine-storage/checkpoint-storage-plugins/checkpoint-storage-hdfs/src/main/java/org/apache/seatunnel/engine/checkpoint/storage/hdfs/common/HdfsFileStorageInstance.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    private static HdfsStorage HDFS_STORAGE;",
            "    private static volatile HdfsStorage HDFS_STORAGE;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/apache/seatunnel/commit/6f8c0393eaa76a1fa35d627dcb6da3a90a2d6ec4",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/apache/seatunnel/pull/9200"
      },
      {
        "benign_code": {
          "context": "sofa-ark-parent/support/ark-springboot-integration/ark-springboot-starter/src/main/java/com/alipay/sofa/ark/springboot/listener/ArkDeployStaticBizListener.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "import java.util.concurrent.atomic.AtomicBoolean;",
            "",
            "public class ArkDeployStaticBizListener implements ApplicationListener<ApplicationContextEvent>,",
            "",
            "    private final AtomicBoolean deployed = new AtomicBoolean(false);",
            "",
            "    @Override",
            "            if (event instanceof ContextRefreshedEvent) {",
            "            if (event instanceof ContextRefreshedEvent && deployed.compareAndSet(false, true)) {",
            "                // After the master biz is started, statically deploy the other biz from classpath"
          ]
        },
        "vulnerable_code": {
          "context": "sofa-ark-parent/support/ark-springboot-integration/ark-springboot-starter/src/main/java/com/alipay/sofa/ark/springboot/listener/ArkDeployStaticBizListener.java",
          "class": null,
          "func": null,
          "lines": [
            "        if (ArkConfigs.isEmbedEnable() && ArkConfigs.isEmbedStaticBizEnable()) {",
            "            if (event instanceof ContextRefreshedEvent) {",
            "            if (event instanceof ContextRefreshedEvent && deployed.compareAndSet(false, true)) {"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/sofastack/sofa-ark/commit/41fe0263cf7b4edf6fd915ac3846fedecc702692",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/sofastack/sofa-ark/pull/1035"
      },
      {
        "benign_code": {
          "context": "trino/src/main/java/com/netease/arctic/trino/DefaultArcticCatalogFactory.java",
          "class": null,
          "func": null,
          "lines": [
            "  private TableMetaStore tableMetaStore;",
            "  private volatile ArcticCatalog arcticCatalog;",
            "  private volatile TableMetaStore tableMetaStore;",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "trino/src/main/java/com/netease/arctic/trino/DefaultArcticCatalogFactory.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "  private ArcticCatalog arcticCatalog;",
            "  private TableMetaStore tableMetaStore;",
            "  private volatile ArcticCatalog arcticCatalog;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/apache/amoro/commit/35ab427f0fc29a85ae50767fb0072406c27ee8ed",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/apache/amoro/pull/2153"
      },
      {
        "benign_code": {
          "context": "src/org/python/core/PySystemState.java",
          "class": null,
          "func": null,
          "lines": [
            "    private codecs.CodecState codecState;",
            "    private volatile codecs.CodecState codecState;",
            "",
            "    public synchronized codecs.CodecState getCodecState() {",
            "    public codecs.CodecState getCodecState() {",
            "        if (codecState == null) {",
            "            codecState = new codecs.CodecState();",
            "            importLock.lock();",
            "            try {",
            "                imp.load(\"encodings\");",
            "                if (codecState == null) {",
            "                    codecState = new codecs.CodecState();",
            "                    // we have the importLock locked",
            "                    imp.load(\"encodings\");",
            "                }",
            "            } catch (PyException exc) {",
            "                }",
            "            } finally {",
            "                importLock.unlock();",
            "            }"
          ]
        },
        "vulnerable_code": {
          "context": "src/org/python/core/PySystemState.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    private codecs.CodecState codecState;",
            "    private volatile codecs.CodecState codecState;",
            "",
            "    public synchronized codecs.CodecState getCodecState() {",
            "    public codecs.CodecState getCodecState() {",
            "        if (codecState == null) {",
            "            codecState = new codecs.CodecState();",
            "            importLock.lock();",
            "            try {",
            "                imp.load(\"encodings\");",
            "                if (codecState == null) {"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/jython/jython/commit/47812da91f1d37ce26db2590b837963ff29038f2",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/jython/jython/pull/243"
      },
      {
        "benign_code": {
          "context": "src/main/java/com/fasterxml/jackson/databind/deser/std/EnumDeserializer.java",
          "class": null,
          "func": null,
          "lines": [
            "    protected CompactStringObjectMap _lookupByToString;",
            "    protected volatile CompactStringObjectMap _lookupByToString;",
            "",
            "    {",
            "    protected CompactStringObjectMap _getToStringLookup(DeserializationContext ctxt) {",
            "        CompactStringObjectMap lookup = _lookupByToString;",
            "                    .constructLookup();",
            "                lookup = _lookupByToString;",
            "                if (lookup == null) {",
            "                    lookup = EnumResolver.constructUsingToString(ctxt.getConfig(), _enumClass())",
            "                        .constructLookup();",
            "                    _lookupByToString = lookup;",
            "                }",
            "            }"
          ]
        },
        "vulnerable_code": {
          "context": "src/main/java/com/fasterxml/jackson/databind/deser/std/EnumDeserializer.java",
          "class": null,
          "func": null,
          "lines": [
            "     */",
            "    protected CompactStringObjectMap _lookupByToString;",
            "    protected volatile CompactStringObjectMap _lookupByToString;",
            "",
            "    protected CompactStringObjectMap _getToStringLookup(DeserializationContext ctxt)",
            "    {",
            "    protected CompactStringObjectMap _getToStringLookup(DeserializationContext ctxt) {",
            "        CompactStringObjectMap lookup = _lookupByToString;",
            "        // note: exact locking not needed; all we care for here is to try to",
            "        // reduce contention for the initial resolution",
            "        if (lookup == null) {",
            "            synchronized (this) {",
            "                lookup = EnumResolver.constructUsingToString(ctxt.getConfig(), _enumClass())",
            "                    .constructLookup();",
            "                lookup = _lookupByToString;",
            "            }",
            "            _lookupByToString = lookup;",
            "        }"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/FasterXML/jackson-databind/commit/1ba4cd03991129304b7163de10c3f8efcc9d7be1",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/FasterXML/jackson-databind/pull/3768"
      },
      {
        "benign_code": {
          "context": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java",
          "class": null,
          "func": null,
          "lines": [
            "                    responseData = new LinkedHashMap<>();",
            "                    // Assigning the lazy-initialized responseData in the last step",
            "                    // to avoid other threads accessing a half-initialized object.",
            "                    final LinkedHashMap<TopicPartition, FetchResponseData.PartitionData> responseDataTmp =",
            "                            new LinkedHashMap<>();",
            "                    data.responses().forEach(topicResponse -> {",
            "                                responseData.put(new TopicPartition(name, partition.partitionIndex()), partition));",
            "                                responseDataTmp.put(new TopicPartition(name, partition.partitionIndex()), partition));",
            "                        }",
            "                    });",
            "                    responseData = responseDataTmp;",
            "                }"
          ]
        },
        "vulnerable_code": {
          "context": "clients/src/main/java/org/apache/kafka/common/requests/FetchResponse.java",
          "class": null,
          "func": null,
          "lines": [
            "                if (responseData == null) {",
            "                    responseData = new LinkedHashMap<>();",
            "                    // Assigning the lazy-initialized responseData in the last step",
            "                            topicResponse.partitions().forEach(partition ->",
            "                                responseData.put(new TopicPartition(name, partition.partitionIndex()), partition));",
            "                                responseDataTmp.put(new TopicPartition(name, partition.partitionIndex()), partition));"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/apache/kafka/commit/366b998a229f26aa4601e6b114c2198de0697562",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/apache/kafka/pull/11963"
      },
      {
        "benign_code": {
          "context": "kubernetes-client/src/main/java/io/fabric8/kubernetes/client/internal/readiness/Readiness.java",
          "class": null,
          "func": null,
          "lines": [
            "  private static Readiness instance;",
            "  private static class ReadinessHolder {",
            "    public static final Readiness INSTANCE = new Readiness();",
            "  }",
            "",
            "    return instance;",
            "    return ReadinessHolder.INSTANCE;",
            "  }"
          ]
        },
        "vulnerable_code": {
          "context": "kubernetes-client/src/main/java/io/fabric8/kubernetes/client/internal/readiness/Readiness.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "  private static Readiness instance;",
            "  private static class ReadinessHolder {",
            "  public static Readiness getInstance() {",
            "    if (instance == null) {",
            "      synchronized (Readiness.class) {",
            "        instance = new Readiness();",
            "      }",
            "    }",
            "    return instance;",
            "    return ReadinessHolder.INSTANCE;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/fabric8io/kubernetes-client/commit/6d93cbcd677c2675cfccc52bb69ea0046d821fc4",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/fabric8io/kubernetes-client/pull/2817"
      },
      {
        "benign_code": {
          "context": "src/fmllauncher/java/net/minecraftforge/fml/loading/moddiscovery/ModFileInfo.java",
          "class": null,
          "func": null,
          "lines": [
            "    private final String license;",
            "    // Caches the manifest of the mod jar as parsing the manifest can be expensive for",
            "    // signed jars.",
            "    private final Optional<Manifest> manifest;",
            "",
            "        if (modConfigs.isEmpty()) {",
            "        if (modConfigs.isEmpty())",
            "        {",
            "            throw new InvalidModFileException(\"Missing mods list\", this);",
            "                () -> this.mods.stream().map(IModInfo::getVersion).map(Objects::toString).collect(Collectors.joining(\",\", \"{\", \"}\")));",
            "        this.manifest = modFile.getLocator().findManifest(modFile.getFilePath());",
            "    }",
            "    public Map<String, Object> getFileProperties() {",
            "    public Map<String, Object> getFileProperties()",
            "    {",
            "        return this.properties;",
            "        return modFile.getLocator().findManifest(modFile.getFilePath());",
            "    public Optional<Manifest> getManifest()",
            "    {",
            "        return manifest;",
            "    }",
            "    public boolean showAsResourcePack() {",
            "    public boolean showAsResourcePack()",
            "    {",
            "        return this.showAsResourcePack;",
            "    public <T> Optional<T> getConfigElement(final String... key) {",
            "    public <T> Optional<T> getConfigElement(final String... key)",
            "    {",
            "        return this.config.getConfigElement(key);",
            "    public List<? extends IConfigurable> getConfigList(final String... key) {",
            "    public List<? extends IConfigurable> getConfigList(final String... key)",
            "    {",
            "        return this.config.getConfigList(key);",
            "    public String getLicense() {",
            "    public String getLicense()",
            "    {",
            "        return license;"
          ]
        },
        "vulnerable_code": {
          "context": "src/fmllauncher/java/net/minecraftforge/fml/loading/moddiscovery/ModFileInfo.java",
          "class": null,
          "func": null,
          "lines": [
            "        final List<? extends IConfigurable> modConfigs = config.getConfigList(\"mods\");",
            "        if (modConfigs.isEmpty()) {",
            "        if (modConfigs.isEmpty())",
            "    @Override",
            "    public Map<String, Object> getFileProperties() {",
            "    public Map<String, Object> getFileProperties()",
            "",
            "    public Optional<Manifest> getManifest() {",
            "        return modFile.getLocator().findManifest(modFile.getFilePath());",
            "    public Optional<Manifest> getManifest()",
            "    @Override",
            "    public boolean showAsResourcePack() {",
            "    public boolean showAsResourcePack()",
            "    @Override",
            "    public <T> Optional<T> getConfigElement(final String... key) {",
            "    public <T> Optional<T> getConfigElement(final String... key)",
            "    @Override",
            "    public List<? extends IConfigurable> getConfigList(final String... key) {",
            "    public List<? extends IConfigurable> getConfigList(final String... key)",
            "    @Override",
            "    public String getLicense() {",
            "    public String getLicense()"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/MinecraftForge/MinecraftForge/commit/5037adede907b1c03b0f4e8227d882f491f8481d",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/MinecraftForge/MinecraftForge/pull/7256"
      },
      {
        "benign_code": {
          "context": "transport-native-epoll/src/main/java/io/netty/channel/epoll/AbstractEpollStreamChannel.java",
          "class": null,
          "func": null,
          "lines": [
            "    // Lazy init these if we need to splice(...)",
            "    private volatile Queue<SpliceInTask> spliceQueue;",
            "    private FileDescriptor pipeIn;",
            "        if (spliceQueue == null) {",
            "        Queue<SpliceInTask> sQueue = spliceQueue;",
            "        if (sQueue == null) {",
            "            return;",
            "            SpliceInTask task = spliceQueue.poll();",
            "            SpliceInTask task = sQueue.poll();",
            "            if (task == null) {",
            "            try {",
            "                Queue<SpliceInTask> sQueue = null;",
            "                do {",
            "                        SpliceInTask spliceTask = spliceQueue.peek();",
            "                    if (sQueue != null || (sQueue = spliceQueue) != null) {",
            "                        SpliceInTask spliceTask = sQueue.peek();",
            "                        if (spliceTask != null) {",
            "                                    spliceQueue.remove();",
            "                                    sQueue.remove();",
            "                                }",
            "                    addToSpliceQueue0(task);",
            "        Queue<SpliceInTask> sQueue = spliceQueue;",
            "        if (sQueue == null) {",
            "            synchronized (this) {",
            "                sQueue = spliceQueue;",
            "                if (sQueue == null) {",
            "                    spliceQueue = sQueue = PlatformDependent.newMpscQueue();",
            "                }",
            "            spliceQueue = PlatformDependent.newMpscQueue();",
            "            }",
            "        }",
            "        spliceQueue.add(task);",
            "        sQueue.add(task);",
            "    }"
          ]
        },
        "vulnerable_code": {
          "context": "transport-native-epoll/src/main/java/io/netty/channel/epoll/AbstractEpollStreamChannel.java",
          "class": null,
          "func": null,
          "lines": [
            "    };",
            "    private Queue<SpliceInTask> spliceQueue;",
            "",
            "    private void clearSpliceQueue() {",
            "        if (spliceQueue == null) {",
            "        Queue<SpliceInTask> sQueue = spliceQueue;",
            "        for (;;) {",
            "            SpliceInTask task = spliceQueue.poll();",
            "            SpliceInTask task = sQueue.poll();",
            "                do {",
            "                    if (spliceQueue != null) {",
            "                        SpliceInTask spliceTask = spliceQueue.peek();",
            "                    if (sQueue != null || (sQueue = spliceQueue) != null) {",
            "                                if (isActive()) {",
            "                                    spliceQueue.remove();",
            "                                    sQueue.remove();",
            "    private void addToSpliceQueue(final SpliceInTask task) {",
            "        EventLoop eventLoop = eventLoop();",
            "        if (eventLoop.inEventLoop()) {",
            "            addToSpliceQueue0(task);",
            "        } else {",
            "            eventLoop.execute(new Runnable() {",
            "                @Override",
            "                public void run() {",
            "                    addToSpliceQueue0(task);",
            "        Queue<SpliceInTask> sQueue = spliceQueue;",
            "                }",
            "            });",
            "        }",
            "    }",
            "",
            "    private void addToSpliceQueue0(SpliceInTask task) {",
            "        if (spliceQueue == null) {",
            "            spliceQueue = PlatformDependent.newMpscQueue();",
            "            }",
            "        }",
            "        spliceQueue.add(task);",
            "        sQueue.add(task);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/netty/netty/commit/5384bbcf851a12c26c8a249310528f22f32ee803",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/netty/netty/pull/9354"
      },
      {
        "benign_code": {
          "context": "com.ibm.wala.core/src/com/ibm/wala/classLoader/BytecodeClass.java",
          "class": null,
          "func": null,
          "lines": [
            "          IMethod[] methods = computeDeclaredMethods();",
            "          ",
            "          final Map<Selector, IMethod> tmpMethodMap;",
            "          if (methods.length > 5) {",
            "            methodMap = HashMapFactory.make(methods.length);",
            "            tmpMethodMap = HashMapFactory.make(methods.length);",
            "          } else {",
            "            methodMap = new SmallMap<Selector, IMethod>();",
            "            tmpMethodMap= new SmallMap<Selector, IMethod>();",
            "          }",
            "            methodMap.put(m.getReference().getSelector(), m);",
            "            tmpMethodMap.put(m.getReference().getSelector(), m);",
            "          }",
            "          ",
            "          methodMap = tmpMethodMap;",
            "        }"
          ]
        },
        "vulnerable_code": {
          "context": "com.ibm.wala.core/src/com/ibm/wala/classLoader/BytecodeClass.java",
          "class": null,
          "func": null,
          "lines": [
            "          if (methods.length > 5) {",
            "            methodMap = HashMapFactory.make(methods.length);",
            "            tmpMethodMap = HashMapFactory.make(methods.length);",
            "          } else {",
            "            methodMap = new SmallMap<Selector, IMethod>();",
            "            tmpMethodMap= new SmallMap<Selector, IMethod>();",
            "            IMethod m = methods[i];",
            "            methodMap.put(m.getReference().getSelector(), m);",
            "            tmpMethodMap.put(m.getReference().getSelector(), m);"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/wala/WALA/commit/4fc5d9100644704c5972e3d53abd4b047c40258e",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/wala/WALA/pull/24"
      }
    ],
    "CWD-1126": [
      {
        "benign_code": {
          "context": "hudi-trino-plugin/src/main/java/io/trino/plugin/hudi/partition/HudiPartitionInfoLoader.java",
          "class": null,
          "func": null,
          "lines": [
            "import io.airlift.concurrent.MoreFutures;",
            "import com.google.common.util.concurrent.ListenableFuture;",
            "import io.airlift.log.Logger;",
            "import io.trino.plugin.hive.HivePartitionKey;",
            "import io.trino.plugin.hive.util.AsyncQueue;",
            "import io.trino.plugin.hive.util.ResumableTask;",
            "import io.trino.plugin.hive.util.ResumableTask.TaskStatus;",
            "import io.trino.plugin.hudi.query.HudiDirectoryLister;",
            "import java.util.Deque;",
            "import java.util.Iterator;",
            "import java.util.List;",
            "",
            "import static com.google.common.util.concurrent.Futures.immediateVoidFuture;",
            "",
            "public class HudiPartitionInfoLoader",
            "        implements Runnable",
            "        implements ResumableTask",
            "{",
            "    private static final Logger log = Logger.get(HudiPartitionInfoLoader.class);",
            "",
            "    private final HudiDirectoryLister hudiDirectoryLister;",
            "    private final boolean useIndex;",
            "    private final Deque<Iterator<ConnectorSplit>> splitIterators;",
            "",
            "",
            "    /**",
            "     * Creates a new split loader.",
            "     *",
            "     * @param hudiDirectoryLister Service for listing files in a partition.",
            "     * @param commitTime The latest Hudi commit time for snapshot isolation.",
            "     * @param hudiSplitFactory Factory to generate {@link ConnectorSplit}s.",
            "     * @param asyncQueue The output queue to send generated splits to.",
            "     * @param partitionQueue The input queue of partitions to process.",
            "     * @param useIndex Whether to use the metadata index for file listing.",
            "     * @param splitIterators A deque, private to this worker, used to store",
            "     * partially processed split iterators. This allows the task to save",
            "     * its state when yielding (e.g., when the asyncQueue is full) and",
            "     * resume processing from the same point.",
            "     */",
            "    public HudiPartitionInfoLoader(",
            "            boolean useIndex)",
            "            boolean useIndex,",
            "            Deque<Iterator<ConnectorSplit>> splitIterators)",
            "    {",
            "        this.useIndex = useIndex;",
            "        this.splitIterators = splitIterators;",
            "    }",
            "    public void run()",
            "    public TaskStatus process()",
            "    {",
            "        while (isRunning || (!partitionQueue.isEmpty() || !splitIterators.isEmpty())) {",
            "            try {",
            "                ListenableFuture<Void> future = loadSplits();",
            "                if (!future.isDone()) {",
            "                    return TaskStatus.continueOn(future);",
            "                }",
            "            }",
            "            catch (Exception e) {",
            "                throw new RuntimeException(\"Error loading splits\", e);",
            "            }",
            "        }",
            "",
            "        return TaskStatus.finished();",
            "    }",
            "",
            "    private ListenableFuture<Void> loadSplits()",
            "    {",
            "            HiveHudiPartitionInfo hudiPartitionInfo = partitionQueue.poll();",
            "        Iterator<ConnectorSplit> splits = splitIterators.poll();",
            "        if (splits == null) {",
            "            HiveHudiPartitionInfo partition = partitionQueue.poll();",
            "            if (partition == null) {",
            "                return immediateVoidFuture();",
            "            }",
            "            splits = generateSplitsFromPartition(partition);",
            "        }",
            "",
            "                generateSplitsFromPartition(hudiPartitionInfo);",
            "        while (splits.hasNext()) {",
            "            ConnectorSplit split = splits.next();",
            "            ListenableFuture<Void> future = asyncQueue.offer(split);",
            "            if (!future.isDone()) {",
            "                log.debug(\"AsyncQueue is full, yielding split loader task\");",
            "                splitIterators.addFirst(splits);",
            "                return future;",
            "            }",
            "        }",
            "",
            "        return immediateVoidFuture();",
            "    }",
            "    private void generateSplitsFromPartition(HiveHudiPartitionInfo hudiPartitionInfo)",
            "    private Iterator<ConnectorSplit> generateSplitsFromPartition(HiveHudiPartitionInfo hudiPartitionInfo)",
            "    {",
            "        partitionFileSlices.stream()",
            "        return partitionFileSlices.stream()",
            "                .flatMap(slice -> hudiSplitFactory.createSplits(partitionKeys, slice, this.commitTime).stream())",
            "                .forEachOrdered(MoreFutures::getFutureValue);",
            "                .map(ConnectorSplit.class::cast)",
            "                .iterator();",
            "    }"
          ]
        },
        "vulnerable_code": {
          "context": "hudi-trino-plugin/src/main/java/io/trino/plugin/hudi/partition/HudiPartitionInfoLoader.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "import io.airlift.concurrent.MoreFutures;",
            "import com.google.common.util.concurrent.ListenableFuture;",
            "public class HudiPartitionInfoLoader",
            "        implements Runnable",
            "        implements ResumableTask",
            "            Deque<HiveHudiPartitionInfo> partitionQueue,",
            "            boolean useIndex)",
            "            boolean useIndex,",
            "    @Override",
            "    public void run()",
            "    public TaskStatus process()",
            "    {",
            "        while (isRunning || !partitionQueue.isEmpty()) {",
            "            HiveHudiPartitionInfo hudiPartitionInfo = partitionQueue.poll();",
            "        Iterator<ConnectorSplit> splits = splitIterators.poll();",
            "",
            "            if (hudiPartitionInfo != null && hudiPartitionInfo.getHivePartitionName() != null) {",
            "                generateSplitsFromPartition(hudiPartitionInfo);",
            "        while (splits.hasNext()) {",
            "",
            "    private void generateSplitsFromPartition(HiveHudiPartitionInfo hudiPartitionInfo)",
            "    private Iterator<ConnectorSplit> generateSplitsFromPartition(HiveHudiPartitionInfo hudiPartitionInfo)",
            "        List<FileSlice> partitionFileSlices = hudiDirectoryLister.listStatus(hudiPartitionInfo, useIndex);",
            "        partitionFileSlices.stream()",
            "        return partitionFileSlices.stream()",
            "                .flatMap(slice -> hudiSplitFactory.createSplits(partitionKeys, slice, this.commitTime).stream())",
            "                .map(asyncQueue::offer)",
            "                .forEachOrdered(MoreFutures::getFutureValue);",
            "                .map(ConnectorSplit.class::cast)"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/apache/hudi/commit/9b97fbe7b3981b9549d4720455cccd2c84086cbc",
        "CWE": "CWE-764",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/apache/hudi/pull/14225"
      }
    ],
    "CWD-1129": [
      {
        "benign_code": {
          "context": "engine/src/main/java/org/pentaho/di/shared/RepositorySharedObjectsIO.java",
          "class": null,
          "func": null,
          "lines": [
            "    lock.lock();",
            "    // because this class creates new Nodes for every operation, we don't actually need to lock. ",
            "  }",
            "    lock.unlock();",
            "    // no locking needed",
            "  }"
          ]
        },
        "vulnerable_code": {
          "context": "engine/src/main/java/org/pentaho/di/shared/RepositorySharedObjectsIO.java",
          "class": null,
          "func": null,
          "lines": [
            "import java.util.Objects;",
            "import java.util.concurrent.locks.ReentrantLock;",
            "import org.w3c.dom.Node;",
            "  private final SlaveServersSupplier slaveServerSupplier;",
            "  private final ReentrantLock lock = new ReentrantLock();",
            "",
            "  public void lock() {",
            "    lock.lock();",
            "    // because this class creates new Nodes for every operation, we don't actually need to lock. ",
            "  public void unlock() {",
            "    lock.unlock();",
            "    // no locking needed"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/pentaho/pentaho-kettle/commit/9cc59d59073a8fbc7cf71799f324bb8b60a7d09c",
        "CWE": "CWE-833",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/pentaho/pentaho-kettle/pull/10279"
      },
      {
        "benign_code": {
          "context": "fe/fe-core/src/main/java/org/apache/doris/catalog/TabletInvertedIndex.java",
          "class": null,
          "func": null,
          "lines": [
            "import java.util.concurrent.ForkJoinPool;",
            "import java.util.concurrent.CompletableFuture;",
            "import java.util.concurrent.ExecutorService;",
            "import java.util.concurrent.LinkedBlockingQueue;",
            "import java.util.concurrent.ThreadPoolExecutor;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.concurrent.locks.StampedLock;",
            "    private ForkJoinPool taskPool = new ForkJoinPool(Runtime.getRuntime().availableProcessors());",
            "    private final ExecutorService taskPool = new ThreadPoolExecutor(",
            "            Config.tablet_report_thread_pool_num,",
            "            2 * Config.tablet_report_thread_pool_num,",
            "            // tablet report task default 60s once",
            "            120L,",
            "            TimeUnit.SECONDS,",
            "            new LinkedBlockingQueue<>(Config.tablet_report_queue_size),",
            "            new ThreadPoolExecutor.DiscardOldestPolicy());",
            "",
            "        long start = System.currentTimeMillis();",
            "",
            "        try {",
            "            }",
            "",
            "            Map<Long, Replica> replicaMetaWithBackend = backingReplicaMetaTable.row(backendId);",
            "                            }",
            "                processTabletReportAsync(backendId, backendTablets, backendPartitionsVersion, storageMediumMap,",
            "                        tabletSyncMap, tabletDeleteFromMeta, tabletFoundInMeta, tabletMigrationMap,",
            "                        partitionVersionSyncMap, transactionsToPublish, transactionsToClear, tabletRecoveryMap,",
            "                        tabletToUpdate, cooldownTablets, replicaMetaWithBackend);",
            "            }",
            "        } finally {",
            "            readUnlock(stamp);",
            "        }",
            "",
            "                            }",
            "        // Process cooldown configs outside of read lock to avoid deadlock",
            "        cooldownTablets.forEach(p -> handleCooldownConf(p.first, p.second, cooldownConfToPush, cooldownConfToUpdate));",
            "",
            "                            }",
            "        logTabletReportSummary(backendId, feTabletNum, backendTablets, backendPartitionsVersion,",
            "                tabletSyncMap, tabletDeleteFromMeta, tabletFoundInMeta, tabletMigrationMap,",
            "                partitionVersionSyncMap, transactionsToPublish, transactionsToClear,",
            "                tabletToUpdate, tabletRecoveryMap, start);",
            "    }",
            "",
            "                            }",
            "    /**",
            "     * Process tablet report asynchronously using thread pool",
            "     */",
            "    private void processTabletReportAsync(long backendId, Map<Long, TTablet> backendTablets,",
            "                                          Map<Long, Long> backendPartitionsVersion,",
            "                                          HashMap<Long, TStorageMedium> storageMediumMap,",
            "                                          ListMultimap<Long, Long> tabletSyncMap,",
            "                                          ListMultimap<Long, Long> tabletDeleteFromMeta,",
            "                                          Set<Long> tabletFoundInMeta,",
            "                                          ListMultimap<TStorageMedium, Long> tabletMigrationMap,",
            "                                          Map<Long, Long> partitionVersionSyncMap,",
            "                                          Map<Long, SetMultimap<Long, TPartitionVersionInfo>> transactionsToPublish,",
            "                                          SetMultimap<Long, Long> transactionsToClear,",
            "                                          ListMultimap<Long, Long> tabletRecoveryMap,",
            "                                          List<TTabletMetaInfo> tabletToUpdate,",
            "                                          List<Pair<TabletMeta, TTabletInfo>> cooldownTablets,",
            "                                          Map<Long, Replica> replicaMetaWithBackend) {",
            "        // Calculate optimal chunk size to balance task granularity and concurrency",
            "        // For large tablet counts (40W-50W), we want smaller chunks to maximize parallelism",
            "        // Target: create at least threadPoolSize * 4 tasks for better load balancing",
            "        int totalTablets = replicaMetaWithBackend.size();",
            "        int threadPoolSize = Config.tablet_report_thread_pool_num;",
            "        int targetTasks = threadPoolSize * 4; // Create 4x tasks as threads for better load balancing",
            "        int chunkSize = Math.max(500, totalTablets / targetTasks);",
            "",
            "        // Cap chunk size to avoid too large tasks",
            "        // so thread pool queue will not be fulled with few large tasks",
            "        int maxChunkSize = 10000;",
            "        chunkSize = Math.min(chunkSize, maxChunkSize);",
            "        List<Map.Entry<Long, Replica>> entries = new ArrayList<>(replicaMetaWithBackend.entrySet());",
            "        List<CompletableFuture<Void>> tabletFutures = new ArrayList<>();",
            "        int estimatedTasks = (totalTablets + chunkSize - 1) / chunkSize;",
            "        if (LOG.isDebugEnabled()) {",
            "            LOG.debug(\"Processing tablet report for backend[{}]: total tablets={}, chunkSize={}, estimated tasks={}\",",
            "                    backendId, totalTablets, chunkSize, estimatedTasks);",
            "        }",
            "",
            "                            }",
            "        for (int i = 0; i < entries.size(); i += chunkSize) {",
            "            final int start = i;",
            "            final int end = Math.min(i + chunkSize, entries.size());",
            "",
            "            CompletableFuture<Void> future = CompletableFuture.runAsync(() -> {",
            "                for (int j = start; j < end; j++) {",
            "                    Map.Entry<Long, Replica> entry = entries.get(j);",
            "                    processTabletEntry(backendId, backendTablets, storageMediumMap, tabletSyncMap,",
            "                            tabletDeleteFromMeta, tabletFoundInMeta, tabletMigrationMap,",
            "                            transactionsToPublish, transactionsToClear, tabletRecoveryMap,",
            "                            tabletToUpdate, cooldownTablets, entry);",
            "                }",
            "            }, taskPool);",
            "",
            "                            }",
            "            tabletFutures.add(future);",
            "        }",
            "",
            "                }).join();",
            "        // Process partition versions in parallel",
            "        CompletableFuture<Void> partitionFuture = CompletableFuture.runAsync(() -> {",
            "            processPartitionVersions(backendPartitionsVersion, partitionVersionSyncMap);",
            "        }, taskPool);",
            "",
            "        // Wait for all tasks to complete",
            "        CompletableFuture.allOf(tabletFutures.toArray(new CompletableFuture[0])).join();",
            "        partitionFuture.join();",
            "    }",
            "",
            "    /**",
            "     * Process a single tablet entry from backend report",
            "     */",
            "    private void processTabletEntry(long backendId, Map<Long, TTablet> backendTablets,",
            "                                     HashMap<Long, TStorageMedium> storageMediumMap,",
            "                                     ListMultimap<Long, Long> tabletSyncMap,",
            "                                     ListMultimap<Long, Long> tabletDeleteFromMeta,",
            "                                     Set<Long> tabletFoundInMeta,",
            "                                     ListMultimap<TStorageMedium, Long> tabletMigrationMap,",
            "                                     Map<Long, SetMultimap<Long, TPartitionVersionInfo>> transactionsToPublish,",
            "                                     SetMultimap<Long, Long> transactionsToClear,",
            "                                     ListMultimap<Long, Long> tabletRecoveryMap,",
            "                                     List<TTabletMetaInfo> tabletToUpdate,",
            "                                     List<Pair<TabletMeta, TTabletInfo>> cooldownTablets,",
            "                                     Map.Entry<Long, Replica> entry) {",
            "        long tabletId = entry.getKey();",
            "        Replica replica = entry.getValue();",
            "",
            "        Preconditions.checkState(tabletMetaMap.containsKey(tabletId),",
            "                \"tablet \" + tabletId + \" not exists, backend \" + backendId);",
            "        TabletMeta tabletMeta = tabletMetaMap.get(tabletId);",
            "",
            "        if (backendTablets.containsKey(tabletId)) {",
            "            // Tablet exists in both FE and BE",
            "            TTablet backendTablet = backendTablets.get(tabletId);",
            "            TTabletInfo backendTabletInfo = backendTablet.getTabletInfos().get(0);",
            "",
            "            tabletFoundInMeta.add(tabletId);",
            "",
            "            processExistingTablet(backendId, tabletId, replica, tabletMeta, backendTabletInfo,",
            "                    storageMediumMap, tabletSyncMap, tabletMigrationMap, transactionsToPublish,",
            "                    transactionsToClear, tabletRecoveryMap, tabletToUpdate, cooldownTablets);",
            "        } else {",
            "            // Tablet exists in FE but not in BE - may need deletion",
            "            processDeletedTablet(backendId, tabletId, tabletMeta, tabletDeleteFromMeta);",
            "        }",
            "    }",
            "",
            "    /**",
            "     * Process tablet that exists in both FE and BE",
            "     */",
            "    private void processExistingTablet(long backendId, long tabletId, Replica replica,",
            "                                        TabletMeta tabletMeta, TTabletInfo backendTabletInfo,",
            "                                        HashMap<Long, TStorageMedium> storageMediumMap,",
            "                                        ListMultimap<Long, Long> tabletSyncMap,",
            "                                        ListMultimap<TStorageMedium, Long> tabletMigrationMap,",
            "                                        Map<Long, SetMultimap<Long, TPartitionVersionInfo>> transactionsToPublish,",
            "                                        SetMultimap<Long, Long> transactionsToClear,",
            "                                        ListMultimap<Long, Long> tabletRecoveryMap,",
            "                                        List<TTabletMetaInfo> tabletToUpdate,",
            "                                        List<Pair<TabletMeta, TTabletInfo>> cooldownTablets) {",
            "        // Check and prepare tablet meta info update",
            "        TTabletMetaInfo tabletMetaInfo = prepareTabletMetaInfo(replica, tabletMeta, backendTabletInfo);",
            "",
            "        // Check if version sync is needed",
            "        if (needSync(replica, backendTabletInfo)) {",
            "            synchronized (tabletSyncMap) {",
            "                tabletSyncMap.put(tabletMeta.getDbId(), tabletId);",
            "            }",
            "        long end = System.currentTimeMillis();",
            "        // Update replica path and schema hash",
            "        updateReplicaBasicInfo(replica, backendTabletInfo);",
            "",
            "        // Check if replica needs recovery",
            "        if (needRecover(replica, tabletMeta.getOldSchemaHash(), backendTabletInfo)) {",
            "            logReplicaRecovery(replica, tabletId, backendId, backendTabletInfo);",
            "            synchronized (tabletRecoveryMap) {",
            "                tabletRecoveryMap.put(tabletMeta.getDbId(), tabletId);",
            "            }",
            "        }",
            "",
            "        // Handle cooldown policy",
            "        if (Config.enable_storage_policy && backendTabletInfo.isSetCooldownTerm()) {",
            "            synchronized (cooldownTablets) {",
            "                cooldownTablets.add(Pair.of(tabletMeta, backendTabletInfo));",
            "            }",
            "            replica.setCooldownMetaId(backendTabletInfo.getCooldownMetaId());",
            "            replica.setCooldownTerm(backendTabletInfo.getCooldownTerm());",
            "        }",
            "",
            "        // Check storage medium migration",
            "        checkStorageMediumMigration(tabletId, tabletMeta, backendTabletInfo,",
            "                storageMediumMap, tabletMigrationMap);",
            "",
            "        // Handle transactions",
            "        if (backendTabletInfo.isSetTransactionIds()) {",
            "            handleBackendTransactions(backendId, backendTabletInfo.getTransactionIds(), tabletId,",
            "                    tabletMeta, transactionsToPublish, transactionsToClear);",
            "        }",
            "",
            "        // Update replica version count",
            "        updateReplicaVersionCount(replica, backendTabletInfo);",
            "",
            "        // Add tablet meta info to update list if needed",
            "        if (tabletMetaInfo != null) {",
            "            tabletMetaInfo.setTabletId(tabletId);",
            "            synchronized (tabletToUpdate) {",
            "                tabletToUpdate.add(tabletMetaInfo);",
            "            }",
            "        }",
            "    }",
            "",
            "    /**",
            "     * Prepare tablet meta info for BE update if needed",
            "     */",
            "    private TTabletMetaInfo prepareTabletMetaInfo(Replica replica, TabletMeta tabletMeta,",
            "                                                   TTabletInfo backendTabletInfo) {",
            "        TTabletMetaInfo tabletMetaInfo = null;",
            "",
            "        // Check replica id mismatch",
            "        if (backendTabletInfo.getReplicaId() != replica.getId()",
            "                && replica.getState() != ReplicaState.CLONE) {",
            "            tabletMetaInfo = new TTabletMetaInfo();",
            "            tabletMetaInfo.setReplicaId(replica.getId());",
            "        }",
            "",
            "        // Check in-memory flag",
            "        PartitionCollectInfo partitionCollectInfo =",
            "                partitionCollectInfoMap.get(backendTabletInfo.getPartitionId());",
            "        boolean isInMemory = partitionCollectInfo != null && partitionCollectInfo.isInMemory();",
            "        if (isInMemory != backendTabletInfo.isIsInMemory()) {",
            "            if (tabletMetaInfo == null) {",
            "                tabletMetaInfo = new TTabletMetaInfo();",
            "            }",
            "            tabletMetaInfo.setIsInMemory(isInMemory);",
            "        }",
            "",
            "        // Check partition id mismatch",
            "        if (Config.fix_tablet_partition_id_eq_0",
            "                && tabletMeta.getPartitionId() > 0",
            "                && backendTabletInfo.getPartitionId() == 0) {",
            "            LOG.warn(\"be report tablet partition id not eq fe, in be {} but in fe {}\",",
            "                    backendTabletInfo, tabletMeta);",
            "            if (tabletMetaInfo == null) {",
            "                tabletMetaInfo = new TTabletMetaInfo();",
            "            }",
            "            tabletMetaInfo.setPartitionId(tabletMeta.getPartitionId());",
            "        }",
            "",
            "        return tabletMetaInfo;",
            "    }",
            "",
            "    /**",
            "     * Update replica's basic info like path hash and schema hash",
            "     */",
            "    private void updateReplicaBasicInfo(Replica replica, TTabletInfo backendTabletInfo) {",
            "        // Update path hash",
            "        if (backendTabletInfo.isSetPathHash()",
            "                && replica.getPathHash() != backendTabletInfo.getPathHash()) {",
            "            replica.setPathHash(backendTabletInfo.getPathHash());",
            "        }",
            "",
            "        // Update schema hash",
            "        if (backendTabletInfo.isSetSchemaHash()",
            "                && replica.getState() == ReplicaState.NORMAL",
            "                && replica.getSchemaHash() != backendTabletInfo.getSchemaHash()) {",
            "            replica.setSchemaHash(backendTabletInfo.getSchemaHash());",
            "        }",
            "    }",
            "",
            "    /**",
            "     * Log replica recovery information",
            "     */",
            "    private void logReplicaRecovery(Replica replica, long tabletId, long backendId,",
            "                                     TTabletInfo backendTabletInfo) {",
            "        LOG.warn(\"replica {} of tablet {} on backend {} need recovery. \"",
            "                        + \"replica in FE: {}, report version {}, report schema hash: {}, \"",
            "                        + \"is bad: {}, is version missing: {}\",",
            "                replica.getId(), tabletId, backendId, replica,",
            "                backendTabletInfo.getVersion(),",
            "                backendTabletInfo.getSchemaHash(),",
            "                backendTabletInfo.isSetUsed() ? !backendTabletInfo.isUsed() : \"false\",",
            "                backendTabletInfo.isSetVersionMiss() ? backendTabletInfo.isVersionMiss() : \"unset\");",
            "    }",
            "",
            "    /**",
            "     * Check if storage medium migration is needed",
            "     */",
            "    private void checkStorageMediumMigration(long tabletId, TabletMeta tabletMeta,",
            "                                              TTabletInfo backendTabletInfo,",
            "                                              HashMap<Long, TStorageMedium> storageMediumMap,",
            "                                              ListMultimap<TStorageMedium, Long> tabletMigrationMap) {",
            "        if (Config.disable_storage_medium_check) {",
            "            return;",
            "        }",
            "",
            "        long partitionId = tabletMeta.getPartitionId();",
            "        TStorageMedium storageMedium = storageMediumMap.get(partitionId);",
            "",
            "        if (storageMedium != null && backendTabletInfo.isSetStorageMedium()",
            "                && isLocal(storageMedium)",
            "                && isLocal(backendTabletInfo.getStorageMedium())",
            "                && isLocal(tabletMeta.getStorageMedium())) {",
            "",
            "            if (storageMedium != backendTabletInfo.getStorageMedium()) {",
            "                synchronized (tabletMigrationMap) {",
            "                    tabletMigrationMap.put(storageMedium, tabletId);",
            "                }",
            "            }",
            "",
            "            if (storageMedium != tabletMeta.getStorageMedium()) {",
            "                tabletMeta.setStorageMedium(storageMedium);",
            "            }",
            "        }",
            "    }",
            "",
            "    /**",
            "     * Update replica's version count",
            "     */",
            "    private void updateReplicaVersionCount(Replica replica, TTabletInfo backendTabletInfo) {",
            "        if (backendTabletInfo.isSetTotalVersionCount()) {",
            "            replica.setTotalVersionCount(backendTabletInfo.getTotalVersionCount());",
            "            replica.setVisibleVersionCount(backendTabletInfo.isSetVisibleVersionCount()",
            "                    ? backendTabletInfo.getVisibleVersionCount()",
            "                    : backendTabletInfo.getTotalVersionCount());",
            "        }",
            "    }",
            "",
            "    /**",
            "     * Process tablet that exists in FE but not reported by BE",
            "     */",
            "    private void processDeletedTablet(long backendId, long tabletId, TabletMeta tabletMeta,",
            "                                       ListMultimap<Long, Long> tabletDeleteFromMeta) {",
            "        if (LOG.isDebugEnabled()) {",
            "            LOG.debug(\"backend[{}] does not report tablet[{}-{}]\", backendId, tabletId, tabletMeta);",
            "        }",
            "        synchronized (tabletDeleteFromMeta) {",
            "            tabletDeleteFromMeta.put(tabletMeta.getDbId(), tabletId);",
            "        }",
            "    }",
            "",
            "    /**",
            "     * Process partition versions reported by BE",
            "     */",
            "    private void processPartitionVersions(Map<Long, Long> backendPartitionsVersion,",
            "                                           Map<Long, Long> partitionVersionSyncMap) {",
            "        for (Map.Entry<Long, Long> entry : backendPartitionsVersion.entrySet()) {",
            "            long partitionId = entry.getKey();",
            "            long backendVersion = entry.getValue();",
            "            PartitionCollectInfo partitionInfo = partitionCollectInfoMap.get(partitionId);",
            "",
            "            if (partitionInfo != null && partitionInfo.getVisibleVersion() > backendVersion) {",
            "                partitionVersionSyncMap.put(partitionId, partitionInfo.getVisibleVersion());",
            "            }",
            "        }",
            "    }",
            "",
            "    /**",
            "     * Log tablet report summary",
            "     */",
            "    private void logTabletReportSummary(long backendId, long feTabletNum,",
            "                                         Map<Long, TTablet> backendTablets,",
            "                                         Map<Long, Long> backendPartitionsVersion,",
            "                                         ListMultimap<Long, Long> tabletSyncMap,",
            "                                         ListMultimap<Long, Long> tabletDeleteFromMeta,",
            "                                         Set<Long> tabletFoundInMeta,",
            "                                         ListMultimap<TStorageMedium, Long> tabletMigrationMap,",
            "                                         Map<Long, Long> partitionVersionSyncMap,",
            "                                         Map<Long, SetMultimap<Long, TPartitionVersionInfo>> transactionsToPublish,",
            "                                         SetMultimap<Long, Long> transactionsToClear,",
            "                                         List<TTabletMetaInfo> tabletToUpdate,",
            "                                         ListMultimap<Long, Long> tabletRecoveryMap,",
            "                                         long startTime) {",
            "        long endTime = System.currentTimeMillis();",
            "        long toClearTransactionsNum = transactionsToClear.keySet().size();",
            "                                        .mapToLong(m -> m.keySet().size()).sum();",
            "                .mapToLong(m -> m.keySet().size()).sum();",
            "        long toPublishTransactionsPartitions = transactionsToPublish.values().stream()",
            "                        + \"transactions {}(partitions: {}). tabletToUpdate: {}. need recovery: {}. cost: {} ms\",",
            "                .mapToLong(m -> m.values().size()).sum();",
            "",
            "        LOG.info(\"finished to do tablet diff with backend[{}]. fe tablet num: {}, backend tablet num: {}. \"",
            "                        + \"sync: {}, metaDel: {}, foundInMeta: {}, migration: {}, \"",
            "                        + \"backend partition num: {}, backend need update: {}, \"",
            "                        + \"found invalid transactions {}(partitions: {}), \"",
            "                        + \"found republish transactions {}(partitions: {}), \"",
            "                        + \"tabletToUpdate: {}, need recovery: {}, cost: {} ms\",",
            "                backendId, feTabletNum, backendTablets.size(), tabletSyncMap.size(),",
            "                tabletToUpdate.size(), tabletRecoveryMap.size(), (end - start));",
            "                backendPartitionsVersion.size(), partitionVersionSyncMap.size(),",
            "                toClearTransactionsNum, toClearTransactionsPartitions,",
            "                toPublishTransactionsNum, toPublishTransactionsPartitions,",
            "                tabletToUpdate.size(), tabletRecoveryMap.size(), (endTime - startTime));",
            "    }"
          ]
        },
        "vulnerable_code": {
          "context": "fe/fe-core/src/main/java/org/apache/doris/catalog/TabletInvertedIndex.java",
          "class": null,
          "func": null,
          "lines": [
            "import java.util.Set;",
            "import java.util.concurrent.ForkJoinPool;",
            "import java.util.concurrent.CompletableFuture;",
            "",
            "    private ForkJoinPool taskPool = new ForkJoinPool(Runtime.getRuntime().availableProcessors());",
            "    private final ExecutorService taskPool = new ThreadPoolExecutor(",
            "                feTabletNum = replicaMetaWithBackend.size();",
            "                taskPool.submit(() -> {",
            "                    // traverse replicas in meta with this backend",
            "                    replicaMetaWithBackend.entrySet().parallelStream().forEach(entry -> {",
            "                        long tabletId = entry.getKey();",
            "                        Preconditions.checkState(tabletMetaMap.containsKey(tabletId),",
            "                                \"tablet \" + tabletId + \" not exists, backend \" + backendId);",
            "                        TabletMeta tabletMeta = tabletMetaMap.get(tabletId);",
            "",
            "                        if (backendTablets.containsKey(tabletId)) {",
            "                            TTablet backendTablet = backendTablets.get(tabletId);",
            "                            Replica replica = entry.getValue();",
            "                            tabletFoundInMeta.add(tabletId);",
            "                            TTabletInfo backendTabletInfo = backendTablet.getTabletInfos().get(0);",
            "                            TTabletMetaInfo tabletMetaInfo = null;",
            "                            if (backendTabletInfo.getReplicaId() != replica.getId()",
            "                                    && replica.getState() != ReplicaState.CLONE) {",
            "                                // Need to update replica id in BE",
            "                                tabletMetaInfo = new TTabletMetaInfo();",
            "                                tabletMetaInfo.setReplicaId(replica.getId());",
            "                            }",
            "                            PartitionCollectInfo partitionCollectInfo =",
            "                                    partitionCollectInfoMap.get(backendTabletInfo.getPartitionId());",
            "                            boolean isInMemory = partitionCollectInfo != null && partitionCollectInfo.isInMemory();",
            "                            if (isInMemory != backendTabletInfo.isIsInMemory()) {",
            "                                if (tabletMetaInfo == null) {",
            "                                    tabletMetaInfo = new TTabletMetaInfo();",
            "                                    tabletMetaInfo.setIsInMemory(isInMemory);",
            "                                }",
            "                            }",
            "                            if (Config.fix_tablet_partition_id_eq_0",
            "                                    && tabletMeta.getPartitionId() > 0",
            "                                    && backendTabletInfo.getPartitionId() == 0) {",
            "                                LOG.warn(\"be report tablet partition id not eq fe, in be {} but in fe {}\",",
            "                                        backendTabletInfo, tabletMeta);",
            "                                // Need to update partition id in BE",
            "                                tabletMetaInfo = new TTabletMetaInfo();",
            "                                tabletMetaInfo.setPartitionId(tabletMeta.getPartitionId());",
            "                            }",
            "                            // 1. (intersection)",
            "                            if (needSync(replica, backendTabletInfo)) {",
            "                                // need sync",
            "                                synchronized (tabletSyncMap) {",
            "                                    tabletSyncMap.put(tabletMeta.getDbId(), tabletId);",
            "                                }",
            "                            }",
            "                processTabletReportAsync(backendId, backendTablets, backendPartitionsVersion, storageMediumMap,",
            "",
            "                            // check and set path",
            "                            // path info of replica is only saved in Master FE",
            "                            if (backendTabletInfo.isSetPathHash()",
            "                                    && replica.getPathHash() != backendTabletInfo.getPathHash()) {",
            "                                replica.setPathHash(backendTabletInfo.getPathHash());",
            "                            }",
            "        // Process cooldown configs outside of read lock to avoid deadlock",
            "",
            "                            if (backendTabletInfo.isSetSchemaHash() && replica.getState() == ReplicaState.NORMAL",
            "                                    && replica.getSchemaHash() != backendTabletInfo.getSchemaHash()) {",
            "                                // update the schema hash only when replica is normal",
            "                                replica.setSchemaHash(backendTabletInfo.getSchemaHash());",
            "                            }",
            "        logTabletReportSummary(backendId, feTabletNum, backendTablets, backendPartitionsVersion,",
            "",
            "                            if (needRecover(replica, tabletMeta.getOldSchemaHash(), backendTabletInfo)) {",
            "                                LOG.warn(\"replica {} of tablet {} on backend {} need recovery. \"",
            "                                                + \"replica in FE: {}, report version {}, report schema hash: {},\"",
            "                                                + \" is bad: {}, is version missing: {}\",",
            "                                        replica.getId(), tabletId, backendId, replica,",
            "                                        backendTabletInfo.getVersion(),",
            "                                        backendTabletInfo.getSchemaHash(),",
            "                                        backendTabletInfo.isSetUsed() ? !backendTabletInfo.isUsed() : \"false\",",
            "                                        backendTabletInfo.isSetVersionMiss() ? backendTabletInfo.isVersionMiss() :",
            "                                                \"unset\");",
            "                                synchronized (tabletRecoveryMap) {",
            "                                    tabletRecoveryMap.put(tabletMeta.getDbId(), tabletId);",
            "                                }",
            "                            }",
            "    /**",
            "",
            "                            if (Config.enable_storage_policy && backendTabletInfo.isSetCooldownTerm()) {",
            "                                // Place tablet info in a container and process it outside of read lock to avoid",
            "                                // deadlock with OlapTable lock",
            "                                synchronized (cooldownTablets) {",
            "                                    cooldownTablets.add(Pair.of(tabletMeta, backendTabletInfo));",
            "                                }",
            "                                replica.setCooldownMetaId(backendTabletInfo.getCooldownMetaId());",
            "                                replica.setCooldownTerm(backendTabletInfo.getCooldownTerm());",
            "                            }",
            "        for (int i = 0; i < entries.size(); i += chunkSize) {",
            "",
            "                            long partitionId = tabletMeta.getPartitionId();",
            "                            if (!Config.disable_storage_medium_check) {",
            "                                // check if need migration",
            "                                TStorageMedium storageMedium = storageMediumMap.get(partitionId);",
            "                                if (storageMedium != null && backendTabletInfo.isSetStorageMedium()",
            "                                        && isLocal(storageMedium) && isLocal(backendTabletInfo.getStorageMedium())",
            "                                        && isLocal(tabletMeta.getStorageMedium())) {",
            "                                    if (storageMedium != backendTabletInfo.getStorageMedium()) {",
            "                                        synchronized (tabletMigrationMap) {",
            "                                            tabletMigrationMap.put(storageMedium, tabletId);",
            "                                        }",
            "                                    }",
            "                                    if (storageMedium != tabletMeta.getStorageMedium()) {",
            "                                        tabletMeta.setStorageMedium(storageMedium);",
            "                                    }",
            "                                }",
            "                            }",
            "            tabletFutures.add(future);",
            "",
            "                            // check if should clear transactions",
            "                            if (backendTabletInfo.isSetTransactionIds()) {",
            "                                handleBackendTransactions(backendId, backendTabletInfo.getTransactionIds(), tabletId,",
            "                                        tabletMeta, transactionsToPublish, transactionsToClear);",
            "                            } // end for txn id",
            "",
            "                            // update replicase's version count",
            "                            // no need to write log, and no need to get db lock.",
            "                            if (backendTabletInfo.isSetTotalVersionCount()) {",
            "                                replica.setTotalVersionCount(backendTabletInfo.getTotalVersionCount());",
            "                                replica.setVisibleVersionCount(backendTabletInfo.isSetVisibleVersionCount()",
            "                                        ? backendTabletInfo.getVisibleVersionCount()",
            "                                                : backendTabletInfo.getTotalVersionCount());",
            "                            }",
            "                            if (tabletMetaInfo != null) {",
            "                                tabletMetaInfo.setTabletId(tabletId);",
            "                                synchronized (tabletToUpdate) {",
            "                                    tabletToUpdate.add(tabletMetaInfo);",
            "                                }",
            "                            }",
            "                        } else {",
            "                            // 2. (meta - be)",
            "                            // may need delete from meta",
            "                            if (LOG.isDebugEnabled()) {",
            "                                LOG.debug(\"backend[{}] does not report tablet[{}-{}]\", backendId, tabletId, tabletMeta);",
            "                            }",
            "                            synchronized (tabletDeleteFromMeta) {",
            "                                tabletDeleteFromMeta.put(tabletMeta.getDbId(), tabletId);",
            "                            }",
            "                        }",
            "                    });",
            "",
            "                    backendPartitionsVersion.entrySet().parallelStream().forEach(entry -> {",
            "                        long partitionId = entry.getKey();",
            "                        long backendVersion = entry.getValue();",
            "                        PartitionCollectInfo partitionInfo = partitionCollectInfoMap.get(partitionId);",
            "                        if (partitionInfo != null && partitionInfo.getVisibleVersion() > backendVersion) {",
            "                            partitionVersionSyncMap.put(partitionId, partitionInfo.getVisibleVersion());",
            "                        }",
            "                    });",
            "                }).join();",
            "        // Process partition versions in parallel",
            "            }",
            "        } finally {",
            "            readUnlock(stamp);",
            "        }",
            "        cooldownTablets.forEach(p -> handleCooldownConf(p.first, p.second, cooldownConfToPush, cooldownConfToUpdate));",
            "",
            "        long end = System.currentTimeMillis();",
            "        // Update replica path and schema hash",
            "        long toPublishTransactionsNum = transactionsToPublish.values().stream()",
            "                                        .mapToLong(m -> m.keySet().size()).sum();",
            "                .mapToLong(m -> m.keySet().size()).sum();",
            "        long toPublishTransactionsPartitions = transactionsToPublish.values().stream()",
            "                                               .mapToLong(m -> m.values().size()).sum();",
            "        LOG.info(\"finished to do tablet diff with backend[{}]. fe tablet num: {}, backend tablet num: {}. sync: {}.\"",
            "                        + \" metaDel: {}. foundInMeta: {}. migration: {}. backend partition num: {}, backend need \"",
            "                        + \"update: {}. found invalid transactions {}(partitions: {}). found republish \"",
            "                        + \"transactions {}(partitions: {}). tabletToUpdate: {}. need recovery: {}. cost: {} ms\",",
            "                .mapToLong(m -> m.values().size()).sum();",
            "                tabletDeleteFromMeta.size(), tabletFoundInMeta.size(), tabletMigrationMap.size(),",
            "                backendPartitionsVersion.size(), partitionVersionSyncMap.size(), toClearTransactionsNum,",
            "                toClearTransactionsPartitions, toPublishTransactionsNum, toPublishTransactionsPartitions,",
            "                tabletToUpdate.size(), tabletRecoveryMap.size(), (end - start));",
            "                backendPartitionsVersion.size(), partitionVersionSyncMap.size(),"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/apache/doris/commit/da41f40ee90a0ba6beca61c3a2f8f2bc89eec168",
        "CWE": "CWE-833",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/apache/doris/pull/57382"
      },
      {
        "benign_code": {
          "context": "xmppserver/src/main/java/org/jivesoftware/openfire/nio/NettyConnection.java",
          "class": null,
          "func": null,
          "lines": []
        },
        "vulnerable_code": {
          "context": "xmppserver/src/main/java/org/jivesoftware/openfire/nio/NettyConnection.java",
          "class": null,
          "func": null,
          "lines": [
            "import java.util.Optional;",
            "import java.util.concurrent.CountDownLatch;",
            "import java.util.concurrent.TimeUnit;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "",
            "            // OF-2808: Ensure that the connection is done invoking its 'close' listeners before returning from this",
            "            // method, otherwise stream management's \"resume\" functionality breaks (the 'close' listeners have been",
            "            // observed to act on a newly attached stream/connection, instead of the old one).",
            "            final CountDownLatch latch = new CountDownLatch(1);",
            "            f.addListener(e -> Log.trace(\"Flushed any final bytes, closing connection.\"))",
            "                            }",
            "                            latch.countDown(); // Ensure we're not kept waiting! OF-2845",
            "                        });",
            "                .addListener(e -> Log.trace(\"Finished closing connection.\"));",
            "",
            "            try {",
            "                // TODO: OF-2811 Remove this blocking operation, by allowing the invokers of this method to use a Future.",
            "                if (!latch.await(10, TimeUnit.MINUTES)) {",
            "                    Log.warn(\"Timed out waiting for close listeners to complete.\");",
            "                }",
            "            } catch (InterruptedException e) {",
            "                Log.debug(\"Stopped waiting on connection being closed, as an interrupt happened.\", e);",
            "            }",
            "        }"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/igniterealtime/Openfire/commit/fceb3651929f9e40796b5e59d09b03c83d22a4f8",
        "CWE": "CWE-833",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/igniterealtime/Openfire/pull/2696"
      },
      {
        "benign_code": {
          "context": "modules/library/referencing/src/main/java/org/geotools/referencing/factory/epsg/ThreadedEpsgFactory.java",
          "class": null,
          "func": null,
          "lines": [
            "        assert Thread.holdsLock(this);",
            "        /*",
            "         * We are locking on ReferencingFactoryFinder to avoid deadlocks.",
            "         * @see DeferredAuthorityFactory#getBackingStore()",
            "         */",
            "        assert Thread.holdsLock(ReferencingFactoryFinder.class);",
            "        final Hints sourceHints = new Hints(hints);"
          ]
        },
        "vulnerable_code": {
          "context": "modules/library/referencing/src/main/java/org/geotools/referencing/factory/epsg/ThreadedEpsgFactory.java",
          "class": null,
          "func": null,
          "lines": [
            "    private AbstractAuthorityFactory createBackingStore0() throws FactoryException, SQLException {",
            "        assert Thread.holdsLock(this);",
            "        /*"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/geotools/geotools/commit/b5b2597fcd7e5ad8e569d7bc28776668b97a0784",
        "CWE": "CWE-833",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/geotools/geotools/pull/3678"
      }
    ],
    "CWD-1131": [
      {
        "benign_code": {
          "context": "s3stream/src/main/java/com/automq/stream/s3/metrics/stats/NetworkStats.java",
          "class": null,
          "func": null,
          "lines": [
            "    private static volatile NetworkStats instance = null;",
            "    private static final NetworkStats INSTANCE = new NetworkStats();",
            "    // <StreamId, <FastReadBytes, SlowReadBytes>>",
            "        return instance;",
            "        return INSTANCE;",
            "    }",
            "                : networkOutboundUsageTotalStats.computeIfAbsent(strategy, k -> S3StreamMetricsManager.buildNetworkOutboundUsageMetric(strategy, networkOutboundUsageTotal::inc));",
            "        Map<ThrottleStrategy, CounterMetric> stats = type == AsyncNetworkBandwidthLimiter.Type.INBOUND ? networkInboundUsageTotalStats : networkOutboundUsageTotalStats;",
            "        CounterMetric metric = stats.get(strategy);",
            "        if (metric == null) {",
            "            if (type == AsyncNetworkBandwidthLimiter.Type.INBOUND) {",
            "                metric = stats.computeIfAbsent(strategy, k -> S3StreamMetricsManager.buildNetworkInboundUsageMetric(strategy, networkInboundUsageTotal::inc));",
            "            } else {",
            "                metric = stats.computeIfAbsent(strategy, k -> S3StreamMetricsManager.buildNetworkOutboundUsageMetric(strategy, networkOutboundUsageTotal::inc));",
            "            }",
            "        }",
            "        return metric;",
            "    }"
          ]
        },
        "vulnerable_code": {
          "context": "s3stream/src/main/java/com/automq/stream/s3/metrics/stats/NetworkStats.java",
          "class": null,
          "func": null,
          "lines": [
            "public class NetworkStats {",
            "    private static volatile NetworkStats instance = null;",
            "    private static final NetworkStats INSTANCE = new NetworkStats();",
            "    public static NetworkStats getInstance() {",
            "        if (instance == null) {",
            "            synchronized (NetworkStats.class) {",
            "                if (instance == null) {",
            "                    instance = new NetworkStats();",
            "                }",
            "            }",
            "        }",
            "        return instance;",
            "        return INSTANCE;",
            "    public CounterMetric networkUsageTotalStats(AsyncNetworkBandwidthLimiter.Type type, ThrottleStrategy strategy) {",
            "        return type == AsyncNetworkBandwidthLimiter.Type.INBOUND",
            "                ? networkInboundUsageTotalStats.computeIfAbsent(strategy, k -> S3StreamMetricsManager.buildNetworkInboundUsageMetric(strategy, networkInboundUsageTotal::inc))",
            "                : networkOutboundUsageTotalStats.computeIfAbsent(strategy, k -> S3StreamMetricsManager.buildNetworkOutboundUsageMetric(strategy, networkOutboundUsageTotal::inc));",
            "        Map<ThrottleStrategy, CounterMetric> stats = type == AsyncNetworkBandwidthLimiter.Type.INBOUND ? networkInboundUsageTotalStats : networkOutboundUsageTotalStats;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/AutoMQ/automq/commit/fdb40ead4b658e7a1a19979bd4cf19db1b549320",
        "CWE": "CWE-543",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/AutoMQ/automq/pull/2998"
      },
      {
        "benign_code": {
          "context": "shadows/framework/src/main/java/org/robolectric/shadows/ShadowUIModeManager.java",
          "class": null,
          "func": null,
          "lines": [
            "  private final Object lock = new Object();",
            "  @Deprecated public volatile int currentModeType = sharedCurrentModeType;",
            "",
            "  private static int currentNightMode = UiModeManager.MODE_NIGHT_AUTO;",
            "  private static int lastFlags;",
            "  private static int lastCarModePriority;",
            "",
            "  // The public field `currentModeType` is deprecated. To maintain binary compatibility,",
            "  // it remains an instance field. It is initialized with the shared static value upon",
            "  // creation of a shadow instance, but it is not kept in sync afterwards. For up-to-date",
            "  // values, use `getCurrentModeType()`. Direct access to `currentModeType` is discouraged",
            "  // as it may yield stale data.",
            "  private static int sharedCurrentModeType = Configuration.UI_MODE_TYPE_UNDEFINED;",
            "  private static int currentApplicationNightMode = 0;",
            "  private static final Map<Integer, Set<String>> activeProjectionTypes = new HashMap<>();",
            "  private static boolean failOnProjectionToggle;",
            "  private static final Object lock = new Object();",
            "",
            "  private int nightModeCustomType = UiModeManager.MODE_NIGHT_CUSTOM_TYPE_UNKNOWN;",
            "  private static int nightModeCustomType = UiModeManager.MODE_NIGHT_CUSTOM_TYPE_UNKNOWN;",
            "",
            "  private boolean isNightModeOn = false;",
            "  private static boolean isNightModeOn = false;",
            "",
            "    return currentModeType;",
            "    synchronized (lock) {",
            "      return sharedCurrentModeType;",
            "    }",
            "  }",
            "    this.currentModeType = modeType;",
            "    synchronized (lock) {",
            "      sharedCurrentModeType = modeType;",
            "      currentModeType = sharedCurrentModeType;",
            "    }",
            "  }",
            "    lastFlags = flags;",
            "    synchronized (lock) {",
            "      sharedCurrentModeType = Configuration.UI_MODE_TYPE_CAR;",
            "      currentModeType = sharedCurrentModeType;",
            "      lastCarModePriority = priority;",
            "      lastFlags = flags;",
            "    }",
            "  }",
            "    lastFlags = flags;",
            "    synchronized (lock) {",
            "      sharedCurrentModeType = Configuration.UI_MODE_TYPE_NORMAL;",
            "      currentModeType = sharedCurrentModeType;",
            "      lastFlags = flags;",
            "    }",
            "  }",
            "    return lastCarModePriority;",
            "    synchronized (lock) {",
            "      return lastCarModePriority;",
            "    }",
            "  }",
            "    return lastFlags;",
            "    synchronized (lock) {",
            "      return lastFlags;",
            "    }",
            "  }",
            "    return currentNightMode;",
            "    synchronized (lock) {",
            "      return currentNightMode;",
            "    }",
            "  }",
            "      return projections;",
            "    synchronized (lock) {",
            "      if (projectionType == UiModeManager.PROJECTION_TYPE_ALL) {",
            "        Set<String> projections = new HashSet<>();",
            "        activeProjectionTypes.values().forEach(projections::addAll);",
            "        return projections;",
            "      }",
            "      return activeProjectionTypes.getOrDefault(projectionType, new HashSet<>());",
            "    }",
            "    return currentApplicationNightMode;",
            "    synchronized (lock) {",
            "      return currentApplicationNightMode;",
            "    }",
            "  }",
            "    return new HashSet<>(activeProjectionTypes.keySet());",
            "    synchronized (lock) {",
            "      return new HashSet<>(activeProjectionTypes.keySet());",
            "    }",
            "  }",
            "    this.failOnProjectionToggle = failOnProjectionToggle;",
            "    synchronized (lock) {",
            "      ShadowUIModeManager.failOnProjectionToggle = failOnProjectionToggle;",
            "    }",
            "  }",
            "    currentApplicationNightMode = mode;",
            "    synchronized (lock) {",
            "      currentApplicationNightMode = mode;",
            "    }",
            "  }",
            "    activeProjectionTypes.put(projectionType, projections);",
            "    synchronized (lock) {",
            "      if (projectionType == UiModeManager.PROJECTION_TYPE_AUTOMOTIVE) {",
            "        assertHasPermission(android.Manifest.permission.TOGGLE_AUTOMOTIVE_PROJECTION);",
            "      }",
            "      if (failOnProjectionToggle) {",
            "        return false;",
            "      }",
            "      Set<String> projections = activeProjectionTypes.getOrDefault(projectionType, new HashSet<>());",
            "      projections.add(RuntimeEnvironment.getApplication().getPackageName());",
            "      activeProjectionTypes.put(projectionType, projections);",
            "",
            "    return true;",
            "      return true;",
            "    }",
            "  }",
            "        activeProjectionTypes.put(projectionType, projections);",
            "    synchronized (lock) {",
            "      if (projectionType == UiModeManager.PROJECTION_TYPE_AUTOMOTIVE) {",
            "        assertHasPermission(android.Manifest.permission.TOGGLE_AUTOMOTIVE_PROJECTION);",
            "      }",
            "      if (failOnProjectionToggle) {",
            "        return false;",
            "      }",
            "      String packageName = RuntimeEnvironment.getApplication().getPackageName();",
            "      Set<String> projections = activeProjectionTypes.getOrDefault(projectionType, new HashSet<>());",
            "      if (projections.contains(packageName)) {",
            "        projections.remove(packageName);",
            "        if (projections.isEmpty()) {",
            "          activeProjectionTypes.remove(projectionType);",
            "        } else {",
            "          activeProjectionTypes.put(projectionType, projections);",
            "        }",
            "        return true;",
            "      }",
            "    return false;",
            "      return false;",
            "    }",
            "  }",
            "    }",
            "    synchronized (lock) {",
            "      // When the new instance is initialized, the currentModeType will use this",
            "      // shared type to initialize it to keep the initial value same.",
            "      sharedCurrentModeType = Configuration.UI_MODE_TYPE_UNDEFINED;",
            "      currentNightMode = UiModeManager.MODE_NIGHT_AUTO;",
            "      lastFlags = 0;",
            "      lastCarModePriority = 0;",
            "      currentApplicationNightMode = 0;",
            "      activeProjectionTypes.clear();",
            "      failOnProjectionToggle = false;",
            "      nightModeCustomType = UiModeManager.MODE_NIGHT_CUSTOM_TYPE_UNKNOWN;",
            "      isNightModeOn = false;",
            "    }",
            "  }"
          ]
        },
        "vulnerable_code": {
          "context": "shadows/framework/src/main/java/org/robolectric/shadows/ShadowUIModeManager.java",
          "class": null,
          "func": null,
          "lines": [
            "   */",
            "  @Deprecated public int currentModeType = Configuration.UI_MODE_TYPE_UNDEFINED;",
            "",
            "  private int currentNightMode = UiModeManager.MODE_NIGHT_AUTO;",
            "  private int lastFlags;",
            "  private int lastCarModePriority;",
            "  private int currentApplicationNightMode = 0;",
            "  private final Map<Integer, Set<String>> activeProjectionTypes = new HashMap<>();",
            "  private boolean failOnProjectionToggle;",
            "  private final Object lock = new Object();",
            "  @Deprecated public volatile int currentModeType = sharedCurrentModeType;",
            "  @GuardedBy(\"lock\")",
            "  private int nightModeCustomType = UiModeManager.MODE_NIGHT_CUSTOM_TYPE_UNKNOWN;",
            "  private static int nightModeCustomType = UiModeManager.MODE_NIGHT_CUSTOM_TYPE_UNKNOWN;",
            "  @GuardedBy(\"lock\")",
            "  private boolean isNightModeOn = false;",
            "  private static boolean isNightModeOn = false;",
            "  protected int getCurrentModeType() {",
            "    return currentModeType;",
            "    synchronized (lock) {",
            "  public void setCurrentModeType(int modeType) {",
            "    this.currentModeType = modeType;",
            "    synchronized (lock) {",
            "  protected void enableCarMode(int priority, int flags) {",
            "    currentModeType = Configuration.UI_MODE_TYPE_CAR;",
            "    lastCarModePriority = priority;",
            "    lastFlags = flags;",
            "    synchronized (lock) {",
            "  protected void disableCarMode(int flags) {",
            "    currentModeType = Configuration.UI_MODE_TYPE_NORMAL;",
            "    lastFlags = flags;",
            "    synchronized (lock) {",
            "  public int getLastCarModePriority() {",
            "    return lastCarModePriority;",
            "    synchronized (lock) {",
            "  public int getLastFlags() {",
            "    return lastFlags;",
            "    synchronized (lock) {",
            "  protected int getNightMode() {",
            "    return currentNightMode;",
            "    synchronized (lock) {",
            "  protected Set<String> getProjectingPackages(int projectionType) {",
            "    if (projectionType == UiModeManager.PROJECTION_TYPE_ALL) {",
            "      Set<String> projections = new HashSet<>();",
            "      activeProjectionTypes.values().forEach(projections::addAll);",
            "      return projections;",
            "    synchronized (lock) {",
            "    }",
            "    return activeProjectionTypes.getOrDefault(projectionType, new HashSet<>());",
            "  }",
            "  public int getApplicationNightMode() {",
            "    return currentApplicationNightMode;",
            "    synchronized (lock) {",
            "  public Set<Integer> getActiveProjectionTypes() {",
            "    return new HashSet<>(activeProjectionTypes.keySet());",
            "    synchronized (lock) {",
            "  public void setFailOnProjectionToggle(boolean failOnProjectionToggle) {",
            "    this.failOnProjectionToggle = failOnProjectionToggle;",
            "    synchronized (lock) {",
            "  protected void setApplicationNightMode(int mode) {",
            "    currentApplicationNightMode = mode;",
            "    synchronized (lock) {",
            "  protected boolean requestProjection(int projectionType) {",
            "    if (projectionType == UiModeManager.PROJECTION_TYPE_AUTOMOTIVE) {",
            "      assertHasPermission(android.Manifest.permission.TOGGLE_AUTOMOTIVE_PROJECTION);",
            "    }",
            "    if (failOnProjectionToggle) {",
            "      return false;",
            "    }",
            "    Set<String> projections = activeProjectionTypes.getOrDefault(projectionType, new HashSet<>());",
            "    projections.add(RuntimeEnvironment.getApplication().getPackageName());",
            "    activeProjectionTypes.put(projectionType, projections);",
            "    synchronized (lock) {",
            "",
            "    return true;",
            "      return true;",
            "  protected boolean releaseProjection(int projectionType) {",
            "    if (projectionType == UiModeManager.PROJECTION_TYPE_AUTOMOTIVE) {",
            "      assertHasPermission(android.Manifest.permission.TOGGLE_AUTOMOTIVE_PROJECTION);",
            "    }",
            "    if (failOnProjectionToggle) {",
            "      return false;",
            "    }",
            "    String packageName = RuntimeEnvironment.getApplication().getPackageName();",
            "    Set<String> projections = activeProjectionTypes.getOrDefault(projectionType, new HashSet<>());",
            "    if (projections.contains(packageName)) {",
            "      projections.remove(packageName);",
            "      if (projections.isEmpty()) {",
            "        activeProjectionTypes.remove(projectionType);",
            "      } else {",
            "        activeProjectionTypes.put(projectionType, projections);",
            "    synchronized (lock) {",
            "      }",
            "      return true;",
            "    }",
            "",
            "    return false;",
            "      return false;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/robolectric/robolectric/commit/0cad7bbc682123d0316cead431a21ec855456fd4",
        "CWE": "CWE-543",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/robolectric/robolectric/pull/9485"
      },
      {
        "benign_code": {
          "context": "core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftTransportPool.java",
          "class": null,
          "func": null,
          "lines": [
            "import java.util.Set;",
            "import java.util.concurrent.ConcurrentHashMap;",
            "import java.util.concurrent.locks.Lock;",
            "import java.util.concurrent.locks.ReentrantLock;",
            "import java.util.function.Consumer;",
            "import java.util.function.Supplier;",
            "",
            "import org.apache.accumulo.core.util.HostAndPort;",
            "import org.apache.accumulo.core.util.Once;",
            "import org.apache.accumulo.core.util.Pair;",
            "import com.google.common.collect.Iterables;",
            "",
            "import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;",
            "",
            "    }",
            "",
            "    private void removeExpiredConnections(final ArrayList<CachedConnection> expired,",
            "        final long killTime) {",
            "      long currTime = System.currentTimeMillis();",
            "      while (isLastUnreservedExpired(currTime, killTime)) {",
            "        expired.add(unreserved.removeLast());",
            "      }",
            "    }",
            "",
            "    boolean isLastUnreservedExpired(final long currTime, final long killTime) {",
            "      return !unreserved.isEmpty() && (currTime - unreserved.peekLast().lastReturnTime) > killTime;",
            "    }",
            "",
            "    void checkReservedForStuckIO() {",
            "      reserved.values().forEach(c -> c.transport.checkForStuckIO(STUCK_THRESHOLD));",
            "    }",
            "",
            "    void closeAllTransports() {",
            "      closeTransports(unreserved);",
            "      closeTransports(reserved.values());",
            "    }",
            "",
            "    void closeTransports(final Iterable<CachedConnection> stream) {",
            "      stream.forEach((connection) -> {",
            "        try {",
            "          connection.transport.close();",
            "        } catch (Exception e) {",
            "          log.debug(\"Error closing transport during shutdown\", e);",
            "        }",
            "      });",
            "    }",
            "",
            "    CachedConnection removeReserved(CachedTTransport transport) {",
            "      return reserved.remove(transport);",
            "    }",
            "  }",
            "  private Map<ThriftTransportKey,CachedConnections> cache = new HashMap<>();",
            "  private static class ConnectionPool {",
            "    final Lock[] locks;",
            "    final ConcurrentHashMap<ThriftTransportKey,CachedConnections> connections =",
            "        new ConcurrentHashMap<>();",
            "    private volatile boolean shutdown = false;",
            "",
            "    ConnectionPool() {",
            "      // intentionally using a prime number, don't use 31",
            "      locks = new Lock[37];",
            "      for (int i = 0; i < locks.length; i++) {",
            "        locks[i] = new ReentrantLock();",
            "      }",
            "    }",
            "",
            "    Set<ThriftTransportKey> getThriftTransportKeys() {",
            "      return connections.keySet();",
            "    }",
            "",
            "    /**",
            "     * Reserve and return a new {@link CachedConnection} from the {@link CachedConnections} mapped",
            "     * to the specified transport key. If a {@link CachedConnections} is not found, one will be",
            "     * created.",
            "     *",
            "     * <p>",
            "     *",
            "     * This operation locks access to the mapping for the key in {@link ConnectionPool#connections}",
            "     * until the operation completes.",
            "     *",
            "     * @param key",
            "     *          the transport key",
            "     * @return the reserved {@link CachedConnection}",
            "     */",
            "    CachedConnection reserveAny(final ThriftTransportKey key) {",
            "      // It's possible that multiple locks from executeWithinLock will overlap with a single lock",
            "      // inside the ConcurrentHashMap which can unnecessarily block threads. Access the",
            "      // ConcurrentHashMap outside of executeWithinLock to prevent this.",
            "      var connections = getOrCreateCachedConnections(key);",
            "      return executeWithinLock(key, connections::reserveAny);",
            "    }",
            "",
            "    /**",
            "     * Reserve and return a new {@link CachedConnection} from the {@link CachedConnections} mapped",
            "     * to the specified transport key. If a {@link CachedConnections} is not found, null will be",
            "     * returned.",
            "     *",
            "     * <p>",
            "     *",
            "     * This operation locks access to the mapping for the key in {@link ConnectionPool#connections}",
            "     * until the operation completes.",
            "     *",
            "     * @param key",
            "     *          the transport key",
            "     * @return the reserved {@link CachedConnection}, or null if none were available.",
            "     */",
            "    CachedConnection reserveAnyIfPresent(final ThriftTransportKey key) {",
            "      // It's possible that multiple locks from executeWithinLock will overlap with a single lock",
            "      // inside the ConcurrentHashMap which can unnecessarily block threads. Access the",
            "      // ConcurrentHashMap outside of executeWithinLock to prevent this.",
            "      var connections = getCachedConnections(key);",
            "      return connections == null ? null : executeWithinLock(key, connections::reserveAny);",
            "    }",
            "",
            "    /**",
            "     * Puts the specified connection into the reserved map of the {@link CachedConnections} for the",
            "     * specified transport key. If a {@link CachedConnections} is not found, one will be created.",
            "     *",
            "     * <p>",
            "     *",
            "     * This operation locks access to the mapping for the key in {@link ConnectionPool#connections}",
            "     * until the operation completes.",
            "     *",
            "     * @param key",
            "     *          the transport key",
            "     * @param connection",
            "     *          the reserved connection",
            "     */",
            "    void putReserved(final ThriftTransportKey key, final CachedConnection connection) {",
            "      // It's possible that multiple locks from executeWithinLock will overlap with a single lock",
            "      // inside the ConcurrentHashMap which can unnecessarily block threads. Access the",
            "      // ConcurrentHashMap outside of executeWithinLock to prevent this.",
            "      var connections = getOrCreateCachedConnections(key);",
            "      executeWithinLock(key, () -> connections.reserved.put(connection.transport, connection));",
            "    }",
            "",
            "    /**",
            "     * Returns the connection for the specified transport back to the queue of unreserved",
            "     * connections for the {@link CachedConnections} for the specified transport's key. If a",
            "     * {@link CachedConnections} is not found, one will be created. If the transport saw an error,",
            "     * the connection for the transport will be unreserved, and it and all other unreserved",
            "     * connections will be added to the specified toBeClosed list, and the connections' unreserved",
            "     * list will be cleared.",
            "     *",
            "     * <p>",
            "     *",
            "     * This operation locks access to the mapping for the key in {@link ConnectionPool#connections}",
            "     * until the operation completes.",
            "     *",
            "     * @param transport",
            "     *          the transport",
            "     * @param toBeClosed",
            "     *          the list to add connections that must be closed after this operation finishes",
            "     * @return true if the connection for the transport existed and was initially reserved, or false",
            "     *         otherwise",
            "     */",
            "    boolean returnTransport(final CachedTTransport transport,",
            "        final List<CachedConnection> toBeClosed) {",
            "      // It's possible that multiple locks from executeWithinLock will overlap with a single lock",
            "      // inside the ConcurrentHashMap which can unnecessarily block threads. Access the",
            "      // ConcurrentHashMap outside of executeWithinLock to prevent this.",
            "      var connections = getOrCreateCachedConnections(transport.getCacheKey());",
            "      return executeWithinLock(transport.getCacheKey(),",
            "          () -> unreserveConnection(transport, connections, toBeClosed)); // inline",
            "    }",
            "",
            "    @SuppressFBWarnings(value = \"UL_UNRELEASED_LOCK\",",
            "        justification = \"FindBugs doesn't recognize that all locks in ConnectionPool.locks are subsequently unlocked in the try-finally in ConnectionPool.shutdown()\")",
            "    void shutdown() {",
            "      // Obtain all locks.",
            "      for (Lock lock : locks) {",
            "        lock.lock();",
            "      }",
            "",
            "      // All locks are now acquired, so nothing else should be able to run concurrently...",
            "      try {",
            "        // Check if an shutdown has already been initiated.",
            "        if (shutdown) {",
            "          return;",
            "        }",
            "        shutdown = true;",
            "        connections.values().forEach(CachedConnections::closeAllTransports);",
            "      } finally {",
            "        for (Lock lock : locks) {",
            "          lock.unlock();",
            "        }",
            "      }",
            "    }",
            "",
            "    <T> T executeWithinLock(final ThriftTransportKey key, Supplier<T> function) {",
            "      Lock lock = getLock(key);",
            "      try {",
            "        return function.get();",
            "      } finally {",
            "        lock.unlock();",
            "      }",
            "    }",
            "",
            "    void executeWithinLock(final ThriftTransportKey key, Consumer<ThriftTransportKey> consumer) {",
            "      Lock lock = getLock(key);",
            "      try {",
            "        consumer.accept(key);",
            "      } finally {",
            "        lock.unlock();",
            "      }",
            "    }",
            "",
            "    Lock getLock(final ThriftTransportKey key) {",
            "      Lock lock = locks[(key.hashCode() & Integer.MAX_VALUE) % locks.length];",
            "",
            "      lock.lock();",
            "",
            "      if (shutdown) {",
            "        lock.unlock();",
            "        throw new TransportPoolShutdownException(",
            "            \"The Accumulo singleton for connection pooling is disabled.  This is likely caused by \"",
            "                + \"all AccumuloClients being closed or garbage collected.\");",
            "      }",
            "",
            "      return lock;",
            "    }",
            "",
            "    CachedConnections getCachedConnections(final ThriftTransportKey key) {",
            "      return connections.get(key);",
            "    }",
            "",
            "    CachedConnections getOrCreateCachedConnections(final ThriftTransportKey key) {",
            "      return connections.computeIfAbsent(key, k -> new CachedConnections());",
            "    }",
            "",
            "    boolean unreserveConnection(final CachedTTransport transport,",
            "        final CachedConnections connections, final List<CachedConnection> toBeClosed) {",
            "      if (connections != null) {",
            "        CachedConnection connection = connections.removeReserved(transport);",
            "        if (connection != null) {",
            "          if (transport.sawError) {",
            "            unreserveConnectionAndClearUnreserved(connections, connection, toBeClosed);",
            "          } else {",
            "            returnConnectionToUnreserved(connections, connection);",
            "          }",
            "          return true;",
            "        }",
            "      }",
            "      return false;",
            "    }",
            "",
            "    void unreserveConnectionAndClearUnreserved(final CachedConnections connections,",
            "        final CachedConnection connection, final List<CachedConnection> toBeClosed) {",
            "      toBeClosed.add(connection);",
            "      connection.unreserve();",
            "      // Remove all unreserved cached connection when a sever has an error, not just the",
            "      // connection that was returned.",
            "      toBeClosed.addAll(connections.unreserved);",
            "      connections.unreserved.clear();",
            "    }",
            "",
            "    void returnConnectionToUnreserved(final CachedConnections connections,",
            "        final CachedConnection connection) {",
            "      log.trace(\"Returned connection {} ioCount: {}\", connection.transport.getCacheKey(),",
            "          connection.transport.ioCount);",
            "      connection.lastReturnTime = System.currentTimeMillis();",
            "      connection.unreserve();",
            "      // Using LIFO ensures that when the number of pooled connections exceeds the working",
            "      // set size that the idle times at the end of the list grow. The connections with",
            "      // large idle times will be cleaned up. Using a FIFO could continually reset the idle",
            "      // times of all connections, even when there are more than the working set size.",
            "      connections.unreserved.push(connection);",
            "    }",
            "",
            "    List<CachedConnection> removeExpiredConnections(final long killTime) {",
            "      ArrayList<CachedConnection> expired = new ArrayList<>();",
            "      for (Entry<ThriftTransportKey,CachedConnections> entry : connections.entrySet()) {",
            "        CachedConnections connections = entry.getValue();",
            "        executeWithinLock(entry.getKey(), (key) -> {",
            "          connections.removeExpiredConnections(expired, killTime);",
            "          connections.checkReservedForStuckIO();",
            "        });",
            "      }",
            "      return expired;",
            "    }",
            "  }",
            "",
            "  private final ConnectionPool connectionPool = new ConnectionPool();",
            "",
            "  private Map<ThriftTransportKey,Long> errorCount = new HashMap<>();",
            "  private Thread checkThread;",
            "  private Once checkStarter = new Once(() -> {",
            "    new Daemon(new Closer(instance), \"Thrift Connection Pool Checker\").start();",
            "  });",
            "",
            "",
            "        pool.closeExpiredConnections();",
            "        Thread.sleep(500);",
            "    return createNewTransport(cacheKey);",
            "    ConnectionPool pool = getConnectionPool();",
            "    CachedConnection connection = pool.reserveAny(cacheKey);",
            "",
            "    if (connection != null) {",
            "      log.trace(\"Using existing connection to {}\", cacheKey.getServer());",
            "      return connection.transport;",
            "    } else {",
            "      return createNewTransport(cacheKey);",
            "    }",
            "  }",
            "      synchronized (this) {",
            "      ConnectionPool pool = getConnectionPool();",
            "",
            "        serversSet.retainAll(getCache().keySet());",
            "      // randomly pick a server from the connection cache",
            "      serversSet.retainAll(pool.getThriftTransportKeys());",
            "",
            "          Collections.shuffle(cachedServers, random);",
            "      if (serversSet.size() > 0) {",
            "        ArrayList<ThriftTransportKey> cachedServers = new ArrayList<>(serversSet);",
            "        Collections.shuffle(cachedServers, random);",
            "",
            "            }",
            "        for (ThriftTransportKey ttk : cachedServers) {",
            "          CachedConnection connection = pool.reserveAny(ttk);",
            "          if (connection != null) {",
            "            final String serverAddr = ttk.getServer().toString();",
            "            log.trace(\"Using existing connection to {}\", serverAddr);",
            "            return new Pair<>(serverAddr, connection.transport);",
            "          }",
            "",
            "        }",
            "",
            "    ConnectionPool pool = getConnectionPool();",
            "    int retryCount = 0;",
            "    while (servers.size() > 0 && retryCount < 10) {",
            "",
            "      int index = random.nextInt(servers.size());",
            "          }",
            "        CachedConnection connection = pool.reserveAnyIfPresent(ttk);",
            "        if (connection != null) {",
            "          return new Pair<>(ttk.getServer().toString(), connection.transport);",
            "        }",
            "    cc.reserve();",
            "    CachedConnection connection = new CachedConnection(tsc);",
            "    connection.reserve();",
            "",
            "      }",
            "      ConnectionPool pool = getConnectionPool();",
            "      pool.putReserved(cacheKey, connection);",
            "    } catch (TransportPoolShutdownException e) {",
            "      cc.transport.close();",
            "      connection.transport.close();",
            "      throw e;",
            "    return cc.transport;",
            "",
            "    return connection.transport;",
            "  }",
            "    if (tsc == null) {",
            "  public void returnTransport(TTransport transport) {",
            "    if (transport == null) {",
            "      return;",
            "",
            "    CachedTTransport cachedTransport = (CachedTTransport) transport;",
            "    ArrayList<CachedConnection> closeList = new ArrayList<>();",
            "    ConnectionPool pool = getConnectionPool();",
            "    boolean existInCache = pool.returnTransport(cachedTransport, closeList);",
            "",
            "            log.trace(\"Returned connection had error {}\", ctsc.getCacheKey());",
            "    // close outside of sync block",
            "    closeList.forEach((connection) -> {",
            "      try {",
            "        connection.transport.close();",
            "      } catch (Exception e) {",
            "        log.debug(\"Failed to close connection w/ errors\", e);",
            "      }",
            "    });",
            "",
            "            Long ecount = errorCount.merge(ctsc.getCacheKey(), 1L, Long::sum);",
            "    if (cachedTransport.sawError) {",
            "",
            "            errorTime.computeIfAbsent(ctsc.getCacheKey(), k -> System.currentTimeMillis());",
            "      boolean shouldWarn = false;",
            "      Long ecount = null;",
            "",
            "            }",
            "      synchronized (errorCount) {",
            "",
            "            cachedConnection.unreserve();",
            "        ecount = errorCount.merge(cachedTransport.getCacheKey(), 1L, Long::sum);",
            "",
            "            cachedConns.unreserved.clear();",
            "        // logs the first time an error occurred",
            "        errorTime.computeIfAbsent(cachedTransport.getCacheKey(), k -> System.currentTimeMillis());",
            "",
            "          existInCache = true;",
            "        if (ecount >= ERROR_THRESHOLD && serversWarnedAbout.add(cachedTransport.getCacheKey())) {",
            "          // boolean facilitates logging outside of lock",
            "          shouldWarn = true;",
            "        }",
            "        log.debug(\"Failed to close connection w/ errors\", e);",
            "      log.trace(\"Returned connection had error {}\", cachedTransport.getCacheKey());",
            "",
            "      if (shouldWarn) {",
            "        log.warn(\"Server {} had {} failures in a short time period, will not complain anymore\",",
            "            cachedTransport.getCacheKey(), ecount);",
            "      }",
            "      tsc.close();",
            "      transport.close();",
            "    }",
            "    }",
            "  public void startCheckerThread() {",
            "    checkStarter.run();",
            "  }",
            "      }",
            "  void closeExpiredConnections() {",
            "    List<CachedConnection> expiredConnections;",
            "",
            "      this.cache = null;",
            "    ConnectionPool pool = getConnectionPool();",
            "    expiredConnections = pool.removeExpiredConnections(killTime);",
            "",
            "        throw new RuntimeException(e);",
            "    synchronized (errorCount) {",
            "      Iterator<Entry<ThriftTransportKey,Long>> iter = errorTime.entrySet().iterator();",
            "      while (iter.hasNext()) {",
            "        Entry<ThriftTransportKey,Long> entry = iter.next();",
            "        long delta = System.currentTimeMillis() - entry.getValue();",
            "        if (delta >= STUCK_THRESHOLD) {",
            "          errorCount.remove(entry.getKey());",
            "          iter.remove();",
            "        }",
            "      }",
            "    }",
            "",
            "    // Close connections outside of sync block",
            "    expiredConnections.forEach((c) -> c.transport.close());",
            "  }",
            "",
            "  private void shutdown() {",
            "    connectionPool.shutdown();",
            "  }",
            "    return cache;",
            "  private ConnectionPool getConnectionPool() {",
            "    return connectionPool;",
            "  }"
          ]
        },
        "vulnerable_code": {
          "context": "core/src/main/java/org/apache/accumulo/core/clientImpl/ThriftTransportPool.java",
          "class": null,
          "func": null,
          "lines": [
            "import com.google.common.base.Preconditions;",
            "import com.google.common.collect.Iterables;",
            "",
            "    public CachedConnection reserveAny() {",
            "",
            "      CachedConnection cachedConnection = unreserved.poll(); // safe pop",
            "",
            "  private Map<ThriftTransportKey,CachedConnections> cache = new HashMap<>();",
            "  private static class ConnectionPool {",
            "",
            "  private Thread checkThread;",
            "  private Once checkStarter = new Once(() -> {",
            "      while (true) {",
            "",
            "        ArrayList<CachedConnection> connectionsToClose = new ArrayList<>();",
            "",
            "        synchronized (pool) {",
            "          for (CachedConnections cachedConns : pool.getCache().values()) {",
            "            Deque<CachedConnection> unres = cachedConns.unreserved;",
            "",
            "            long currTime = System.currentTimeMillis();",
            "",
            "            // The following code is structured to avoid removing from the middle of the array",
            "            // deqeue which would be costly. It also assumes the oldest are at the end.",
            "            while (!unres.isEmpty() && currTime - unres.peekLast().lastReturnTime > pool.killTime) {",
            "              connectionsToClose.add(unres.removeLast());",
            "            }",
            "",
            "            for (CachedConnection cachedConnection : cachedConns.reserved.values()) {",
            "              cachedConnection.transport.checkForStuckIO(STUCK_THRESHOLD);",
            "            }",
            "          }",
            "",
            "          Iterator<Entry<ThriftTransportKey,Long>> iter = pool.errorTime.entrySet().iterator();",
            "          while (iter.hasNext()) {",
            "            Entry<ThriftTransportKey,Long> entry = iter.next();",
            "            long delta = System.currentTimeMillis() - entry.getValue();",
            "            if (delta >= STUCK_THRESHOLD) {",
            "              pool.errorCount.remove(entry.getKey());",
            "              iter.remove();",
            "            }",
            "          }",
            "        }",
            "",
            "        // close connections outside of sync block",
            "        for (CachedConnection cachedConnection : connectionsToClose) {",
            "          cachedConnection.transport.close();",
            "        }",
            "",
            "        pool.closeExpiredConnections();",
            "    cacheKey.precomputeHashCode();",
            "    synchronized (this) {",
            "      // atomically reserve location if it exist in cache",
            "      CachedConnection cachedConnection =",
            "          getCache().computeIfAbsent(cacheKey, ck -> new CachedConnections()).reserveAny();",
            "      if (cachedConnection != null) {",
            "        log.trace(\"Using existing connection to {}\", cacheKey.getServer());",
            "        return cachedConnection.transport;",
            "      }",
            "    }",
            "",
            "    return createNewTransport(cacheKey);",
            "    ConnectionPool pool = getConnectionPool();",
            "",
            "      synchronized (this) {",
            "      ConnectionPool pool = getConnectionPool();",
            "",
            "        // randomly pick a server from the connection cache",
            "        serversSet.retainAll(getCache().keySet());",
            "      // randomly pick a server from the connection cache",
            "",
            "        if (serversSet.size() > 0) {",
            "          ArrayList<ThriftTransportKey> cachedServers = new ArrayList<>(serversSet);",
            "          Collections.shuffle(cachedServers, random);",
            "      if (serversSet.size() > 0) {",
            "",
            "          for (ThriftTransportKey ttk : cachedServers) {",
            "            CachedConnection cachedConnection = getCache().get(ttk).reserveAny();",
            "            if (cachedConnection != null) {",
            "              final String serverAddr = ttk.getServer().toString();",
            "              log.trace(\"Using existing connection to {}\", serverAddr);",
            "              return new Pair<>(serverAddr, cachedConnection.transport);",
            "            }",
            "        for (ThriftTransportKey ttk : cachedServers) {",
            "      if (preferCachedConnection) {",
            "        synchronized (this) {",
            "          CachedConnections cachedConns = getCache().get(ttk);",
            "          if (cachedConns != null) {",
            "            CachedConnection cachedConnection = cachedConns.reserveAny();",
            "            if (cachedConnection != null) {",
            "              final String serverAddr = ttk.getServer().toString();",
            "              return new Pair<>(serverAddr, cachedConnection.transport);",
            "            }",
            "          }",
            "        CachedConnection connection = pool.reserveAnyIfPresent(ttk);",
            "",
            "    CachedConnection cc = new CachedConnection(tsc);",
            "    cc.reserve();",
            "    CachedConnection connection = new CachedConnection(tsc);",
            "    try {",
            "      synchronized (this) {",
            "        CachedConnections cachedConns =",
            "            getCache().computeIfAbsent(cacheKey, ck -> new CachedConnections());",
            "        cachedConns.reserved.put(cc.transport, cc);",
            "      }",
            "      ConnectionPool pool = getConnectionPool();",
            "    } catch (TransportPoolShutdownException e) {",
            "      cc.transport.close();",
            "      connection.transport.close();",
            "    }",
            "    return cc.transport;",
            "",
            "",
            "  public void returnTransport(TTransport tsc) {",
            "    if (tsc == null) {",
            "  public void returnTransport(TTransport transport) {",
            "",
            "    boolean existInCache = false;",
            "    CachedTTransport ctsc = (CachedTTransport) tsc;",
            "",
            "    CachedTTransport cachedTransport = (CachedTTransport) transport;",
            "",
            "    synchronized (this) {",
            "      CachedConnections cachedConns = getCache().get(ctsc.getCacheKey());",
            "      if (cachedConns != null) {",
            "        CachedConnection cachedConnection = cachedConns.reserved.remove(ctsc);",
            "        if (cachedConnection != null) {",
            "          if (ctsc.sawError) {",
            "            closeList.add(cachedConnection);",
            "",
            "            log.trace(\"Returned connection had error {}\", ctsc.getCacheKey());",
            "    // close outside of sync block",
            "",
            "            Long ecount = errorCount.merge(ctsc.getCacheKey(), 1L, Long::sum);",
            "    if (cachedTransport.sawError) {",
            "",
            "            // logs the first time an error occurred",
            "            errorTime.computeIfAbsent(ctsc.getCacheKey(), k -> System.currentTimeMillis());",
            "      boolean shouldWarn = false;",
            "",
            "            if (ecount >= ERROR_THRESHOLD && serversWarnedAbout.add(ctsc.getCacheKey())) {",
            "              log.warn(",
            "                  \"Server {} had {} failures in a short time period, will not complain anymore\",",
            "                  ctsc.getCacheKey(), ecount);",
            "            }",
            "      synchronized (errorCount) {",
            "",
            "            cachedConnection.unreserve();",
            "        ecount = errorCount.merge(cachedTransport.getCacheKey(), 1L, Long::sum);",
            "",
            "            // remove all unreserved cached connection when a sever has an error, not just the",
            "            // connection that was returned",
            "            closeList.addAll(cachedConns.unreserved);",
            "            cachedConns.unreserved.clear();",
            "        // logs the first time an error occurred",
            "",
            "          } else {",
            "            log.trace(\"Returned connection {} ioCount: {}\", ctsc.getCacheKey(),",
            "                cachedConnection.transport.ioCount);",
            "",
            "            cachedConnection.lastReturnTime = System.currentTimeMillis();",
            "            cachedConnection.unreserve();",
            "            // Using LIFO ensures that when the #",
            "            // of pooled connections exceeds the working set size that the",
            "            // idle times at the end of the list grow. The connections with large idle times will be",
            "            // cleaned up. Using a FIFO could continually reset the idle",
            "            // times of all connections, even when there are more than the working set size.",
            "            cachedConns.unreserved.push(cachedConnection);",
            "          }",
            "          existInCache = true;",
            "        if (ecount >= ERROR_THRESHOLD && serversWarnedAbout.add(cachedTransport.getCacheKey())) {",
            "      }",
            "    }",
            "",
            "    // close outside of sync block",
            "    for (CachedConnection cachedConnection : closeList) {",
            "      try {",
            "        cachedConnection.transport.close();",
            "      } catch (Exception e) {",
            "        log.debug(\"Failed to close connection w/ errors\", e);",
            "      log.trace(\"Returned connection had error {}\", cachedTransport.getCacheKey());",
            "      // close outside of sync block",
            "      tsc.close();",
            "      transport.close();",
            "",
            "  public synchronized void startCheckerThread() {",
            "    if (cache != null && checkThread == null) {",
            "      checkThread = new Daemon(new Closer(instance), \"Thrift Connection Pool Checker\");",
            "      checkThread.start();",
            "    }",
            "  public void startCheckerThread() {",
            "",
            "  private void shutdown() {",
            "    Thread ctl;",
            "    synchronized (this) {",
            "      if (cache == null)",
            "        return;",
            "",
            "      // close any connections in the pool... even ones that are in use",
            "      for (CachedConnections cachedConn : getCache().values()) {",
            "        for (CachedConnection cc : Iterables.concat(cachedConn.reserved.values(),",
            "            cachedConn.unreserved)) {",
            "          try {",
            "            cc.transport.close();",
            "          } catch (Exception e) {",
            "            log.debug(\"Error closing transport during shutdown\", e);",
            "          }",
            "        }",
            "      }",
            "  void closeExpiredConnections() {",
            "",
            "      // this will render the pool unusable and cause the background thread to exit",
            "      this.cache = null;",
            "    ConnectionPool pool = getConnectionPool();",
            "",
            "      ctl = checkThread;",
            "    }",
            "",
            "    if (ctl != null) {",
            "      try {",
            "        ctl.interrupt();",
            "        ctl.join();",
            "      } catch (InterruptedException e) {",
            "        throw new RuntimeException(e);",
            "    synchronized (errorCount) {",
            "",
            "  private Map<ThriftTransportKey,CachedConnections> getCache() {",
            "    if (cache == null)",
            "      throw new TransportPoolShutdownException(",
            "          \"The Accumulo singleton for connection pooling is disabled.  This is likely caused by \"",
            "              + \"all AccumuloClients being closed or garbage collected.\");",
            "    return cache;",
            "  private ConnectionPool getConnectionPool() {"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/apache/accumulo/commit/2e5182b131224c5b4fd692b61ee065b55c43bd5e",
        "CWE": "CWE-543",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/apache/accumulo/pull/1374"
      }
    ],
    "CWD-1132": [
      {
        "benign_code": {
          "context": "org.idempiere.test/src/org/idempiere/test/model/MStorageOnHandTest.java",
          "class": null,
          "func": null,
          "lines": [
            "\t\t\t//test UseGuaranteeDateForMPolicy",
            "",
            "\t\t\t//reload storages for the fifo case",
            "\t\t\tonhands = MStorageOnHand.getAll(Env.getCtx(), product.get_ID(), 0, false, true, getTrxName(), false, 0);",
            "\t\t\ttry {",
            "\t\t\t\t//test UseGuaranteeDateForMPolicy",
            "\t\t\t\tas.setUseGuaranteeDateForMPolicy(true);",
            "\t\t\t\tas.saveEx();\t\t\t",
            "\t\t\t\tas.saveEx();",
            "\t\t\t\t// the Cache reset is called asynchronously in PO, so we need to call it here to make sure the MAttributeSet is reloaded",
            "\t\t\t\tCacheMgt.get().reset(MAttributeSet.Table_Name, as.get_ID());",
            "\t\t\t\t// create asi1 expiring tomorrow",
            "\t\t\t\tMAttributeSetInstance asi1 = new MAttributeSetInstance(Env.getCtx(), 0, getTrxName());",
            "\t\t\t\tDB.executeUpdateEx(\"UPDATE M_StorageOnHand SET M_AttributeSetInstance_ID=? WHERE M_StorageOnHand_UU=?\", new Object[] {asi1.get_ID(), onhands[0].getM_StorageOnHand_UU()}, getTrxName());",
            "\t\t\t\t// create asi2 expiring today",
            "\t\t\t\tMAttributeSetInstance asi2 = new MAttributeSetInstance(Env.getCtx(), 0, getTrxName());",
            "\t\t\t\tCacheMgt.get().reset(MProduct.Table_Name, product.get_ID());",
            "\t\t\t\t// load fifo again but this time by guarantee date, must bring first the asi2 expiring today",
            "\t\t\t\tonhands = MStorageOnHand.getAll(Env.getCtx(), product.get_ID(), 0, false, true, getTrxName(), false, 0);"
          ]
        },
        "vulnerable_code": {
          "context": "org.idempiere.test/src/org/idempiere/test/model/MStorageOnHandTest.java",
          "class": null,
          "func": null,
          "lines": [
            "\t\t\tassertEquals(2, onhands[0].getQtyOnHand().intValue());",
            "\t\t\t",
            "\t\t\t//test UseGuaranteeDateForMPolicy",
            "",
            "\t\t\t\tas.setUseGuaranteeDateForMPolicy(true);",
            "\t\t\t\tas.saveEx();\t\t\t",
            "\t\t\t\tas.saveEx();",
            "\t\t\t\tDB.executeUpdateEx(\"UPDATE M_StorageOnHand SET M_AttributeSetInstance_ID=? WHERE M_StorageOnHand_UU=?\", new Object[] {asi2.get_ID(), onhands[1].getM_StorageOnHand_UU()}, getTrxName());",
            "\t\t\t\tCacheMgt.get().reset(MProduct.Table_Name, product.get_ID());",
            "\t\t\t\t// load fifo again but this time by guarantee date, must bring first the asi2 expiring today"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/idempiere/idempiere/commit/6db0440a286020dff4ee7f5326b140c05c82fbc7",
        "CWE": "CWE-567",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/idempiere/idempiere/pull/2976"
      },
      {
        "benign_code": {
          "context": "agents-common/src/main/java/org/apache/ranger/plugin/service/RangerBasePlugin.java",
          "class": null,
          "func": null,
          "lines": [
            "    private       boolean                     synchronousPolicyRefresh;",
            "    private final RangerPluginConfig        pluginConfig;",
            "    private final RangerPluginContext       pluginContext;",
            "    private final Map<String, LogHistory>   logHistoryList = new Hashtable<>();",
            "    private final int                       logInterval    = 30000; // 30 seconds",
            "    private final DownloadTrigger           accessTrigger  = new DownloadTrigger();",
            "    private final List<RangerChainedPlugin> chainedPlugins;",
            "    private final boolean                   dedupStrings;",
            "",
            "    private volatile RangerPolicyEngine  policyEngine;",
            "    private volatile RangerAuthContext   currentAuthContext;",
            "    private volatile RangerRoles         roles;",
            "    private volatile Map<String, String> serviceConfigs;",
            "",
            "    private PolicyRefresher             refresher;",
            "    private RangerAccessResultProcessor resultProcessor;",
            "    private boolean                     isUserStoreEnricherAddedImplcitly;",
            "    private boolean                     synchronousPolicyRefresh;",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "agents-common/src/main/java/org/apache/ranger/plugin/service/RangerBasePlugin.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    private final RangerPluginConfig          pluginConfig;",
            "    private final RangerPluginContext         pluginContext;",
            "    private final Map<String, LogHistory>     logHistoryList = new Hashtable<>();",
            "    private final int                         logInterval    = 30000; // 30 seconds",
            "    private final DownloadTrigger             accessTrigger  = new DownloadTrigger();",
            "    private final List<RangerChainedPlugin>   chainedPlugins;",
            "    private final boolean                     dedupStrings;",
            "    private       PolicyRefresher             refresher;",
            "    private       RangerPolicyEngine          policyEngine;",
            "    private       RangerAuthContext           currentAuthContext;",
            "    private       RangerAccessResultProcessor resultProcessor;",
            "    private       RangerRoles                 roles;",
            "    private       boolean                     isUserStoreEnricherAddedImplcitly;",
            "    private       Map<String, String>         serviceConfigs;",
            "    private       boolean                     synchronousPolicyRefresh;",
            "    private final RangerPluginConfig        pluginConfig;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/apache/ranger/commit/4fb301152ed2ba72596419ebdda90f8a5912e787",
        "CWE": "CWE-567",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/apache/ranger/pull/729"
      },
      {
        "benign_code": {
          "context": "dd-java-agent/appsec/src/main/java/com/datadog/appsec/gateway/AppSecRequestContext.java",
          "class": null,
          "func": null,
          "lines": [
            "import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;",
            "import java.util.concurrent.atomic.AtomicReference;",
            "import org.slf4j.Logger;",
            "  private volatile Map<String, Object> derivatives;",
            "  private final AtomicReference<Map<String, Object>> derivatives = new AtomicReference<>();",
            "",
            "      persistentData.clear();",
            "      final Map<String, Object> derivatives = this.derivatives.getAndSet(null);",
            "      if (derivatives != null) {",
            "                literalValue.getClass().getSimpleName());",
            "    // Initialize or update derivatives atomically",
            "    derivatives.updateAndGet(",
            "        current -> {",
            "          Map<String, Object> updated = current != null ? new HashMap<>(current) : new HashMap<>();",
            "",
            "          // Process each attribute according to the specification",
            "          for (Map.Entry<String, Object> entry : data.entrySet()) {",
            "            String attributeKey = entry.getKey();",
            "            Object attributeConfig = entry.getValue();",
            "",
            "            if (attributeConfig instanceof Map) {",
            "              @SuppressWarnings(\"unchecked\")",
            "              Map<String, Object> config = (Map<String, Object>) attributeConfig;",
            "",
            "              // Check if it's a literal value schema",
            "              if (config.containsKey(\"value\")) {",
            "                Object literalValue = config.get(\"value\");",
            "                if (literalValue != null) {",
            "                  // Preserve the original type - don't convert to string",
            "                  updated.put(attributeKey, literalValue);",
            "                  log.debug(",
            "                      \"Added literal attribute: {} = {} (type: {})\",",
            "                      attributeKey,",
            "                      literalValue,",
            "                      literalValue.getClass().getSimpleName());",
            "                }",
            "              }",
            "              // Check if it's a request data schema",
            "              else if (config.containsKey(\"address\")) {",
            "                String address = (String) config.get(\"address\");",
            "                @SuppressWarnings(\"unchecked\")",
            "                List<String> keyPath = (List<String>) config.get(\"key_path\");",
            "                @SuppressWarnings(\"unchecked\")",
            "                List<String> transformers = (List<String>) config.get(\"transformers\");",
            "",
            "                Object extractedValue = extractValueFromRequestData(address, keyPath, transformers);",
            "                if (extractedValue != null) {",
            "                  // For extracted values, convert to string as they come from request data",
            "                  updated.put(attributeKey, extractedValue.toString());",
            "                  log.debug(\"Added extracted attribute: {} = {}\", attributeKey, extractedValue);",
            "                }",
            "              }",
            "            } else {",
            "              // Handle plain string/numeric values",
            "              updated.put(attributeKey, attributeConfig);",
            "              log.debug(\"Added direct attribute: {} = {}\", attributeKey, attributeConfig);",
            "            }",
            "          }",
            "    }",
            "          return updated;",
            "        });",
            "  }",
            "",
            "    // Get and clear derivatives atomically",
            "    Map<String, Object> derivativesToCommit = derivatives.getAndSet(null);",
            "    log.debug(\"Committing derivatives: {} for {}\", derivativesToCommit, traceSegment);",
            "",
            "    // Process and commit derivatives directly",
            "      for (Map.Entry<String, Object> entry : derivatives.entrySet()) {",
            "    if (derivativesToCommit != null && !derivativesToCommit.isEmpty()) {",
            "      for (Map.Entry<String, Object> entry : derivativesToCommit.entrySet()) {",
            "        String key = entry.getKey();",
            "    return derivatives == null ? emptySet() : new HashSet<>(derivatives.keySet());",
            "    Map<String, Object> current = derivatives.get();",
            "    return current == null ? emptySet() : new HashSet<>(current.keySet());",
            "  }"
          ]
        },
        "vulnerable_code": {
          "context": "dd-java-agent/appsec/src/main/java/com/datadog/appsec/gateway/AppSecRequestContext.java",
          "class": null,
          "func": null,
          "lines": [
            "  private boolean pathParamsPublished;",
            "  private volatile Map<String, Object> derivatives;",
            "  private final AtomicReference<Map<String, Object>> derivatives = new AtomicReference<>();",
            "        derivatives.clear();",
            "        derivatives = null;",
            "      }",
            "",
            "    // Store raw derivatives",
            "    if (derivatives == null) {",
            "      derivatives = new HashMap<>();",
            "    }",
            "",
            "    // Process each attribute according to the specification",
            "    for (Map.Entry<String, Object> entry : data.entrySet()) {",
            "      String attributeKey = entry.getKey();",
            "      Object attributeConfig = entry.getValue();",
            "",
            "      if (attributeConfig instanceof Map) {",
            "        @SuppressWarnings(\"unchecked\")",
            "        Map<String, Object> config = (Map<String, Object>) attributeConfig;",
            "",
            "        // Check if it's a literal value schema",
            "        if (config.containsKey(\"value\")) {",
            "          Object literalValue = config.get(\"value\");",
            "          if (literalValue != null) {",
            "            // Preserve the original type - don't convert to string",
            "            derivatives.put(attributeKey, literalValue);",
            "            log.debug(",
            "                \"Added literal attribute: {} = {} (type: {})\",",
            "                attributeKey,",
            "                literalValue,",
            "                literalValue.getClass().getSimpleName());",
            "    // Initialize or update derivatives atomically",
            "          }",
            "        }",
            "        // Check if it's a request data schema",
            "        else if (config.containsKey(\"address\")) {",
            "          String address = (String) config.get(\"address\");",
            "          @SuppressWarnings(\"unchecked\")",
            "          List<String> keyPath = (List<String>) config.get(\"key_path\");",
            "          @SuppressWarnings(\"unchecked\")",
            "          List<String> transformers = (List<String>) config.get(\"transformers\");",
            "",
            "          Object extractedValue = extractValueFromRequestData(address, keyPath, transformers);",
            "          if (extractedValue != null) {",
            "            // For extracted values, convert to string as they come from request data",
            "            derivatives.put(attributeKey, extractedValue.toString());",
            "            log.debug(\"Added extracted attribute: {} = {}\", attributeKey, extractedValue);",
            "          }",
            "        }",
            "      } else {",
            "        // Handle plain string/numeric values",
            "        derivatives.put(attributeKey, attributeConfig);",
            "        log.debug(\"Added direct attribute: {} = {}\", attributeKey, attributeConfig);",
            "      }",
            "    }",
            "          return updated;",
            "  public boolean commitDerivatives(TraceSegment traceSegment) {",
            "    log.debug(\"Committing derivatives: {} for {}\", derivatives, traceSegment);",
            "    if (traceSegment == null) {",
            "    // Process and commit derivatives directly",
            "    if (derivatives != null && !derivatives.isEmpty()) {",
            "      for (Map.Entry<String, Object> entry : derivatives.entrySet()) {",
            "    if (derivativesToCommit != null && !derivativesToCommit.isEmpty()) {",
            "",
            "    // Clear all attribute maps",
            "    derivatives = null;",
            "    return true;",
            "  Set<String> getDerivativeKeys() {",
            "    return derivatives == null ? emptySet() : new HashSet<>(derivatives.keySet());",
            "    Map<String, Object> current = derivatives.get();"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/DataDog/dd-trace-java/commit/d0b125896bc6c5e1d698f8e7dc56386890352895",
        "CWE": "CWE-567",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/DataDog/dd-trace-java/pull/9923"
      },
      {
        "benign_code": {
          "context": "core/src/main/java/com/alibaba/nacos/core/remote/grpc/GrpcConnection.java",
          "class": null,
          "func": null,
          "lines": [
            "    private static TpsControlManager tpsControlManager;",
            "    private static volatile TpsControlManager tpsControlManager;",
            "    ",
            "                    synchronized (GrpcConnection.class.getClass()) {",
            "                    synchronized (GrpcConnection.class) {",
            "                        if (tpsControlManager == null) {"
          ]
        },
        "vulnerable_code": {
          "context": "core/src/main/java/com/alibaba/nacos/core/remote/grpc/GrpcConnection.java",
          "class": null,
          "func": null,
          "lines": [
            "    ",
            "    private static TpsControlManager tpsControlManager;",
            "    private static volatile TpsControlManager tpsControlManager;",
            "                if (tpsControlManager == null) {",
            "                    synchronized (GrpcConnection.class.getClass()) {",
            "                    synchronized (GrpcConnection.class) {"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/alibaba/nacos/commit/d964cd78706761e2d465d2dcee4e115068592413",
        "CWE": "CWE-567",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/alibaba/nacos/pull/13882"
      }
    ],
    "CWD-1133": [
      {
        "benign_code": {
          "context": "s3stream/src/main/java/com/automq/stream/s3/metrics/stats/NetworkStats.java",
          "class": null,
          "func": null,
          "lines": [
            "    private static volatile NetworkStats instance = null;",
            "    private static final NetworkStats INSTANCE = new NetworkStats();",
            "    // <StreamId, <FastReadBytes, SlowReadBytes>>",
            "        return instance;",
            "        return INSTANCE;",
            "    }",
            "                : networkOutboundUsageTotalStats.computeIfAbsent(strategy, k -> S3StreamMetricsManager.buildNetworkOutboundUsageMetric(strategy, networkOutboundUsageTotal::inc));",
            "        Map<ThrottleStrategy, CounterMetric> stats = type == AsyncNetworkBandwidthLimiter.Type.INBOUND ? networkInboundUsageTotalStats : networkOutboundUsageTotalStats;",
            "        CounterMetric metric = stats.get(strategy);",
            "        if (metric == null) {",
            "            if (type == AsyncNetworkBandwidthLimiter.Type.INBOUND) {",
            "                metric = stats.computeIfAbsent(strategy, k -> S3StreamMetricsManager.buildNetworkInboundUsageMetric(strategy, networkInboundUsageTotal::inc));",
            "            } else {",
            "                metric = stats.computeIfAbsent(strategy, k -> S3StreamMetricsManager.buildNetworkOutboundUsageMetric(strategy, networkOutboundUsageTotal::inc));",
            "            }",
            "        }",
            "        return metric;",
            "    }"
          ]
        },
        "vulnerable_code": {
          "context": "s3stream/src/main/java/com/automq/stream/s3/metrics/stats/NetworkStats.java",
          "class": null,
          "func": null,
          "lines": [
            "public class NetworkStats {",
            "    private static volatile NetworkStats instance = null;",
            "    private static final NetworkStats INSTANCE = new NetworkStats();",
            "    public static NetworkStats getInstance() {",
            "        if (instance == null) {",
            "            synchronized (NetworkStats.class) {",
            "                if (instance == null) {",
            "                    instance = new NetworkStats();",
            "                }",
            "            }",
            "        }",
            "        return instance;",
            "        return INSTANCE;",
            "    public CounterMetric networkUsageTotalStats(AsyncNetworkBandwidthLimiter.Type type, ThrottleStrategy strategy) {",
            "        return type == AsyncNetworkBandwidthLimiter.Type.INBOUND",
            "                ? networkInboundUsageTotalStats.computeIfAbsent(strategy, k -> S3StreamMetricsManager.buildNetworkInboundUsageMetric(strategy, networkInboundUsageTotal::inc))",
            "                : networkOutboundUsageTotalStats.computeIfAbsent(strategy, k -> S3StreamMetricsManager.buildNetworkOutboundUsageMetric(strategy, networkOutboundUsageTotal::inc));",
            "        Map<ThrottleStrategy, CounterMetric> stats = type == AsyncNetworkBandwidthLimiter.Type.INBOUND ? networkInboundUsageTotalStats : networkOutboundUsageTotalStats;"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/AutoMQ/automq/commit/fdb40ead4b658e7a1a19979bd4cf19db1b549320",
        "CWE": "CWE-1096",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/AutoMQ/automq/pull/2998"
      }
    ],
    "CWD-1137": [
      {
        "benign_code": {
          "context": "app/src/main/java/com/nutomic/syncthingandroid/service/RestApi.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    /**",
            "     * Object that must be locked upon accessing mConfig",
            "     */",
            "    private final Object mConfigLock = new Object();",
            "",
            "    /**",
            "        if (mConfig == null) {",
            "        Boolean configParseSuccess;",
            "        synchronized(mConfigLock) {",
            "            mConfig = new Gson().fromJson(result, Config.class);",
            "            configParseSuccess = mConfig != null;",
            "        }",
            "        if (!configParseSuccess) {",
            "            throw new RuntimeException(\"config is null: \" + result);",
            "            Log.d(TAG, \"Ignored device [\" + deviceId + \"]\");",
            "        synchronized (mConfigLock) {",
            "            if (!mConfig.ignoredDevices.contains(deviceId)) {",
            "                mConfig.ignoredDevices.add(deviceId);",
            "                sendConfig();",
            "                Log.d(TAG, \"Ignored device [\" + deviceId + \"]\");",
            "            }",
            "        }",
            "            Log.d(TAG, \"Ignored folder [\" + folderId + \"]\");",
            "        synchronized (mConfigLock) {",
            "            if (!mConfig.ignoredFolders.contains(folderId)) {",
            "                mConfig.ignoredFolders.add(folderId);",
            "                sendConfig();",
            "                Log.d(TAG, \"Ignored folder [\" + folderId + \"]\");",
            "            }",
            "        }",
            "        mConfig.ignoredFolders.clear();",
            "        synchronized (mConfigLock) {",
            "            mConfig.ignoredDevices.clear();",
            "            mConfig.ignoredFolders.clear();",
            "        }",
            "    }",
            "        new PostConfigRequest(mContext, mUrl, mApiKey, new Gson().toJson(mConfig), null);",
            "        String jsonConfig;",
            "        synchronized (mConfigLock) {",
            "            jsonConfig = new Gson().toJson(mConfig);",
            "        }",
            "        new PostConfigRequest(mContext, mUrl, mApiKey, jsonConfig, null);",
            "        mOnConfigChangedListener.onConfigChanged();",
            "        new PostConfigRequest(mContext, mUrl, mApiKey, new Gson().toJson(mConfig), result -> {",
            "        String jsonConfig;",
            "        synchronized (mConfigLock) {",
            "            jsonConfig = new Gson().toJson(mConfig);",
            "        }",
            "        new PostConfigRequest(mContext, mUrl, mApiKey, jsonConfig, result -> {",
            "            Intent intent = new Intent(mContext, SyncthingService.class)",
            "        List<Folder> folders = deepCopy(mConfig.folders, new TypeToken<List<Folder>>(){}.getType());",
            "        List<Folder> folders;",
            "        synchronized (mConfigLock) {",
            "            folders = deepCopy(mConfig.folders, new TypeToken<List<Folder>>(){}.getType());",
            "        }",
            "        Collections.sort(folders, FOLDERS_COMPARATOR);",
            "        sendConfig();",
            "        synchronized (mConfigLock) {",
            "            // Add the new folder to the model.",
            "            mConfig.folders.add(folder);",
            "            // Send model changes to syncthing, does not require a restart.",
            "            sendConfig();",
            "        }",
            "    }",
            "        sendConfig();",
            "        synchronized (mConfigLock) {",
            "            removeFolderInternal(newFolder.id);",
            "            mConfig.folders.add(newFolder);",
            "            sendConfig();",
            "        }",
            "    }",
            "        // Remove saved data from share activity for this folder.",
            "        synchronized (mConfigLock) {",
            "            removeFolderInternal(id);",
            "            // mCompletion will be updated after the ConfigSaved event.",
            "            sendConfig();",
            "            // Remove saved data from share activity for this folder.",
            "        }",
            "        PreferenceManager.getDefaultSharedPreferences(mContext).edit()",
            "                it.remove();",
            "        synchronized (mConfigLock) {",
            "            Iterator<Folder> it = mConfig.folders.iterator();",
            "            while (it.hasNext()) {",
            "                Folder f = it.next();",
            "                if (f.id.equals(id)) {",
            "                    it.remove();",
            "                    break;",
            "                }",
            "            }",
            "        List<Device> devices = deepCopy(mConfig.devices, new TypeToken<List<Device>>(){}.getType());",
            "        List<Device> devices;",
            "        synchronized (mConfigLock) {",
            "            devices = deepCopy(mConfig.devices, new TypeToken<List<Device>>(){}.getType());",
            "        }",
            "",
            "            sendConfig();",
            "            synchronized (mConfigLock) {",
            "                mConfig.devices.add(device);",
            "                sendConfig();",
            "            }",
            "        }, errorListener);",
            "        sendConfig();",
            "        synchronized (mConfigLock) {",
            "            removeDeviceInternal(newDevice.deviceID);",
            "            mConfig.devices.add(newDevice);",
            "            sendConfig();",
            "        }",
            "    }",
            "        sendConfig();",
            "        synchronized (mConfigLock) {",
            "            removeDeviceInternal(deviceId);",
            "            // mCompletion will be updated after the ConfigSaved event.",
            "            sendConfig();",
            "        }",
            "    }",
            "                it.remove();",
            "        synchronized (mConfigLock) {",
            "            Iterator<Device> it = mConfig.devices.iterator();",
            "            while (it.hasNext()) {",
            "                Device d = it.next();",
            "                if (d.deviceID.equals(deviceId)) {",
            "                    it.remove();",
            "                    break;",
            "                }",
            "            }",
            "        return deepCopy(mConfig.options, Options.class);",
            "        synchronized (mConfigLock) {",
            "            return deepCopy(mConfig.options, Options.class);",
            "        }",
            "    }",
            "        return deepCopy(mConfig.gui, Config.Gui.class);",
            "        synchronized (mConfigLock) {",
            "            return deepCopy(mConfig.gui, Config.Gui.class);",
            "        }",
            "    }",
            "        mConfig.options = newOptions;",
            "        synchronized (mConfigLock) {",
            "            mConfig.gui = newGui;",
            "            mConfig.options = newOptions;",
            "        }",
            "    }",
            "        return mConfig != null;",
            "        synchronized(mConfigLock) {",
            "            return mConfig != null;",
            "        }",
            "    }",
            "        mConfig.options = options;",
            "        synchronized (mConfigLock) {",
            "            mConfig.options = options;",
            "        }",
            "    }"
          ]
        },
        "vulnerable_code": {
          "context": "app/src/main/java/com/nutomic/syncthingandroid/service/RestApi.java",
          "class": null,
          "func": null,
          "lines": [
            "    private void onReloadConfigComplete(String result) {",
            "        mConfig = new Gson().fromJson(result, Config.class);",
            "        if (mConfig == null) {",
            "        Boolean configParseSuccess;",
            "    public void ignoreDevice(String deviceId) {",
            "        if (!mConfig.ignoredDevices.contains(deviceId)) {",
            "            mConfig.ignoredDevices.add(deviceId);",
            "            sendConfig();",
            "            Log.d(TAG, \"Ignored device [\" + deviceId + \"]\");",
            "        synchronized (mConfigLock) {",
            "    public void ignoreFolder(String folderId) {",
            "        if (!mConfig.ignoredFolders.contains(folderId)) {",
            "            mConfig.ignoredFolders.add(folderId);",
            "            sendConfig();",
            "            Log.d(TAG, \"Ignored folder [\" + folderId + \"]\");",
            "        synchronized (mConfigLock) {",
            "        Log.d(TAG, \"Undo ignoring devices and folders ...\");",
            "        mConfig.ignoredDevices.clear();",
            "        mConfig.ignoredFolders.clear();",
            "        synchronized (mConfigLock) {",
            "    private void sendConfig() {",
            "        new PostConfigRequest(mContext, mUrl, mApiKey, new Gson().toJson(mConfig), null);",
            "        String jsonConfig;",
            "    public void saveConfigAndRestart() {",
            "        new PostConfigRequest(mContext, mUrl, mApiKey, new Gson().toJson(mConfig), result -> {",
            "        String jsonConfig;",
            "    public List<Folder> getFolders() {",
            "        List<Folder> folders = deepCopy(mConfig.folders, new TypeToken<List<Folder>>(){}.getType());",
            "        List<Folder> folders;",
            "    public void createFolder(Folder folder) {",
            "        // Add the new folder to the model.",
            "        mConfig.folders.add(folder);",
            "        // Send model changes to syncthing, does not require a restart.",
            "        sendConfig();",
            "        synchronized (mConfigLock) {",
            "    public void updateFolder(Folder newFolder) {",
            "        removeFolderInternal(newFolder.id);",
            "        mConfig.folders.add(newFolder);",
            "        sendConfig();",
            "        synchronized (mConfigLock) {",
            "    public void removeFolder(String id) {",
            "        removeFolderInternal(id);",
            "        // mCompletion will be updated after the ConfigSaved event.",
            "        sendConfig();",
            "        // Remove saved data from share activity for this folder.",
            "        synchronized (mConfigLock) {",
            "    private void removeFolderInternal(String id) {",
            "        Iterator<Folder> it = mConfig.folders.iterator();",
            "        while (it.hasNext()) {",
            "            Folder f = it.next();",
            "            if (f.id.equals(id)) {",
            "                it.remove();",
            "        synchronized (mConfigLock) {",
            "    public List<Device> getDevices(boolean includeLocal) {",
            "        List<Device> devices = deepCopy(mConfig.devices, new TypeToken<List<Device>>(){}.getType());",
            "        List<Device> devices;",
            "        normalizeDeviceId(device.deviceID, normalizedId -> {",
            "            mConfig.devices.add(device);",
            "            sendConfig();",
            "            synchronized (mConfigLock) {",
            "    public void editDevice(Device newDevice) {",
            "        removeDeviceInternal(newDevice.deviceID);",
            "        mConfig.devices.add(newDevice);",
            "        sendConfig();",
            "        synchronized (mConfigLock) {",
            "    public void removeDevice(String deviceId) {",
            "        removeDeviceInternal(deviceId);",
            "        // mCompletion will be updated after the ConfigSaved event.",
            "        sendConfig();",
            "        synchronized (mConfigLock) {",
            "    private void removeDeviceInternal(String deviceId) {",
            "        Iterator<Device> it = mConfig.devices.iterator();",
            "        while (it.hasNext()) {",
            "            Device d = it.next();",
            "            if (d.deviceID.equals(deviceId)) {",
            "                it.remove();",
            "        synchronized (mConfigLock) {",
            "    public Options getOptions() {",
            "        return deepCopy(mConfig.options, Options.class);",
            "        synchronized (mConfigLock) {",
            "    public Config.Gui getGui() {",
            "        return deepCopy(mConfig.gui, Config.Gui.class);",
            "        synchronized (mConfigLock) {",
            "    public void editSettings(Config.Gui newGui, Options newOptions) {",
            "        mConfig.gui = newGui;",
            "        mConfig.options = newOptions;",
            "        synchronized (mConfigLock) {",
            "    public boolean isConfigLoaded() {",
            "        return mConfig != null;",
            "        synchronized(mConfigLock) {",
            "        options.urAccepted = acceptUsageReporting ? mUrVersionMax : Options.USAGE_REPORTING_DENIED;",
            "        mConfig.options = options;",
            "        synchronized (mConfigLock) {"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/syncthing/syncthing-android/commit/b50fcf1fa140038bae094ab025f9790933ea6b4b",
        "CWE": "CWE-663",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/syncthing/syncthing-android/pull/1195"
      }
    ],
    "CWD-1622": [
      {
        "benign_code": {
          "context": "src/main/java/com/divudi/ws/common/ApplicationConfig.java",
          "class": null,
          "func": null,
          "lines": [
            "    private void addRestResourceClasses(Set<Class<?>> resources) {",
            "        resources.add(com.divudi.ws.channel.ChannelApi.class);",
            "        resources.add(com.divudi.ws.channel.CorsResponseFilter.class);",
            "        resources.add(com.divudi.ws.common.ApiMembership.class);",
            "        resources.add(com.divudi.ws.common.ConfigResource.class);",
            "        resources.add(com.divudi.ws.fhir.Fhir.class);",
            "        resources.add(com.divudi.ws.finance.CostingData.class);",
            "        resources.add(com.divudi.ws.finance.Finance.class);",
            "        resources.add(com.divudi.ws.finance.Qb.class);",
            "        resources.add(com.divudi.ws.finance.clinical.Fhir.class);",
            "        resources.add(com.divudi.ws.inward.ApiInward.class);",
            "        resources.add(com.divudi.ws.lims.Lims.class);",
            "        resources.add(com.divudi.ws.lims.LimsMiddlewareController.class);",
            "        resources.add(com.divudi.ws.lims.MiddlewareController.class);",
            "    }"
          ]
        },
        "vulnerable_code": {
          "context": "src/main/java/com/divudi/ws/common/ApplicationConfig.java",
          "class": null,
          "func": null,
          "lines": []
        },
        "source": "github",
        "commit_url": "https://github.com/hmislk/hmis/commit/605eafbeb93ffb72d537cee6068da5d1c35a0fa7",
        "CWE": "CWE-1286",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/hmislk/hmis/pull/16452"
      }
    ],
    "CWD-1049": [
      {
        "benign_code": {
          "context": "src/main/java/com/divudi/bean/pharmacy/PharmacySaleBhtController.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    /**",
            "     * Validates if decimal quantities are allowed based on three-tier configuration hierarchy.",
            "     *",
            "     * Priority 1: Universal decimal allowance (application-wide setting)",
            "     * Priority 2: Item-specific configuration (Item.allowFractions field)",
            "     * Priority 3: Integer-only enforcement (existing behavior)",
            "     *",
            "     * @param qty The quantity to validate",
            "     * @param item The item being validated",
            "     * @return true if the quantity contains decimals and decimals are not allowed, false otherwise",
            "     */",
            "    private boolean isDecimalQuantityNotAllowed(Double qty, Item item) {",
            "        // If quantity is null or is already an integer, no validation needed",
            "        if (qty == null || qty % 1 == 0) {",
            "            return false;",
            "        }",
            "",
            "        // Priority 1: Check if decimals are allowed universally",
            "        boolean allowDecimalsUniversally = configOptionApplicationController.getBooleanValueByKey(",
            "            \"Pharmacy Direct Issue to BHT - Allow Decimals Universally\", false);",
            "        if (allowDecimalsUniversally) {",
            "            return false; // Decimals allowed universally",
            "        }",
            "",
            "        // Priority 2: Check if the specific item allows fractions",
            "        boolean itemAllowsFractions = (item != null && item.isAllowFractions());",
            "        if (itemAllowsFractions) {",
            "            return false; // Item-specific setting allows decimals",
            "        }",
            "",
            "        // Priority 3: Integer-only enforcement (existing behavior)",
            "        boolean mustBeInteger = configOptionApplicationController.getBooleanValueByKey(",
            "            \"Pharmacy Direct Issue to BHT - Quantity Must Be Integer\", true);",
            "        return mustBeInteger; // Decimals not allowed if integer-only is enforced",
            "    }",
            "",
            "    //Check when edititng Qty",
            "            }",
            "        // Validate quantity based on three-tier configuration hierarchy",
            "        if (isDecimalQuantityNotAllowed(tmp.getQty(), tmp.getItem())) {",
            "            setZeroToQty(tmp);",
            "            onEditCalculation(tmp);",
            "            JsfUtil.addErrorMessage(\"Please enter only whole numbers (integers). Decimal values are not allowed for this item.\");",
            "            return true;",
            "        }",
            "            }",
            "        // Validate quantity based on three-tier configuration hierarchy",
            "        if (isDecimalQuantityNotAllowed(getQty(), getStock().getItemBatch().getItem())) {",
            "            errorMessage = \"Please enter only whole numbers (integers). Decimal values are not allowed for this item.\";",
            "            JsfUtil.addErrorMessage(\"Please enter only whole numbers (integers). Decimal values are not allowed for this item.\");",
            "            return;",
            "        }"
          ]
        },
        "vulnerable_code": {
          "context": "src/main/java/com/divudi/bean/pharmacy/PharmacySaleBhtController.java",
          "class": null,
          "func": null,
          "lines": [
            "",
            "        // Validate integer-only quantity if configuration is enabled",
            "        if (configOptionController.getBooleanValueByKey(\"Pharmacy Direct Issue to BHT - Quantity Must Be Integer\", true)) {",
            "            if (tmp.getQty() % 1 != 0) {",
            "                setZeroToQty(tmp);",
            "                onEditCalculation(tmp);",
            "                JsfUtil.addErrorMessage(\"Please enter only whole numbers (integers). Decimal values are not allowed.\");",
            "                return true;",
            "            }",
            "        // Validate quantity based on three-tier configuration hierarchy",
            "        }",
            "        // Validate integer-only quantity if configuration is enabled",
            "        if (configOptionController.getBooleanValueByKey(\"Pharmacy Direct Issue to BHT - Quantity Must Be Integer\", true)) {",
            "            if (getQty() % 1 != 0) {",
            "                errorMessage = \"Please enter only whole numbers (integers). Decimal values are not allowed.\";",
            "                JsfUtil.addErrorMessage(\"Please enter only whole numbers (integers). Decimal values are not allowed.\");",
            "                return;",
            "            }",
            "        // Validate quantity based on three-tier configuration hierarchy"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/hmislk/hmis/commit/62850bef279614eb89c1c4e1d2914ffe876b9fbd",
        "CWE": "CWE-106",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/hmislk/hmis/pull/16552"
      }
    ]
  },
  "python": {
    "CWD-1060": [
      {
        "benign_code": {
          "context": ".scripts/scrub_unit_tests.py",
          "class": null,
          "func": null,
          "lines": [
            "import logging",
            "import os",
            "import subprocess",
            "import sys",
            "from pathlib import Path",
            "",
            "",
            "# Get the repository root to validate paths",
            "REPO_ROOT = Path(__file__).parent.parent.resolve()",
            "",
            "",
            "def validate_file_path(file_path: str) -> Path:",
            "    \"\"\"",
            "    Validate and resolve a file path to prevent directory traversal attacks.",
            "",
            "    Args:",
            "        file_path: The file path to validate",
            "",
            "    Returns:",
            "        Resolved Path object",
            "",
            "    Raises:",
            "        ValueError: If the path is invalid or attempts directory traversal",
            "    \"\"\"",
            "    try:",
            "        # Convert to Path and resolve to absolute path",
            "        path = Path(file_path).resolve()",
            "",
            "        # Ensure the path is within the repository",
            "        if not str(path).startswith(str(REPO_ROOT)):",
            "            raise ValueError(f\"Path {file_path} is outside repository root\")",
            "",
            "        # Ensure it's a file (not a directory or special file)",
            "        if path.exists() and not path.is_file():",
            "            raise ValueError(f\"Path {file_path} is not a regular file\")",
            "",
            "        # Ensure it ends with .yml for files we process",
            "        if not str(path).endswith('.yml'):",
            "            raise ValueError(f\"Path {file_path} does not end with .yml\")",
            "",
            "        return path",
            "    except (OSError, RuntimeError) as e:",
            "        raise ValueError(f\"Invalid file path {file_path}: {e}\")",
            "",
            "",
            "    \"\"\"Compares the line numbers for each test log to those changed in the commit.\"\"\"",
            "    # Validate the file path to prevent directory traversal",
            "    validated_path = validate_file_path(fname)",
            "",
            "    # First, let's get the line numbers for each test log",
            "    with open(fname, \"r\") as f:",
            "    with open(validated_path, \"r\", encoding=\"utf-8\") as f:",
            "        raw_text = f.read()",
            "    # Next, we fetch the git diff for the file",
            "    # Use relative path from repo root for git commands",
            "    relative_path = validated_path.relative_to(REPO_ROOT)",
            "    result = subprocess.run(",
            "        [\"git\", \"diff\", \"--diff-filter=AM\", \"--staged\", fname], capture_output=True, text=True",
            "        [\"git\", \"diff\", \"--diff-filter=AM\", \"--staged\", \"--\", str(relative_path)],",
            "        capture_output=True,",
            "        text=True,",
            "        check=False,",
            "        cwd=str(REPO_ROOT),",
            "    )",
            "        text=True,",
            "        check=False,",
            "        cwd=str(REPO_ROOT),",
            "    )",
            "    return files",
            "    # Return absolute paths for consistency with the rest of the code",
            "    return [str(REPO_ROOT / f) for f in files]",
            "",
            "        with open(file, \"r\") as f:",
            "        try:",
            "            # Validate the file path before processing",
            "            validated_path = validate_file_path(file)",
            "        except ValueError as e:",
            "            logging.error(f\"Skipping invalid file path {file}: {e}\")",
            "            continue",
            "",
            "        with open(validated_path, \"r\", encoding=\"utf-8\") as f:",
            "            raw_text = f.read()",
            "        with open(file, \"w\") as f:",
            "        with open(validated_path, \"w\", encoding=\"utf-8\") as f:",
            "            f.write(raw_text)",
            "            subprocess.run([\"git\", \"add\", file], check=True)",
            "            # Use relative path from repo root for git commands",
            "            relative_path = validated_path.relative_to(REPO_ROOT)",
            "            subprocess.run(",
            "                [\"git\", \"add\", \"--\", str(relative_path)],",
            "                check=True,",
            "                cwd=str(REPO_ROOT),",
            "            )",
            ""
          ]
        },
        "vulnerable_code": {
          "context": ".scripts/scrub_unit_tests.py",
          "class": null,
          "func": null,
          "lines": [
            "    # First, let's get the line numbers for each test log",
            "    with open(fname, \"r\") as f:",
            "    with open(validated_path, \"r\", encoding=\"utf-8\") as f:",
            "    result = subprocess.run(",
            "        [\"git\", \"diff\", \"--diff-filter=AM\", \"--staged\", fname], capture_output=True, text=True",
            "        [\"git\", \"diff\", \"--diff-filter=AM\", \"--staged\", \"--\", str(relative_path)],",
            "    files = result.stdout.splitlines()",
            "    return files",
            "    # Return absolute paths for consistency with the rest of the code",
            "",
            "        with open(file, \"r\") as f:",
            "        try:",
            "",
            "        with open(file, \"w\") as f:",
            "        with open(validated_path, \"w\", encoding=\"utf-8\") as f:",
            "            # Now we need to add the file back to the git index",
            "            subprocess.run([\"git\", \"add\", file], check=True)",
            "            # Use relative path from repo root for git commands"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/panther-labs/panther-analysis/commit/ae0ca142f34aefc444b04b35ce6415e8199d1f41",
        "CWE": "CWE-23",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/panther-labs/panther-analysis/pull/1780"
      },
      {
        "benign_code": {
          "context": "src/pyload/webui/app/blueprints/cnl_blueprint.py",
          "class": null,
          "func": null,
          "lines": [
            "    )",
            "    dlc_filename = package.replace(\"/\", \"\").replace(\"\\\\\", \"\").replace(\":\", \"\") + \".dlc\"",
            "    dlc_path = os.path.join(dl_path, dlc_filename)",
            "    dlc_path = os.path.normpath(dlc_path)",
            "    # Ensure dlc_path is within dl_path",
            "    if not os.path.abspath(dlc_path).startswith(os.path.abspath(dl_path) + os.sep):",
            "        return \"failed: invalid package name\\r\\n\", 400",
            "    dlc = flask.request.form[\"crypted\"].replace(\" \", \"+\")"
          ]
        },
        "vulnerable_code": {
          "context": "src/pyload/webui/app/blueprints/cnl_blueprint.py",
          "class": null,
          "func": null,
          "lines": [
            "    dl_path = api.get_config_value(\"general\", \"storage_folder\")",
            "    dlc_path = os.path.join(",
            "        dl_path, package.replace(\"/\", \"\").replace(\"\\\\\", \"\").replace(\":\", \"\") + \".dlc\"",
            "    )",
            "    dlc_filename = package.replace(\"/\", \"\").replace(\"\\\\\", \"\").replace(\":\", \"\") + \".dlc\""
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/pyload/pyload/commit/70a44fe02c03bce92337b5d370d2a45caa4de3d4",
        "CWE": "CWE-23",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/pyload/pyload/pull/4596"
      }
    ],
    "CWD-1061": [
      {
        "benign_code": {
          "context": ".scripts/scrub_unit_tests.py",
          "class": null,
          "func": null,
          "lines": [
            "import logging",
            "import os",
            "import subprocess",
            "import sys",
            "from pathlib import Path",
            "",
            "",
            "# Get the repository root to validate paths",
            "REPO_ROOT = Path(__file__).parent.parent.resolve()",
            "",
            "",
            "def validate_file_path(file_path: str) -> Path:",
            "    \"\"\"",
            "    Validate and resolve a file path to prevent directory traversal attacks.",
            "",
            "    Args:",
            "        file_path: The file path to validate",
            "",
            "    Returns:",
            "        Resolved Path object",
            "",
            "    Raises:",
            "        ValueError: If the path is invalid or attempts directory traversal",
            "    \"\"\"",
            "    try:",
            "        # Convert to Path and resolve to absolute path",
            "        path = Path(file_path).resolve()",
            "",
            "        # Ensure the path is within the repository",
            "        if not str(path).startswith(str(REPO_ROOT)):",
            "            raise ValueError(f\"Path {file_path} is outside repository root\")",
            "",
            "        # Ensure it's a file (not a directory or special file)",
            "        if path.exists() and not path.is_file():",
            "            raise ValueError(f\"Path {file_path} is not a regular file\")",
            "",
            "        # Ensure it ends with .yml for files we process",
            "        if not str(path).endswith('.yml'):",
            "            raise ValueError(f\"Path {file_path} does not end with .yml\")",
            "",
            "        return path",
            "    except (OSError, RuntimeError) as e:",
            "        raise ValueError(f\"Invalid file path {file_path}: {e}\")",
            "",
            "",
            "    \"\"\"Compares the line numbers for each test log to those changed in the commit.\"\"\"",
            "    # Validate the file path to prevent directory traversal",
            "    validated_path = validate_file_path(fname)",
            "",
            "    # First, let's get the line numbers for each test log",
            "    with open(fname, \"r\") as f:",
            "    with open(validated_path, \"r\", encoding=\"utf-8\") as f:",
            "        raw_text = f.read()",
            "    # Next, we fetch the git diff for the file",
            "    # Use relative path from repo root for git commands",
            "    relative_path = validated_path.relative_to(REPO_ROOT)",
            "    result = subprocess.run(",
            "        [\"git\", \"diff\", \"--diff-filter=AM\", \"--staged\", fname], capture_output=True, text=True",
            "        [\"git\", \"diff\", \"--diff-filter=AM\", \"--staged\", \"--\", str(relative_path)],",
            "        capture_output=True,",
            "        text=True,",
            "        check=False,",
            "        cwd=str(REPO_ROOT),",
            "    )",
            "        text=True,",
            "        check=False,",
            "        cwd=str(REPO_ROOT),",
            "    )",
            "    return files",
            "    # Return absolute paths for consistency with the rest of the code",
            "    return [str(REPO_ROOT / f) for f in files]",
            "",
            "        with open(file, \"r\") as f:",
            "        try:",
            "            # Validate the file path before processing",
            "            validated_path = validate_file_path(file)",
            "        except ValueError as e:",
            "            logging.error(f\"Skipping invalid file path {file}: {e}\")",
            "            continue",
            "",
            "        with open(validated_path, \"r\", encoding=\"utf-8\") as f:",
            "            raw_text = f.read()",
            "        with open(file, \"w\") as f:",
            "        with open(validated_path, \"w\", encoding=\"utf-8\") as f:",
            "            f.write(raw_text)",
            "            subprocess.run([\"git\", \"add\", file], check=True)",
            "            # Use relative path from repo root for git commands",
            "            relative_path = validated_path.relative_to(REPO_ROOT)",
            "            subprocess.run(",
            "                [\"git\", \"add\", \"--\", str(relative_path)],",
            "                check=True,",
            "                cwd=str(REPO_ROOT),",
            "            )",
            ""
          ]
        },
        "vulnerable_code": {
          "context": ".scripts/scrub_unit_tests.py",
          "class": null,
          "func": null,
          "lines": [
            "    # First, let's get the line numbers for each test log",
            "    with open(fname, \"r\") as f:",
            "    with open(validated_path, \"r\", encoding=\"utf-8\") as f:",
            "    result = subprocess.run(",
            "        [\"git\", \"diff\", \"--diff-filter=AM\", \"--staged\", fname], capture_output=True, text=True",
            "        [\"git\", \"diff\", \"--diff-filter=AM\", \"--staged\", \"--\", str(relative_path)],",
            "    files = result.stdout.splitlines()",
            "    return files",
            "    # Return absolute paths for consistency with the rest of the code",
            "",
            "        with open(file, \"r\") as f:",
            "        try:",
            "",
            "        with open(file, \"w\") as f:",
            "        with open(validated_path, \"w\", encoding=\"utf-8\") as f:",
            "            # Now we need to add the file back to the git index",
            "            subprocess.run([\"git\", \"add\", file], check=True)",
            "            # Use relative path from repo root for git commands"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/panther-labs/panther-analysis/commit/ae0ca142f34aefc444b04b35ce6415e8199d1f41",
        "CWE": "CWE-36",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/panther-labs/panther-analysis/pull/1780"
      }
    ],
    "CWD-1118": [
      {
        "benign_code": {
          "context": "tidal_dl_ng/download.py",
          "class": null,
          "func": null,
          "lines": [
            "from concurrent import futures",
            "from dataclasses import dataclass",
            "from threading import Event",
            "",
            "@dataclass",
            "class TrackStreamInfo:",
            "    \"\"\"Container for track stream information.\"\"\"",
            "",
            "    stream_manifest: StreamManifest | None",
            "    file_extension: str",
            "    requires_flac_extraction: bool",
            "    media_stream: Stream | None",
            "",
            "",
            "# TODO: Set appropriate client string and use it for video download.",
            "                - atmos_session_context(): Dolby Atmos credential switching",
            "                - switch_to_atmos_session(): Dolby Atmos credential switching",
            "                - restore_normal_session(): Restore original session credentials",
            "            path_base (str): Base path for downloads.",
            "            tuple[StreamManifest | None, str, bool, Stream | None]: Stream info.",
            "        tuple[StreamManifest | None, str, bool, Stream | None]: Stream info.",
            "        \"\"\"",
            "        if isinstance(media, Track):",
            "        file_extension: str = \"\"",
            "",
            "        # CRITICAL: This lock is intentionally broad and serializes all",
            "        # stream-fetching (Phase 1) to prevent a critical race condition.",
            "        #",
            "        # THE PROBLEM:",
            "        # The single, shared session (self.tidal.session) must change its",
            "        # credentials to switch between Atmos and Hi-Res/Normal streams.",
            "        #",
            "        # THE RACE CONDITION IT FIXES:",
            "        # If this lock is released *before* get_stream() is called,",
            "        # another thread could change the session (e.g., back to \"Normal\")",
            "        # right after this thread switched it to \"Atmos\". This would",
            "        # cause this thread to call get_stream() with the wrong credentials,",
            "        # resulting in the API returning AAC 320 instead of Atmos.",
            "        #",
            "        # THE TRADEOFF:",
            "        # This creates a \"tollbooth\" bottleneck, serializing the get_stream()",
            "        # calls. However, the *actual* segment downloads (Phase 2)",
            "        # still run in parallel, governed by `downloads_concurrent_max`.",
            "        #",
            "        # DO NOT \"OPTIMIZE\" THIS by making the lock more granular.",
            "        # Correctness > Performance.",
            "",
            "        with self.tidal.stream_lock:",
            "            try:",
            "                    media_stream = media.get_stream()",
            "                if isinstance(media, Track):",
            "                    track_info = self._get_track_stream_info(media)",
            "",
            "                    if track_info.stream_manifest is None:",
            "                        return None, \"\", False, None",
            "",
            "                stream_manifest = media_stream.get_stream_manifest()",
            "                    stream_manifest = track_info.stream_manifest",
            "                    file_extension = track_info.file_extension",
            "                    do_flac_extract = track_info.requires_flac_extraction",
            "                    media_stream = track_info.media_stream",
            "",
            "                elif isinstance(media, Video):",
            "                    # Videos always require the normal session",
            "                    if not self.tidal.restore_normal_session():",
            "                        self.fn_logger.error(f\"Failed to restore normal session for video: {media.id}\")",
            "                        return None, \"\", False, None",
            "",
            "                    file_extension = AudioExtensions.MP4 if self.settings.data.video_convert_mp4 else VideoExtensions.TS",
            "",
            "                    stream_manifest = None",
            "                    media_stream = None",
            "                    do_flac_extract = False",
            "",
            "                else:",
            "                    self.fn_logger.error(f\"Unknown media type for stream info: {type(media)}\")",
            "                    return None, \"\", False, None",
            "",
            "                return None, \"\", False, None",
            "",
            "            except Exception:",
            "            file_extension = stream_manifest.file_extension",
            "        return stream_manifest, file_extension, do_flac_extract, media_stream",
            "",
            "            file_extension = AudioExtensions.MP4 if self.settings.data.video_convert_mp4 else VideoExtensions.TS",
            "    def _get_track_stream_info(self, media: Track) -> TrackStreamInfo:",
            "        \"\"\"",
            "        Gets stream info for a Track, handling Atmos/Normal session switching.",
            "",
            "        return stream_manifest, file_extension, do_flac_extract, media_stream",
            "        Args:",
            "            media: The track to get stream information for.",
            "",
            "        Returns:",
            "            TrackStreamInfo: Container with stream manifest, file extension,",
            "                            FLAC extraction flag, and media stream object.",
            "                            Returns TrackStreamInfo with None/empty values if fails.",
            "        \"\"\"",
            "        want_atmos = (",
            "            self.settings.data.download_dolby_atmos",
            "            and hasattr(media, \"audio_modes\")",
            "            and AudioMode.dolby_atmos.value in media.audio_modes",
            "        )",
            "",
            "        if want_atmos:",
            "            if not self.tidal.switch_to_atmos_session():",
            "                self.fn_logger.error(f\"Failed to switch to Atmos session for track: {media.id}\")",
            "                return TrackStreamInfo(None, \"\", False, None)",
            "        else:",
            "            if not self.tidal.restore_normal_session():",
            "                self.fn_logger.error(f\"Failed to restore normal session for track: {media.id}\")",
            "                return TrackStreamInfo(None, \"\", False, None)",
            "",
            "        media_stream = self.session.track(media.id).get_stream() if want_atmos else media.get_stream()",
            "",
            "        stream_manifest = media_stream.get_stream_manifest()",
            "        file_extension = stream_manifest.file_extension",
            "        requires_flac_extraction = False",
            "",
            "        if self.settings.data.extract_flac and (",
            "            stream_manifest.codecs.upper() == Codec.FLAC and file_extension != AudioExtensions.FLAC",
            "        ):",
            "            file_extension = AudioExtensions.FLAC",
            "            requires_flac_extraction = True",
            "",
            "        return TrackStreamInfo(",
            "            stream_manifest=stream_manifest,",
            "            file_extension=file_extension,",
            "            requires_flac_extraction=requires_flac_extraction,",
            "            media_stream=media_stream,",
            "        )",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "tidal_dl_ng/download.py",
          "class": null,
          "func": null,
          "lines": [
            "                - session: Main TIDAL API session",
            "                - atmos_session_context(): Dolby Atmos credential switching",
            "                - switch_to_atmos_session(): Dolby Atmos credential switching",
            "        Returns:",
            "            tuple[StreamManifest | None, str, bool, Stream | None]: Stream info.",
            "        tuple[StreamManifest | None, str, bool, Stream | None]: Stream info.",
            "        do_flac_extract: bool = False",
            "",
            "        if isinstance(media, Track):",
            "        file_extension: str = \"\"",
            "            try:",
            "                if (",
            "                    self.settings.data.download_dolby_atmos",
            "                    and hasattr(media, \"audio_modes\")",
            "                    and AudioMode.dolby_atmos.value in media.audio_modes",
            "                ):",
            "                    with self.tidal.atmos_session_context():",
            "                        atmos_track = self.session.track(media.id)",
            "                        media_stream = atmos_track.get_stream()",
            "                else:",
            "                    media_stream = media.get_stream()",
            "                if isinstance(media, Track):",
            "",
            "                stream_manifest = media_stream.get_stream_manifest()",
            "                    stream_manifest = track_info.stream_manifest",
            "                )",
            "",
            "                return None, \"\", False, None",
            "                self.fn_logger.exception(f\"Something went wrong. Skipping '{name_builder_item(media)}'.\")",
            "",
            "                return None, \"\", False, None",
            "",
            "            file_extension = stream_manifest.file_extension",
            "        return stream_manifest, file_extension, do_flac_extract, media_stream",
            "",
            "            if self.settings.data.extract_flac and (",
            "                stream_manifest.codecs.upper() == Codec.FLAC and file_extension != AudioExtensions.FLAC",
            "            ):",
            "                file_extension = AudioExtensions.FLAC",
            "                do_flac_extract = True",
            "        elif isinstance(media, Video):",
            "            file_extension = AudioExtensions.MP4 if self.settings.data.video_convert_mp4 else VideoExtensions.TS",
            "    def _get_track_stream_info(self, media: Track) -> TrackStreamInfo:",
            "",
            "        return stream_manifest, file_extension, do_flac_extract, media_stream",
            "        Args:"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/exislow/tidal-dl-ng/commit/0939bc0742cbc45f9cd2a2a21c335b238a288737",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/exislow/tidal-dl-ng/pull/626"
      },
      {
        "benign_code": {
          "context": "libraries/microsoft-agents-hosting-core/microsoft_agents/hosting/core/app/typing_indicator.py",
          "class": null,
          "func": null,
          "lines": [
            "from __future__ import annotations",
            "",
            "import asyncio",
            "    Encapsulates the logic for sending \"typing\" activity to the user.",
            "",
            "    Scoped to a single turn of conversation with the user.",
            "    \"\"\"",
            "        self._lock = asyncio.Lock()",
            "    def __init__(self, context: TurnContext, interval_seconds: float = 10.0) -> None:",
            "        \"\"\"Initializes a new instance of the TypingIndicator class.",
            "",
            "                return",
            "        :param context: The turn context.",
            "        :param interval_seconds: The interval in seconds between typing indicators.",
            "        \"\"\"",
            "        if interval_seconds <= 0:",
            "            raise ValueError(\"interval_seconds must be greater than 0\")",
            "        self._context: TurnContext = context",
            "        self._interval: float = interval_seconds",
            "        self._task: Optional[asyncio.Task[None]] = None",
            "",
            "        \"\"\"Continuously send typing indicators at the specified interval.\"\"\"",
            "    async def _run(self) -> None:",
            "        \"\"\"Sends typing indicators at regular intervals.\"\"\"",
            "",
            "        running_task = self._task",
            "        try:",
            "                await asyncio.sleep(self._intervalSeconds)",
            "            while running_task is self._task:",
            "                await self._context.send_activity(Activity(type=ActivityTypes.typing))",
            "                await asyncio.sleep(self._interval)",
            "        except asyncio.CancelledError:",
            "            logger.debug(\"Typing indicator loop cancelled\")",
            "            # Task was cancelled, exit gracefully",
            "            pass",
            "",
            "    def start(self) -> None:",
            "        \"\"\"Starts sending typing indicators.\"\"\"",
            "",
            "        if self._task is not None:",
            "            logger.warning(",
            "                \"Typing indicator is already running for conversation %s\",",
            "                self._context.activity.conversation.id,",
            "            )",
            "            return",
            "",
            "        logger.debug(",
            "            \"Starting typing indicator with interval: %s seconds in conversation %s\",",
            "            self._interval,",
            "            self._context.activity.conversation.id,",
            "        )",
            "        self._task = asyncio.create_task(self._run())",
            "",
            "    def stop(self) -> None:",
            "        \"\"\"Stops sending typing indicators.\"\"\"",
            "",
            "        if self._task is None:",
            "            logger.warning(",
            "                \"Typing indicator is not running for conversation %s\",",
            "                self._context.activity.conversation.id,",
            "            )",
            "            return",
            "",
            "        logger.debug(",
            "            \"Stopping typing indicator for conversation %s\",",
            "            self._context.activity.conversation.id,",
            "        )",
            "        self._task.cancel()",
            "        self._task = None"
          ]
        },
        "vulnerable_code": {
          "context": "libraries/microsoft-agents-hosting-core/microsoft_agents/hosting/core/app/typing_indicator.py",
          "class": null,
          "func": null,
          "lines": [
            "import logging",
            "",
            "from typing import Optional",
            "",
            "    def __init__(self, intervalSeconds=1) -> None:",
            "        self._intervalSeconds = intervalSeconds",
            "        self._task: Optional[asyncio.Task] = None",
            "        self._running: bool = False",
            "        self._lock = asyncio.Lock()",
            "    def __init__(self, context: TurnContext, interval_seconds: float = 10.0) -> None:",
            "",
            "    async def start(self, context: TurnContext) -> None:",
            "        async with self._lock:",
            "            if self._running:",
            "                return",
            "        :param context: The turn context.",
            "",
            "            logger.debug(",
            "                f\"Starting typing indicator with interval: {self._intervalSeconds} seconds\"",
            "            )",
            "            self._running = True",
            "            self._task = asyncio.create_task(self._typing_loop(context))",
            "",
            "    async def stop(self) -> None:",
            "        async with self._lock:",
            "            if not self._running:",
            "                return",
            "",
            "            logger.debug(\"Stopping typing indicator\")",
            "            self._running = False",
            "            task = self._task",
            "            self._task = None",
            "",
            "        # Cancel outside the lock to avoid blocking",
            "        if task and not task.done():",
            "            task.cancel()",
            "            try:",
            "                await task",
            "            except asyncio.CancelledError:",
            "                pass",
            "",
            "    async def _typing_loop(self, context: TurnContext):",
            "        \"\"\"Continuously send typing indicators at the specified interval.\"\"\"",
            "    async def _run(self) -> None:",
            "        try:",
            "            while True:",
            "                # Check running status under lock",
            "                async with self._lock:",
            "                    if not self._running:",
            "                        break",
            "",
            "                try:",
            "                    logger.debug(\"Sending typing activity\")",
            "                    await context.send_activity(Activity(type=ActivityTypes.typing))",
            "                except Exception as e:",
            "                    logger.error(f\"Error sending typing activity: {e}\")",
            "                    async with self._lock:",
            "                        self._running = False",
            "                    break",
            "",
            "                await asyncio.sleep(self._intervalSeconds)",
            "            while running_task is self._task:",
            "        except asyncio.CancelledError:",
            "            logger.debug(\"Typing indicator loop cancelled\")",
            "            # Task was cancelled, exit gracefully"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/microsoft/Agents-for-python/commit/10efb95e18529aaa3a1927e10add95f93683658a",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/microsoft/Agents-for-python/pull/212"
      },
      {
        "benign_code": {
          "context": "tronbyt_server/sync.py",
          "class": null,
          "func": null,
          "lines": [
            "import logging",
            "import os",
            "import ast",
            "import base64",
            "import redis",
            "from abc import ABC, abstractmethod",
            "from multiprocessing import Manager",
            "from multiprocessing import current_process",
            "from multiprocessing.synchronize import Event as MPEvent",
            "from multiprocessing.synchronize import Lock as MPLock",
            "from multiprocessing.managers import (",
            "    SyncManager,",
            "    DictProxy,",
            ")",
            "from typing import Any, cast",
            "class SyncManager(ABC):",
            "class AbstractSyncManager(ABC):",
            "    \"\"\"Abstract base class for synchronization managers.\"\"\"",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "    def __enter__(self) -> \"SyncManager\":",
            "    def __enter__(self) -> \"AbstractSyncManager\":",
            "        \"\"\"Enter the context manager.\"\"\"",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "                return False",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "                return False",
            "class MultiprocessingSyncManager(SyncManager):",
            "class ServerSyncManager(SyncManager):",
            "    \"\"\"A custom SyncManager that creates and vends singleton sync primitives.\"\"\"",
            "",
            "    _init_lock = Lock()",
            "    _initialized = False",
            "",
            "    _conditions: DictProxy[Any, Any]",
            "    _waiter_counts: DictProxy[Any, Any]",
            "    _lock: MPLock",
            "    _shutdown_event: MPEvent",
            "",
            "    def _lazy_init(self) -> None:",
            "        \"\"\"",
            "        Initialize the shared primitives using double-checked locking to avoid",
            "        acquiring the lock on every call.",
            "        \"\"\"",
            "        if not self._initialized:",
            "            with self._init_lock:",
            "                if not self._initialized:",
            "                    self._conditions = self.dict()",
            "                    self._waiter_counts = self.dict()",
            "                    self._lock = cast(MPLock, self.Lock())",
            "                    self._shutdown_event = cast(MPEvent, self.Event())",
            "                    self._initialized = True",
            "",
            "    def get_conditions(self) -> DictProxy[Any, Any]:",
            "        self._lazy_init()",
            "        return self._conditions",
            "",
            "    def get_waiter_counts(self) -> DictProxy[Any, Any]:",
            "        self._lazy_init()",
            "        return self._waiter_counts",
            "",
            "    def get_lock(self) -> MPLock:",
            "        self._lazy_init()",
            "        return self._lock",
            "",
            "    def get_shutdown_event(self) -> MPEvent:",
            "        self._lazy_init()",
            "        return self._shutdown_event",
            "",
            "    def notify_all_and_clear(self) -> None:",
            "        \"\"\"Notify all waiting conditions and clear them to prevent new waiters.\"\"\"",
            "        self._lazy_init()",
            "        with self._lock:",
            "            conditions = list(self._conditions.values())",
            "            # Clear conditions to prevent new waiters",
            "            self._conditions.clear()",
            "            self._waiter_counts.clear()",
            "        for condition in conditions:",
            "            with condition:",
            "                condition.notify_all()",
            "",
            "",
            "class MultiprocessingSyncManager(AbstractSyncManager):",
            "    \"\"\"A synchronization manager that uses multiprocessing primitives.\"\"\"",
            "        self.shutdown_event = manager.Event()",
            "    _manager: \"ServerSyncManager\"",
            "",
            "    def __init__(self, address: Any = None, authkey: bytes | None = None) -> None:",
            "        if address:",
            "            # Client mode: connect to the server manager.",
            "            manager = ServerSyncManager(address=address, authkey=authkey)",
            "            manager.connect()",
            "            self._manager = manager",
            "            self._is_server = False",
            "        else:",
            "            # Server mode: create the manager that hosts the singletons.",
            "            manager = ServerSyncManager()",
            "            manager.start()",
            "            self._manager = manager",
            "            self._is_server = True",
            "            self._export_connection_details()",
            "",
            "        # Get proxies to the singleton objects.",
            "        self._conditions = self._manager.get_conditions()",
            "        self._waiter_counts = self._manager.get_waiter_counts()",
            "        self._lock = self._manager.get_lock()",
            "        self._shutdown_event = self._manager.get_shutdown_event()",
            "",
            "    def _export_connection_details(self) -> None:",
            "        \"\"\"Set environment variables for client processes to connect.\"\"\"",
            "        if not self._is_server:",
            "            return",
            "",
            "        address = self.address",
            "        authkey = current_process().authkey",
            "",
            "        if address and authkey:",
            "            os.environ[\"TRONBYT_MP_MANAGER_ADDR\"] = repr(address)",
            "            os.environ[\"TRONBYT_MP_MANAGER_AUTHKEY\"] = base64.b64encode(authkey).decode(",
            "                \"ascii\"",
            "            )",
            "",
            "    @property",
            "    def address(self) -> Any:",
            "        \"\"\"Get the address of the manager process (server mode only).\"\"\"",
            "        if not self._is_server:",
            "            return None",
            "        return self._manager.address",
            "",
            "    def is_shutdown(self) -> bool:",
            "        return self._shutdown_event.is_set()",
            "",
            "            if device_id not in self._conditions:",
            "                # We need to create the Condition object via the manager so it can",
            "                # be shared across processes.",
            "                self._conditions[device_id] = self._manager.Condition()",
            "            condition = self._conditions[device_id]",
            "",
            "        return MultiprocessingWaiter(condition, self, device_id)",
            "        condition: \"ConditionType | None\" = None",
            "        condition: ConditionType | None = None",
            "        with self._lock:",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "        self._manager.shutdown()",
            "        if self._is_server:",
            "            self._shutdown_event.set()",
            "            self._manager.notify_all_and_clear()",
            "            self._manager.shutdown()",
            "",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "class RedisSyncManager(SyncManager):",
            "class RedisSyncManager(AbstractSyncManager):",
            "    \"\"\"A synchronization manager that uses Redis Pub/Sub.\"\"\"",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "_sync_manager: SyncManager | None = None",
            "_sync_manager: AbstractSyncManager | None = None",
            "_sync_manager_lock = Lock()",
            "def get_sync_manager() -> SyncManager:",
            "def get_sync_manager() -> AbstractSyncManager:",
            "    \"\"\"Get the synchronization manager for the application.\"\"\"",
            "                    _sync_manager = MultiprocessingSyncManager()",
            "                    # Check env vars for multiprocessing client mode",
            "                    manager_address_str = os.environ.get(\"TRONBYT_MP_MANAGER_ADDR\")",
            "                    manager_authkey_b64 = os.environ.get(\"TRONBYT_MP_MANAGER_AUTHKEY\")",
            "",
            "                    if manager_address_str and manager_authkey_b64:",
            "                        # Client mode for worker processes",
            "",
            "                        logger.info(\"Connecting to parent sync manager...\")",
            "                        try:",
            "                            address = ast.literal_eval(manager_address_str)",
            "                            authkey = base64.b64decode(manager_authkey_b64)",
            "                            _sync_manager = MultiprocessingSyncManager(",
            "                                address=address, authkey=authkey",
            "                            )",
            "                        except Exception as e:",
            "                            logger.error(",
            "                                f\"Failed to connect to parent sync manager: {e}\"",
            "                            )",
            "                            raise",
            "                    else:",
            "                        # Server mode for the main process",
            "                        logger.info(",
            "                            \"Using multiprocessing for synchronization (server)\"",
            "                        )",
            "                        _sync_manager = MultiprocessingSyncManager()",
            "    assert _sync_manager is not None"
          ]
        },
        "vulnerable_code": {
          "context": "tronbyt_server/sync.py",
          "class": null,
          "func": null,
          "lines": [
            "from abc import ABC, abstractmethod",
            "from multiprocessing import Manager",
            "from multiprocessing import current_process",
            "from typing import Any, cast",
            "",
            "import redis",
            "from threading import Lock",
            "",
            "class SyncManager(ABC):",
            "class AbstractSyncManager(ABC):",
            "    @abstractmethod",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "    @abstractmethod",
            "    def __enter__(self) -> \"SyncManager\":",
            "    def __enter__(self) -> \"AbstractSyncManager\":",
            "        with self._condition:",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "            notified = self._condition.wait(timeout=timeout)",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "",
            "class MultiprocessingSyncManager(SyncManager):",
            "class ServerSyncManager(SyncManager):",
            "",
            "    def __init__(self) -> None:",
            "        manager = Manager()",
            "        self._conditions: dict[str, \"ConditionType\"] = cast(",
            "            dict[str, \"ConditionType\"], manager.dict()",
            "        )",
            "        self._waiter_counts: dict[str, int] = cast(dict[str, int], manager.dict())",
            "        self._lock = manager.Lock()",
            "        self._manager = manager",
            "        self.shutdown_event = manager.Event()",
            "    _manager: \"ServerSyncManager\"",
            "        \"\"\"Notify waiters for a given device ID.\"\"\"",
            "        condition: \"ConditionType | None\" = None",
            "        condition: ConditionType | None = None",
            "",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "        self.shutdown_event.set()",
            "        with self._lock:",
            "            for condition in self._conditions.values():",
            "                with condition:",
            "                    condition.notify_all()",
            "        self._manager.shutdown()",
            "        if self._is_server:",
            "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "class RedisSyncManager(SyncManager):",
            "class RedisSyncManager(AbstractSyncManager):",
            "",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "_sync_manager: SyncManager | None = None",
            "_sync_manager: AbstractSyncManager | None = None",
            "",
            "def get_sync_manager() -> SyncManager:",
            "def get_sync_manager() -> AbstractSyncManager:",
            "                else:",
            "                    logger.info(\"Using multiprocessing for synchronization\")",
            "                    _sync_manager = MultiprocessingSyncManager()",
            "                    # Check env vars for multiprocessing client mode"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/tronbyt/server/commit/a9182d00866f8b6776c99bd640049ac954f8c541",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/tronbyt/server/pull/467"
      },
      {
        "benign_code": {
          "context": "vllm/v1/worker/gpu_model_runner.py",
          "class": null,
          "func": null,
          "lines": [
            "                                              torch.Tensor]] = None",
            "        self.transfer_event = torch.cuda.Event()",
            "        self.sampled_token_ids_pinned_cpu = torch.empty(",
            "            (self.max_model_len, 1),",
            "            dtype=torch.int64,",
            "            device=\"cpu\",",
            "            pin_memory=True)",
            "",
            "            valid_sampled_token_ids = sampled_token_ids.tolist()",
            "            valid_sampled_token_ids = self._to_list(sampled_token_ids)",
            "        else:",
            "            force_attention: If True, always create attention metadata. Used to ",
            "            force_attention: If True, always create attention metadata. Used to",
            "                warm up attention backend when mode is NONE.",
            "        return kv_cache_spec",
            "",
            "    def _to_list(self, sampled_token_ids: torch.Tensor) -> list[list[int]]:",
            "        # This is a short term mitigation for issue mentioned in",
            "        # https://github.com/vllm-project/vllm/issues/22754.",
            "        # `tolist` would trigger a cuda wise stream sync, which",
            "        # would block other copy ops from other cuda streams.",
            "        # A cuda event sync would avoid such a situation. Since",
            "        # this is in the critical path of every single model",
            "        # forward loop, this has caused perf issue for a disagg",
            "        # setup.",
            "        pinned = self.sampled_token_ids_pinned_cpu[:sampled_token_ids.shape[0]]",
            "        pinned.copy_(sampled_token_ids, non_blocking=True)",
            "        self.transfer_event.record()",
            "        self.transfer_event.synchronize()",
            "        return pinned.tolist()"
          ]
        },
        "vulnerable_code": {
          "context": "vllm/v1/worker/gpu_model_runner.py",
          "class": null,
          "func": null,
          "lines": [
            "            # No spec decode tokens.",
            "            valid_sampled_token_ids = sampled_token_ids.tolist()",
            "            valid_sampled_token_ids = self._to_list(sampled_token_ids)",
            "                    needed.",
            "            force_attention: If True, always create attention metadata. Used to ",
            "            force_attention: If True, always create attention metadata. Used to"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/vllm-project/vllm/commit/b395b3b0a3166d17c75e74f4eaf0ff4b15f2554f",
        "CWE": "CWE-366",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/vllm-project/vllm/pull/22760"
      }
    ],
    "CWD-1120": [
      {
        "benign_code": {
          "context": "src/exabgp/configuration/process/parser.py",
          "class": null,
          "func": null,
          "lines": [
            "def run(tokeniser):",
            "    \"\"\"Parse and validate the 'run' command for a process.",
            "",
            "    Args:",
            "        tokeniser: Configuration tokeniser providing command tokens",
            "",
            "    Returns:",
            "        List containing program path and arguments",
            "",
            "    Raises:",
            "        ValueError: If program cannot be found or validated",
            "        OSError: If file access fails",
            "    \"\"\"",
            "    prg = tokeniser()",
            "        raise ValueError('exabgp will not be able to run this program \"%s\"' % prg)",
            "    # Validate program using file descriptor to mitigate TOCTOU attacks",
            "    # Open file first to get a handle, then validate using fstat on the handle",
            "    #",
            "    # Security note: We allow following symlinks (no O_NOFOLLOW) because:",
            "    # 1. Symlinks are commonly used for executables (e.g., /usr/bin/python3)",
            "    # 2. TOCTOU protection comes from using fstat() on the file descriptor,",
            "    #    not from blocking symlinks",
            "    # 3. We validate the final target file that the descriptor points to",
            "    fd = None",
            "    try:",
            "        try:",
            "            fd = os.open(prg, os.O_RDONLY)",
            "        except OSError as e:",
            "            if e.errno == 2:  # ENOENT",
            "                raise ValueError('can not locate the program \"%s\"' % prg) from e",
            "            # Preserve exception chain for debugging while providing clear message",
            "            raise ValueError('can not access program \"%s\": %s' % (prg, e)) from e",
            "",
            "        # Use fstat on file descriptor - this is safe from TOCTOU",
            "        # The file descriptor points to the final target, even if prg was a symlink",
            "        s = os.fstat(fd)",
            "",
            "        if stat.S_ISDIR(s.st_mode):",
            "            raise ValueError('can not execute directories \"%s\"' % prg)",
            "",
            "        # Security check: refuse to run setuid/setgid programs",
            "        if s.st_mode & stat.S_ISUID:",
            "            raise ValueError('refusing to run setuid programs \"%s\"' % prg)",
            "",
            "        if s.st_mode & stat.S_ISGID:",
            "            raise ValueError('refusing to run setgid programs \"%s\"' % prg)",
            "",
            "        # Check if file is executable by current user",
            "        check = stat.S_IXOTH",
            "        if s.st_uid == os.getuid():",
            "            check |= stat.S_IXUSR",
            "        if s.st_gid == os.getgid():",
            "            check |= stat.S_IXGRP",
            "",
            "        if not (check & s.st_mode):",
            "            raise ValueError('exabgp will not be able to run this program \"%s\"' % prg)",
            "",
            "        # Additional security check: ensure it's a regular file",
            "        if not stat.S_ISREG(s.st_mode):",
            "            raise ValueError('program must be a regular file \"%s\"' % prg)",
            "",
            "    finally:",
            "        if fd is not None:",
            "            os.close(fd)",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "src/exabgp/configuration/process/parser.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    if not os.path.exists(prg):",
            "        raise ValueError('can not locate the the program \"%s\"' % prg)",
            "",
            "    # race conditions are possible, those are sanity checks not security ones ...",
            "    s = os.stat(prg)",
            "",
            "    if stat.S_ISDIR(s.st_mode):",
            "        raise ValueError('can not execute directories \"%s\"' % prg)",
            "",
            "    if s.st_mode & stat.S_ISUID:",
            "        raise ValueError('refusing to run setuid programs \"%s\"' % prg)",
            "",
            "    check = stat.S_IXOTH",
            "    if s.st_uid == os.getuid():",
            "        check |= stat.S_IXUSR",
            "    if s.st_gid == os.getgid():",
            "        check |= stat.S_IXGRP",
            "",
            "    if not check & s.st_mode:",
            "        raise ValueError('exabgp will not be able to run this program \"%s\"' % prg)",
            "    # Validate program using file descriptor to mitigate TOCTOU attacks"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/Exa-Networks/exabgp/commit/ff31a8718e0d73ae7f3beb0203fc9a641b353cc4",
        "CWE": "CWE-368",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/Exa-Networks/exabgp/pull/1279"
      }
    ],
    "CWD-1122": [
      {
        "benign_code": {
          "context": "newton/_src/utils/download_assets.py",
          "class": null,
          "func": null,
          "lines": [
            "import tempfile",
            "import time",
            "from pathlib import Path",
            "",
            "try:",
            "    from warp.thirdparty.appdirs import user_cache_dir",
            "except (ImportError, ModuleNotFoundError):",
            "    from warp._src.thirdparty.appdirs import user_cache_dir",
            "",
            "",
            "def _get_newton_cache_dir() -> str:",
            "    \"\"\"Gets the persistent Newton cache directory.\"\"\"",
            "    if \"NEWTON_CACHE_PATH\" in os.environ:",
            "        return os.environ[\"NEWTON_CACHE_PATH\"]",
            "    return user_cache_dir(\"newton\", \"newton-physics\")",
            "",
            "",
            "",
            "def _get_latest_commit_via_git(git_url: str, branch: str) -> str | None:",
            "    \"\"\"Resolve latest commit SHA for a branch via 'git ls-remote'.\"\"\"",
            "    try:",
            "        import git  # noqa: PLC0415",
            "",
            "        out = git.cmd.Git().ls_remote(\"--heads\", git_url, branch)",
            "        # Output format: \"<sha>\\trefs/heads/<branch>\\n\"",
            "        return out.split()[0] if out else None",
            "    except Exception:",
            "        # Fail silently on any error (offline, auth issue, etc.)",
            "        return None",
            "",
            "",
            "def _read_cached_commit(cache_folder: Path) -> str | None:",
            "    \"\"\"Return HEAD commit of cached repo, or None on failure.\"\"\"",
            "    try:",
            "        import git  # noqa: PLC0415",
            "",
            "        repo = git.Repo(cache_folder)",
            "        try:",
            "            return repo.head.commit.hexsha",
            "        finally:",
            "            repo.close()",
            "    except Exception:",
            "        return None",
            "",
            "",
            "def _stamp_fresh(stamp_file: Path, ttl_seconds: int) -> bool:",
            "    \"\"\"True if stamp file exists and is younger than TTL.\"\"\"",
            "    try:",
            "        return stamp_file.exists() and (time.time() - stamp_file.stat().st_mtime) < ttl_seconds",
            "    except OSError:",
            "        return False",
            "",
            "",
            "def _touch(path: Path) -> None:",
            "    \"\"\"Create/refresh a file's mtime; ignore filesystem errors.\"\"\"",
            "    try:",
            "        path.parent.mkdir(parents=True, exist_ok=True)",
            "        path.touch(exist_ok=True)",
            "    except OSError:",
            "        pass",
            "",
            "",
            "def download_git_folder(",
            "",
            "    Uses the cached version when up-to-date; otherwise refreshes by comparing the",
            "    cached repo's HEAD with the remote's latest commit (via 'git ls-remote').",
            "",
            "    Args:",
            "        cache_dir: Directory to cache downloads. If None, uses system temp directory",
            "        cache_dir: Directory to cache downloads.",
            "            If ``None``, the path is determined in the following order:",
            "            1. ``NEWTON_CACHE_PATH`` environment variable.",
            "            2. System's user cache directory (via ``appdirs.user_cache_dir``).",
            "        branch: Git branch/tag/commit to checkout (default: \"main\")",
            "",
            "        cache_dir = _get_newton_cache_dir()",
            "    cache_path = Path(cache_dir)",
            "        if target_folder.exists():",
            "    target_folder = cache_folder / folder_path",
            "",
            "    # 1. Handle force_refresh",
            "    if force_refresh and cache_folder.exists():",
            "        _safe_rmtree(cache_folder)",
            "",
            "    # 2. Check cache validity using Git",
            "    # TTL to avoid repeated network checks",
            "    stamp_file = cache_folder / \".newton_last_check\"",
            "    ttl_seconds = 3600",
            "",
            "    is_cached = target_folder.exists() and (cache_folder / \".git\").exists()",
            "    if is_cached and not force_refresh:",
            "        if _stamp_fresh(stamp_file, ttl_seconds):",
            "            return target_folder",
            "",
            "        current_commit = _read_cached_commit(cache_folder)",
            "        latest_commit = _get_latest_commit_via_git(git_url, branch)",
            "",
            "        # If we cannot determine latest (offline, etc.) or they match, use cache",
            "        if latest_commit is None or (current_commit is not None and latest_commit == current_commit):",
            "            _touch(stamp_file)",
            "            return target_folder",
            "    if cache_folder.exists():",
            "        # Different commit detected: clear cache to refresh",
            "        print(",
            "            f\"New version of {folder_path} found (cached: {str(current_commit)[:7] if current_commit else 'unknown'}, \"",
            "            f\"latest: {latest_commit[:7]}). Refreshing...\"",
            "        )",
            "        _safe_rmtree(cache_folder)",
            "",
            "    # 3. Download if not cached (or if cache was just cleared)",
            "    try:",
            "",
            "        _touch(stamp_file)",
            "",
            "        repo.close()",
            "",
            "        print(f\"Successfully downloaded folder to: {target_folder}\")",
            "        cache_dir: Cache directory to clear. If None, uses default temp directory",
            "        cache_dir: Cache directory to clear.",
            "            If ``None``, the path is determined in the following order:",
            "            1. ``NEWTON_CACHE_PATH`` environment variable.",
            "            2. System's user cache directory (via ``appdirs.user_cache_dir``).",
            "    \"\"\"",
            "        cache_dir = os.path.join(tempfile.gettempdir(), \"newton_git_cache\")",
            "        cache_dir = _get_newton_cache_dir()",
            "",
            "        cache_dir: Directory to cache downloads. If None, uses system temp directory",
            "        cache_dir: Directory to cache downloads.",
            "            If ``None``, the path is determined in the following order:",
            "            1. ``NEWTON_CACHE_PATH`` environment variable.",
            "            2. System's user cache directory (via ``appdirs.user_cache_dir``).",
            "        force_refresh: If True, re-downloads even if cached version exists"
          ]
        },
        "vulnerable_code": {
          "context": "newton/_src/utils/download_assets.py",
          "class": null,
          "func": null,
          "lines": [
            "import stat",
            "import tempfile",
            "import time",
            "        folder_path: The path to the folder within the repository (e.g., \"assets/models\")",
            "        cache_dir: Directory to cache downloads. If None, uses system temp directory",
            "        cache_dir: Directory to cache downloads.",
            "    if cache_dir is None:",
            "        cache_dir = os.path.join(tempfile.gettempdir(), \"newton_git_cache\")",
            "",
            "        cache_dir = _get_newton_cache_dir()",
            "",
            "    # Check if already cached and not forcing refresh",
            "    if cache_folder.exists() and not force_refresh:",
            "        target_folder = cache_folder / folder_path",
            "        if target_folder.exists():",
            "    target_folder = cache_folder / folder_path",
            "",
            "    # Clean up existing cache folder if it exists",
            "    if cache_folder.exists():",
            "        # Different commit detected: clear cache to refresh",
            "        # Verify the folder exists",
            "        target_folder = cache_folder / folder_path",
            "        if not target_folder.exists():",
            "    Args:",
            "        cache_dir: Cache directory to clear. If None, uses default temp directory",
            "        cache_dir: Cache directory to clear.",
            "    if cache_dir is None:",
            "        cache_dir = os.path.join(tempfile.gettempdir(), \"newton_git_cache\")",
            "        cache_dir = _get_newton_cache_dir()",
            "        asset_folder: The folder within the repository to download (e.g., \"assets/models\")",
            "        cache_dir: Directory to cache downloads. If None, uses system temp directory",
            "        cache_dir: Directory to cache downloads."
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/newton-physics/newton/commit/d30d125fce4522a89f6172bdd0cad6880b64b271",
        "CWE": "CWE-412",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/newton-physics/newton/pull/970"
      }
    ],
    "CWD-1124": [
      {
        "benign_code": {
          "context": "press/infrastructure/doctype/virtual_disk_resize/virtual_disk_resize.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "\tdef get_lock(self):",
            "\t\ttry:",
            "\t\t\tfrappe.get_value(\"Virtual Machine\", self.virtual_machine, \"status\", for_update=True)",
            "\t\t\treturn True",
            "\t\texcept frappe.QueryTimeoutError:",
            "\t\t\tfrappe.db.rollback()",
            "\t\t\tself.add_comment(\"Could not acquire lock, the virtual machine seems to be busy.\")",
            "\t\t\tfrappe.db.commit()",
            "\t\t\treturn False",
            "",
            "\t@frappe.whitelist()",
            "\tdef execute(self):",
            "\t\tif not self.get_lock():",
            "\t\t\treturn",
            "\t\tself.run_prerequisites()"
          ]
        },
        "vulnerable_code": {
          "context": "press/infrastructure/doctype/virtual_disk_resize/virtual_disk_resize.py",
          "class": null,
          "func": null,
          "lines": [
            "\t\t\tself.save()",
            "\t\texcept frappe.QueryTimeoutError:",
            "\t\t\tfrappe.db.rollback()",
            "\t\t\tself.status = Status.Scheduled",
            "\t\t\tself.save()",
            "\t\texcept Exception as e:"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/frappe/press/commit/17dcfaa8b0f414a4617eba24e8b993f2cba2f243",
        "CWE": "CWE-414",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/frappe/press/pull/3878"
      },
      {
        "benign_code": {
          "context": "press/infrastructure/doctype/virtual_disk_resize/virtual_disk_resize.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "\tdef get_lock(self):",
            "\t\ttry:",
            "\t\t\tfrappe.get_value(\"Virtual Machine\", self.virtual_machine, \"status\", for_update=True)",
            "\t\t\treturn True",
            "\t\texcept frappe.QueryTimeoutError:",
            "\t\t\tfrappe.db.rollback()",
            "\t\t\tself.add_comment(\"Could not acquire lock, the virtual machine seems to be busy.\")",
            "\t\t\tfrappe.db.commit()",
            "\t\t\treturn False",
            "",
            "\t@frappe.whitelist()",
            "\tdef execute(self):",
            "\t\tif not self.get_lock():",
            "\t\t\treturn",
            "\t\tself.run_prerequisites()"
          ]
        },
        "vulnerable_code": {
          "context": "press/infrastructure/doctype/virtual_disk_resize/virtual_disk_resize.py",
          "class": null,
          "func": null,
          "lines": [
            "\t\t\tself.save()",
            "\t\texcept frappe.QueryTimeoutError:",
            "\t\t\tfrappe.db.rollback()",
            "\t\t\tself.status = Status.Scheduled",
            "\t\t\tself.save()",
            "\t\texcept Exception as e:"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/frappe/press/commit/a734ba4815acfdb670847fbd1aeeff9c1f7284f4",
        "CWE": "CWE-414",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/frappe/press/pull/3875"
      }
    ],
    "CWD-1125": [
      {
        "benign_code": {
          "context": "packages/nvidia_nat_adk/src/nat/plugins/adk/adk_callback_handler.py",
          "class": null,
          "func": null,
          "lines": [
            "        super().__init__()",
            "    _instance: \"ADKProfilerHandler | None\" = None",
            "",
            "    def __new__(cls):",
            "        if cls._instance is None:",
            "            cls._instance = super().__new__(cls)",
            "",
            "        return cls._instance",
            "",
            "    def __init__(self):",
            "        self._lock = threading.Lock()",
            "        self.last_call_ts = time.time()",
            "        self.last_call_ts = 0.0",
            "        self.step_manager = Context.get().intermediate_step_manager",
            "        if getattr(self, \"_instrumented\", False):",
            "        if self._instrumented:",
            "            logger.debug(\"ADKProfilerHandler already instrumented; skipping.\")",
            "            return",
            "",
            "        try:",
            "            import litellm",
            "        except Exception as _e:",
            "            logger.exception(\"litellm import failed; skipping instrumentation\")",
            "            return",
            "        try:",
            "            FunctionTool.run_async = self._tool_use_monkey_patch()",
            "        self._original_tool_call = FunctionTool.run_async",
            "        self._original_llm_call = litellm.acompletion",
            "",
            "            litellm.acompletion = self._llm_call_monkey_patch()",
            "        FunctionTool.run_async = self._tool_use_monkey_patch()",
            "        litellm.acompletion = self._llm_call_monkey_patch()",
            "",
            "            if self._original_tool_call:",
            "            if self._original_tool_call is not None:",
            "                FunctionTool.run_async = self._original_tool_call",
            "            if self._original_llm_call:",
            "                self._original_tool_call = None",
            "",
            "            if self._original_llm_call is not None:",
            "                litellm.acompletion = self._original_llm_call",
            "                self._original_llm_call = None",
            "",
            "            self._instrumented = False",
            "            self.last_call_ts = 0.0",
            "            logger.debug(\"ADKProfilerHandler uninstrumented successfully.\")",
            "",
            "    def ensure_last_call_ts_initialized(self) -> float:",
            "        \"\"\" Ensure that last_call_ts is initialized to avoid issues in async calls. \"\"\"",
            "        if self.last_call_ts == 0.0:",
            "            with self._lock:",
            "                # Now that we have the lock, double-check",
            "                if self.last_call_ts == 0.0:",
            "                    self.last_call_ts = time.time()",
            "        return self.last_call_ts",
            "",
            "    def _tool_use_monkey_patch(self) -> Callable[..., Any]:",
            "            \"\"\"",
            "            self.ensure_last_call_ts_initialized()",
            "            now = time.time()",
            "            \"\"\"",
            "            self.ensure_last_call_ts_initialized()",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "packages/nvidia_nat_adk/src/nat/plugins/adk/adk_callback_handler.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    def __init__(self) -> None:",
            "        super().__init__()",
            "    _instance: \"ADKProfilerHandler | None\" = None",
            "        self._lock = threading.Lock()",
            "        self.last_call_ts = time.time()",
            "        self.last_call_ts = 0.0",
            "        \"\"\"",
            "        import litellm",
            "",
            "        if getattr(self, \"_instrumented\", False):",
            "        if self._instrumented:",
            "        # Save the originals",
            "        self._original_tool_call = getattr(FunctionTool, \"run_async\", None)",
            "        self._original_llm_call = getattr(litellm, \"acompletion\", None)",
            "",
            "        # Patch if available",
            "        if self._original_tool_call:",
            "            FunctionTool.run_async = self._tool_use_monkey_patch()",
            "        self._original_tool_call = FunctionTool.run_async",
            "",
            "        if self._original_llm_call:",
            "            litellm.acompletion = self._llm_call_monkey_patch()",
            "        FunctionTool.run_async = self._tool_use_monkey_patch()",
            "            from google.adk.tools.function_tool import FunctionTool",
            "            if self._original_tool_call:",
            "            if self._original_tool_call is not None:",
            "                FunctionTool.run_async = self._original_tool_call",
            "            if self._original_llm_call:",
            "                self._original_tool_call = None"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/NVIDIA/NeMo-Agent-Toolkit/commit/23e1e20d066e5815821869f2fdb054413aeb2c63",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/NVIDIA/NeMo-Agent-Toolkit/pull/1175"
      },
      {
        "benign_code": {
          "context": "unstract/connectors/src/unstract/connectors/filesystems/google_drive/google_drive.py",
          "class": null,
          "func": null,
          "lines": [
            "import os",
            "import threading",
            "from datetime import datetime",
            "        self.drive = GDriveFileSystem(path=\"root\", google_auth=gauth)",
            "        # Store settings file path for lazy initialization",
            "        self._settings_file = f\"{os.path.dirname(__file__)}/static/settings.yaml\"",
            "",
            "        # Lazy initialization - create client only when needed (after fork)",
            "        # This prevents SIGSEGV crashes in Celery ForkPoolWorker processes",
            "        self._drive = None",
            "        self._drive_lock = threading.Lock()",
            "",
            "        return self.drive",
            "        \"\"\"Get GDrive filesystem with lazy initialization (fork-safe).",
            "",
            "        This method creates the Google Drive API client on first access,",
            "        ensuring it's created AFTER Celery fork to avoid SIGSEGV crashes.",
            "",
            "        The lazy initialization pattern ensures that gRPC-based Google API",
            "        clients are created in the child process after fork(), not in the",
            "        parent process before fork.",
            "",
            "        Returns:",
            "            GDriveFileSystem: The initialized Google Drive filesystem client",
            "        \"\"\"",
            "        if self._drive is None:",
            "            with self._drive_lock:",
            "                # Double-check pattern for thread safety",
            "                if self._drive is None:",
            "                    logger.info(\"Initializing Google Drive client (lazy init after fork)\")",
            "                    gauth = GoogleAuth(",
            "                        settings_file=self._settings_file,",
            "                        settings={\"client_config\": self.client_secrets[\"web\"]},",
            "                    )",
            "                    gauth.credentials = OAuth2Credentials.from_json(",
            "                        json_data=json.dumps(self.oauth2_credentials)",
            "                    )",
            "                    self._drive = GDriveFileSystem(path=\"root\", google_auth=gauth)",
            "                    logger.info(\"Google Drive client initialized successfully\")",
            "",
            "        return self._drive",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "unstract/connectors/src/unstract/connectors/filesystems/google_drive/google_drive.py",
          "class": null,
          "func": null,
          "lines": [
            "        }",
            "        gauth = GoogleAuth(",
            "            settings_file=f\"{os.path.dirname(__file__)}/static/settings.yaml\",",
            "            settings={\"client_config\": self.client_secrets[\"web\"]},",
            "        )",
            "        gauth.credentials = OAuth2Credentials.from_json(",
            "            json_data=json.dumps(self.oauth2_credentials)",
            "        )",
            "        self.drive = GDriveFileSystem(path=\"root\", google_auth=gauth)",
            "        # Store settings file path for lazy initialization",
            "    def get_fsspec_fs(self) -> GDriveFileSystem:",
            "        return self.drive",
            "        \"\"\"Get GDrive filesystem with lazy initialization (fork-safe)."
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/Zipstack/unstract/commit/2bef7d30f20a146109a414116737757c9d24d746",
        "CWE": "CWE-609",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/Zipstack/unstract/pull/1597"
      }
    ],
    "CWD-1126": [
      {
        "benign_code": {
          "context": "astrbot/core/pipeline/process_stage/method/llm_request.py",
          "class": null,
          "func": null,
          "lines": [
            "from astrbot.core.utils.metrics import Metric",
            "from astrbot.core.utils.session_lock import session_lock_manager",
            "",
            "    async def process(",
            "    async def _apply_kb_context(",
            "        self,",
            "        # ",
            "        req: ProviderRequest,",
            "    ):",
            "        \"\"\"\"\"\"",
            "        try:",
            "            return",
            "    def _truncate_contexts(",
            "        self,",
            "        contexts: list[dict],",
            "    ) -> list[dict]:",
            "        \"\"\"\"\"\"",
            "        if self.max_context_length == -1:",
            "            return contexts",
            "",
            "            req.contexts = json.loads(req.contexts)",
            "        if len(contexts) // 2 <= self.max_context_length:",
            "            return contexts",
            "",
            "            req.session_id = event.unified_msg_origin",
            "        truncated_contexts = contexts[",
            "            -(self.max_context_length - self.dequeue_context_length + 1) * 2 :",
            "        ]",
            "        # role  user ",
            "        index = next(",
            "            (",
            "                i",
            "                for i, item in enumerate(truncated_contexts)",
            "                if item.get(\"role\") == \"user\"",
            "            ),",
            "            None,",
            "        )",
            "        if index is not None and index > 0:",
            "            truncated_contexts = truncated_contexts[index:]",
            "",
            "        req.contexts = self.fix_messages(req.contexts)",
            "        return truncated_contexts",
            "",
            "        # //",
            "    def _modalities_fix(",
            "        self,",
            "        provider: Provider,",
            "        req: ProviderRequest,",
            "    ):",
            "        \"\"\"\"\"\"",
            "        if req.image_urls:",
            "        # ",
            "",
            "    def _plugin_tool_fix(",
            "        self,",
            "        event: AstrMessageEvent,",
            "        req: ProviderRequest,",
            "    ):",
            "        \"\"\"\"\"\"",
            "        if event.plugins_name is not None and req.func_tool:",
            "    def fix_messages(self, messages: list[dict]) -> list[dict]:",
            "    def _fix_messages(self, messages: list[dict]) -> list[dict]:",
            "        \"\"\"\"\"\"",
            "        return fixed_messages",
            "",
            "    async def process(",
            "        self,",
            "        event: AstrMessageEvent,",
            "        _nested: bool = False,",
            "    ) -> None | AsyncGenerator[None, None]:",
            "        req: ProviderRequest | None = None",
            "",
            "        if not self.ctx.astrbot_config[\"provider_settings\"][\"enable\"]:",
            "            logger.debug(\" LLM \")",
            "            return",
            "",
            "        # LLM",
            "        if not SessionServiceManager.should_process_llm_request(event):",
            "            logger.debug(f\" {event.unified_msg_origin}  LLM\")",
            "            return",
            "",
            "        provider = self._select_provider(event)",
            "        if provider is None:",
            "            return",
            "        if not isinstance(provider, Provider):",
            "            logger.error(f\"({type(provider)}) LLM \")",
            "            return",
            "",
            "        streaming_response = self.streaming_response",
            "        if (enable_streaming := event.get_extra(\"enable_streaming\")) is not None:",
            "            streaming_response = bool(enable_streaming)",
            "",
            "        logger.debug(\"ready to request llm provider\")",
            "        async with session_lock_manager.acquire_lock(event.unified_msg_origin):",
            "            logger.debug(\"acquired session lock for llm request\")",
            "            if event.get_extra(\"provider_request\"):",
            "                req = event.get_extra(\"provider_request\")",
            "                assert isinstance(req, ProviderRequest), (",
            "                    \"provider_request  ProviderRequest \"",
            "                )",
            "",
            "                if req.conversation:",
            "                    req.contexts = json.loads(req.conversation.history)",
            "",
            "            else:",
            "                req = ProviderRequest(prompt=\"\", image_urls=[])",
            "                if sel_model := event.get_extra(\"selected_model\"):",
            "                    req.model = sel_model",
            "                if self.provider_wake_prefix and not event.message_str.startswith(",
            "                    self.provider_wake_prefix",
            "                ):",
            "                    return",
            "",
            "                req.prompt = event.message_str[len(self.provider_wake_prefix) :]",
            "                # func_tool selection  packages/astrbot ",
            "                # req.func_tool = self.ctx.plugin_manager.context.get_llm_tool_manager()",
            "                for comp in event.message_obj.message:",
            "                    if isinstance(comp, Image):",
            "                        image_path = await comp.convert_to_file_path()",
            "                        req.image_urls.append(image_path)",
            "",
            "                conversation = await self._get_session_conv(event)",
            "                req.conversation = conversation",
            "                req.contexts = json.loads(conversation.history)",
            "",
            "                event.set_extra(\"provider_request\", req)",
            "",
            "            if not req.prompt and not req.image_urls:",
            "                return",
            "",
            "            # apply knowledge base context",
            "            await self._apply_kb_context(event, req)",
            "",
            "            # call event hook",
            "            if await call_event_hook(event, EventType.OnLLMRequestEvent, req):",
            "                return",
            "",
            "            # fix contexts json str",
            "            if isinstance(req.contexts, str):",
            "                req.contexts = json.loads(req.contexts)",
            "",
            "            # truncate contexts to fit max length",
            "            req.contexts = self._truncate_contexts(req.contexts)",
            "",
            "            # session_id",
            "            if not req.session_id:",
            "                req.session_id = event.unified_msg_origin",
            "",
            "            # fix messages",
            "            req.contexts = self._fix_messages(req.contexts)",
            "",
            "            # check provider modalities, if provider does not support image/tool_use, clear them in request.",
            "            self._modalities_fix(provider, req)",
            "",
            "            # filter tools, only keep tools from this pipeline's selected plugins",
            "            self._plugin_tool_fix(event, req)",
            "",
            "            stream_to_general = (",
            "                self.unsupported_streaming_strategy == \"turn_off\"",
            "                and not event.platform_meta.support_streaming_message",
            "            )",
            "            #  req.contexts",
            "            backup_contexts = copy.deepcopy(req.contexts)",
            "",
            "            # run agent",
            "            agent_runner = AgentRunner()",
            "            logger.debug(",
            "                f\"handle provider[id: {provider.provider_config['id']}] request: {req}\",",
            "            )",
            "            astr_agent_ctx = AstrAgentContext(",
            "                provider=provider,",
            "                first_provider_request=req,",
            "                curr_provider_request=req,",
            "                streaming=streaming_response,",
            "                event=event,",
            "            )",
            "            await agent_runner.reset(",
            "                provider=provider,",
            "                request=req,",
            "                run_context=AgentContextWrapper(",
            "                    context=astr_agent_ctx,",
            "                    tool_call_timeout=self.tool_call_timeout,",
            "                ),",
            "                tool_executor=FunctionToolExecutor(),",
            "                agent_hooks=MAIN_AGENT_HOOKS,",
            "                streaming=streaming_response,",
            "            )",
            "",
            "            if streaming_response and not stream_to_general:",
            "                # ",
            "                event.set_result(",
            "                    MessageEventResult()",
            "                    .set_result_content_type(ResultContentType.STREAMING_RESULT)",
            "                    .set_async_stream(",
            "                        run_agent(agent_runner, self.max_step, self.show_tool_use),",
            "                    ),",
            "                )",
            "                yield",
            "                if agent_runner.done():",
            "                    if final_llm_resp := agent_runner.get_final_llm_resp():",
            "                        if final_llm_resp.completion_text:",
            "                            chain = (",
            "                                MessageChain()",
            "                                .message(final_llm_resp.completion_text)",
            "                                .chain",
            "                            )",
            "                        elif final_llm_resp.result_chain:",
            "                            chain = final_llm_resp.result_chain.chain",
            "                        else:",
            "                            chain = MessageChain().chain",
            "                        event.set_result(",
            "                            MessageEventResult(",
            "                                chain=chain,",
            "                                result_content_type=ResultContentType.STREAMING_FINISH,",
            "                            ),",
            "                        )",
            "            else:",
            "                async for _ in run_agent(",
            "                    agent_runner, self.max_step, self.show_tool_use, stream_to_general",
            "                ):",
            "                    yield",
            "",
            "            #  contexts",
            "            req.contexts = backup_contexts",
            "",
            "            await self._save_to_history(event, req, agent_runner.get_final_llm_resp())",
            "",
            "        #  WebChat ",
            "        if event.get_platform_name() == \"webchat\":",
            "            asyncio.create_task(self._handle_webchat(event, req, provider))",
            "",
            "        asyncio.create_task(",
            "            Metric.upload(",
            "                llm_tick=1,",
            "                model_name=agent_runner.provider.get_model(),",
            "                provider_type=agent_runner.provider.meta().type,",
            "            ),",
            "        )"
          ]
        },
        "vulnerable_code": {
          "context": "astrbot/core/pipeline/process_stage/method/llm_request.py",
          "class": null,
          "func": null,
          "lines": [
            "            if \"call\" in ty.__dict__ and ty.__dict__[\"call\"] is not FunctionTool.call:",
            "                logger.debug(f\"Found call in: {ty}\")",
            "                is_override_call = True",
            "",
            "    async def process(",
            "    async def _apply_kb_context(",
            "        event: AstrMessageEvent,",
            "        _nested: bool = False,",
            "    ) -> None | AsyncGenerator[None, None]:",
            "        req: ProviderRequest | None = None",
            "",
            "        if not self.ctx.astrbot_config[\"provider_settings\"][\"enable\"]:",
            "            logger.debug(\" LLM \")",
            "            return",
            "",
            "        # LLM",
            "        if not SessionServiceManager.should_process_llm_request(event):",
            "            logger.debug(f\" {event.unified_msg_origin}  LLM\")",
            "            return",
            "",
            "        provider = self._select_provider(event)",
            "        if provider is None:",
            "            return",
            "        if not isinstance(provider, Provider):",
            "            logger.error(f\"({type(provider)}) LLM \")",
            "            return",
            "",
            "        streaming_response = self.streaming_response",
            "        if (enable_streaming := event.get_extra(\"enable_streaming\")) is not None:",
            "            streaming_response = bool(enable_streaming)",
            "",
            "        if event.get_extra(\"provider_request\"):",
            "            req = event.get_extra(\"provider_request\")",
            "            assert isinstance(req, ProviderRequest), (",
            "                \"provider_request  ProviderRequest \"",
            "            )",
            "",
            "            if req.conversation:",
            "                req.contexts = json.loads(req.conversation.history)",
            "",
            "        else:",
            "            req = ProviderRequest(prompt=\"\", image_urls=[])",
            "            if sel_model := event.get_extra(\"selected_model\"):",
            "                req.model = sel_model",
            "            if self.provider_wake_prefix:",
            "                if not event.message_str.startswith(self.provider_wake_prefix):",
            "                    return",
            "            req.prompt = event.message_str[len(self.provider_wake_prefix) :]",
            "            # func_tool selection  packages/astrbot ",
            "            # req.func_tool = self.ctx.plugin_manager.context.get_llm_tool_manager()",
            "            for comp in event.message_obj.message:",
            "                if isinstance(comp, Image):",
            "                    image_path = await comp.convert_to_file_path()",
            "                    req.image_urls.append(image_path)",
            "",
            "            conversation = await self._get_session_conv(event)",
            "            req.conversation = conversation",
            "            req.contexts = json.loads(conversation.history)",
            "",
            "            event.set_extra(\"provider_request\", req)",
            "",
            "        if not req.prompt and not req.image_urls:",
            "            return",
            "",
            "        # ",
            "        req: ProviderRequest,",
            "",
            "        #  LLM ",
            "        if await call_event_hook(event, EventType.OnLLMRequestEvent, req):",
            "            return",
            "    def _truncate_contexts(",
            "",
            "        if isinstance(req.contexts, str):",
            "            req.contexts = json.loads(req.contexts)",
            "        if len(contexts) // 2 <= self.max_context_length:",
            "",
            "        # max context length",
            "        if (",
            "            self.max_context_length != -1  # -1 ",
            "            and len(req.contexts) // 2 > self.max_context_length",
            "        ):",
            "            logger.debug(\"\")",
            "            req.contexts = req.contexts[",
            "                -(self.max_context_length - self.dequeue_context_length + 1) * 2 :",
            "            ]",
            "            # role  user ",
            "            index = next(",
            "                (",
            "                    i",
            "                    for i, item in enumerate(req.contexts)",
            "                    if item.get(\"role\") == \"user\"",
            "                ),",
            "                None,",
            "            )",
            "            if index is not None and index > 0:",
            "                req.contexts = req.contexts[index:]",
            "",
            "        # session_id",
            "        if not req.session_id:",
            "            req.session_id = event.unified_msg_origin",
            "        truncated_contexts = contexts[",
            "",
            "        # fix messages",
            "        req.contexts = self.fix_messages(req.contexts)",
            "        return truncated_contexts",
            "",
            "        # check provider modalities",
            "        # //",
            "    def _modalities_fix(",
            "                req.func_tool = None",
            "        # ",
            "",
            "",
            "        stream_to_general = (",
            "            self.unsupported_streaming_strategy == \"turn_off\"",
            "            and not event.platform_meta.support_streaming_message",
            "        )",
            "        #  req.contexts",
            "        backup_contexts = copy.deepcopy(req.contexts)",
            "",
            "        # run agent",
            "        agent_runner = AgentRunner()",
            "        logger.debug(",
            "            f\"handle provider[id: {provider.provider_config['id']}] request: {req}\",",
            "        )",
            "        astr_agent_ctx = AstrAgentContext(",
            "            provider=provider,",
            "            first_provider_request=req,",
            "            curr_provider_request=req,",
            "            streaming=streaming_response,",
            "            event=event,",
            "        )",
            "        await agent_runner.reset(",
            "            provider=provider,",
            "            request=req,",
            "            run_context=AgentContextWrapper(",
            "                context=astr_agent_ctx,",
            "                tool_call_timeout=self.tool_call_timeout,",
            "            ),",
            "            tool_executor=FunctionToolExecutor(),",
            "            agent_hooks=MAIN_AGENT_HOOKS,",
            "            streaming=streaming_response,",
            "        )",
            "",
            "        if streaming_response and not stream_to_general:",
            "            # ",
            "            event.set_result(",
            "                MessageEventResult()",
            "                .set_result_content_type(ResultContentType.STREAMING_RESULT)",
            "                .set_async_stream(",
            "                    run_agent(agent_runner, self.max_step, self.show_tool_use),",
            "                ),",
            "            )",
            "            yield",
            "            if agent_runner.done():",
            "                if final_llm_resp := agent_runner.get_final_llm_resp():",
            "                    if final_llm_resp.completion_text:",
            "                        chain = (",
            "                            MessageChain().message(final_llm_resp.completion_text).chain",
            "                        )",
            "                    elif final_llm_resp.result_chain:",
            "                        chain = final_llm_resp.result_chain.chain",
            "                    else:",
            "                        chain = MessageChain().chain",
            "                    event.set_result(",
            "                        MessageEventResult(",
            "                            chain=chain,",
            "                            result_content_type=ResultContentType.STREAMING_FINISH,",
            "                        ),",
            "                    )",
            "        else:",
            "            async for _ in run_agent(",
            "                agent_runner, self.max_step, self.show_tool_use, stream_to_general",
            "            ):",
            "                yield",
            "",
            "        #  contexts",
            "        req.contexts = backup_contexts",
            "",
            "        await self._save_to_history(event, req, agent_runner.get_final_llm_resp())",
            "",
            "        #  WebChat ",
            "        if event.get_platform_name() == \"webchat\":",
            "            asyncio.create_task(self._handle_webchat(event, req, provider))",
            "",
            "        asyncio.create_task(",
            "            Metric.upload(",
            "                llm_tick=1,",
            "                model_name=agent_runner.provider.get_model(),",
            "                provider_type=agent_runner.provider.meta().type,",
            "            ),",
            "        )",
            "",
            "    async def _handle_webchat(",
            "            if llm_resp and llm_resp.completion_text:",
            "                logger.debug(",
            "                    f\"WebChat : {llm_resp.completion_text.strip()}\",",
            "                )",
            "                title = llm_resp.completion_text.strip()",
            "",
            "    def fix_messages(self, messages: list[dict]) -> list[dict]:",
            "    def _fix_messages(self, messages: list[dict]) -> list[dict]:"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/AstrBotDevs/AstrBot/commit/395786187881f5cc56cb4e58addb271627ed4419",
        "CWE": "CWE-764",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/AstrBotDevs/AstrBot/pull/3607"
      }
    ],
    "CWD-1129": [
      {
        "benign_code": {
          "context": "tronbyt_server/sync.py",
          "class": null,
          "func": null,
          "lines": [
            "import logging",
            "import os",
            "import ast",
            "import base64",
            "import redis",
            "from abc import ABC, abstractmethod",
            "from multiprocessing import Manager",
            "from multiprocessing import current_process",
            "from multiprocessing.synchronize import Event as MPEvent",
            "from multiprocessing.synchronize import Lock as MPLock",
            "from multiprocessing.managers import (",
            "    SyncManager,",
            "    DictProxy,",
            ")",
            "from typing import Any, cast",
            "class SyncManager(ABC):",
            "class AbstractSyncManager(ABC):",
            "    \"\"\"Abstract base class for synchronization managers.\"\"\"",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "    def __enter__(self) -> \"SyncManager\":",
            "    def __enter__(self) -> \"AbstractSyncManager\":",
            "        \"\"\"Enter the context manager.\"\"\"",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "                return False",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "                return False",
            "class MultiprocessingSyncManager(SyncManager):",
            "class ServerSyncManager(SyncManager):",
            "    \"\"\"A custom SyncManager that creates and vends singleton sync primitives.\"\"\"",
            "",
            "    _init_lock = Lock()",
            "    _initialized = False",
            "",
            "    _conditions: DictProxy[Any, Any]",
            "    _waiter_counts: DictProxy[Any, Any]",
            "    _lock: MPLock",
            "    _shutdown_event: MPEvent",
            "",
            "    def _lazy_init(self) -> None:",
            "        \"\"\"",
            "        Initialize the shared primitives using double-checked locking to avoid",
            "        acquiring the lock on every call.",
            "        \"\"\"",
            "        if not self._initialized:",
            "            with self._init_lock:",
            "                if not self._initialized:",
            "                    self._conditions = self.dict()",
            "                    self._waiter_counts = self.dict()",
            "                    self._lock = cast(MPLock, self.Lock())",
            "                    self._shutdown_event = cast(MPEvent, self.Event())",
            "                    self._initialized = True",
            "",
            "    def get_conditions(self) -> DictProxy[Any, Any]:",
            "        self._lazy_init()",
            "        return self._conditions",
            "",
            "    def get_waiter_counts(self) -> DictProxy[Any, Any]:",
            "        self._lazy_init()",
            "        return self._waiter_counts",
            "",
            "    def get_lock(self) -> MPLock:",
            "        self._lazy_init()",
            "        return self._lock",
            "",
            "    def get_shutdown_event(self) -> MPEvent:",
            "        self._lazy_init()",
            "        return self._shutdown_event",
            "",
            "    def notify_all_and_clear(self) -> None:",
            "        \"\"\"Notify all waiting conditions and clear them to prevent new waiters.\"\"\"",
            "        self._lazy_init()",
            "        with self._lock:",
            "            conditions = list(self._conditions.values())",
            "            # Clear conditions to prevent new waiters",
            "            self._conditions.clear()",
            "            self._waiter_counts.clear()",
            "        for condition in conditions:",
            "            with condition:",
            "                condition.notify_all()",
            "",
            "",
            "class MultiprocessingSyncManager(AbstractSyncManager):",
            "    \"\"\"A synchronization manager that uses multiprocessing primitives.\"\"\"",
            "        self.shutdown_event = manager.Event()",
            "    _manager: \"ServerSyncManager\"",
            "",
            "    def __init__(self, address: Any = None, authkey: bytes | None = None) -> None:",
            "        if address:",
            "            # Client mode: connect to the server manager.",
            "            manager = ServerSyncManager(address=address, authkey=authkey)",
            "            manager.connect()",
            "            self._manager = manager",
            "            self._is_server = False",
            "        else:",
            "            # Server mode: create the manager that hosts the singletons.",
            "            manager = ServerSyncManager()",
            "            manager.start()",
            "            self._manager = manager",
            "            self._is_server = True",
            "            self._export_connection_details()",
            "",
            "        # Get proxies to the singleton objects.",
            "        self._conditions = self._manager.get_conditions()",
            "        self._waiter_counts = self._manager.get_waiter_counts()",
            "        self._lock = self._manager.get_lock()",
            "        self._shutdown_event = self._manager.get_shutdown_event()",
            "",
            "    def _export_connection_details(self) -> None:",
            "        \"\"\"Set environment variables for client processes to connect.\"\"\"",
            "        if not self._is_server:",
            "            return",
            "",
            "        address = self.address",
            "        authkey = current_process().authkey",
            "",
            "        if address and authkey:",
            "            os.environ[\"TRONBYT_MP_MANAGER_ADDR\"] = repr(address)",
            "            os.environ[\"TRONBYT_MP_MANAGER_AUTHKEY\"] = base64.b64encode(authkey).decode(",
            "                \"ascii\"",
            "            )",
            "",
            "    @property",
            "    def address(self) -> Any:",
            "        \"\"\"Get the address of the manager process (server mode only).\"\"\"",
            "        if not self._is_server:",
            "            return None",
            "        return self._manager.address",
            "",
            "    def is_shutdown(self) -> bool:",
            "        return self._shutdown_event.is_set()",
            "",
            "            if device_id not in self._conditions:",
            "                # We need to create the Condition object via the manager so it can",
            "                # be shared across processes.",
            "                self._conditions[device_id] = self._manager.Condition()",
            "            condition = self._conditions[device_id]",
            "",
            "        return MultiprocessingWaiter(condition, self, device_id)",
            "        condition: \"ConditionType | None\" = None",
            "        condition: ConditionType | None = None",
            "        with self._lock:",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "        self._manager.shutdown()",
            "        if self._is_server:",
            "            self._shutdown_event.set()",
            "            self._manager.notify_all_and_clear()",
            "            self._manager.shutdown()",
            "",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "class RedisSyncManager(SyncManager):",
            "class RedisSyncManager(AbstractSyncManager):",
            "    \"\"\"A synchronization manager that uses Redis Pub/Sub.\"\"\"",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "_sync_manager: SyncManager | None = None",
            "_sync_manager: AbstractSyncManager | None = None",
            "_sync_manager_lock = Lock()",
            "def get_sync_manager() -> SyncManager:",
            "def get_sync_manager() -> AbstractSyncManager:",
            "    \"\"\"Get the synchronization manager for the application.\"\"\"",
            "                    _sync_manager = MultiprocessingSyncManager()",
            "                    # Check env vars for multiprocessing client mode",
            "                    manager_address_str = os.environ.get(\"TRONBYT_MP_MANAGER_ADDR\")",
            "                    manager_authkey_b64 = os.environ.get(\"TRONBYT_MP_MANAGER_AUTHKEY\")",
            "",
            "                    if manager_address_str and manager_authkey_b64:",
            "                        # Client mode for worker processes",
            "",
            "                        logger.info(\"Connecting to parent sync manager...\")",
            "                        try:",
            "                            address = ast.literal_eval(manager_address_str)",
            "                            authkey = base64.b64decode(manager_authkey_b64)",
            "                            _sync_manager = MultiprocessingSyncManager(",
            "                                address=address, authkey=authkey",
            "                            )",
            "                        except Exception as e:",
            "                            logger.error(",
            "                                f\"Failed to connect to parent sync manager: {e}\"",
            "                            )",
            "                            raise",
            "                    else:",
            "                        # Server mode for the main process",
            "                        logger.info(",
            "                            \"Using multiprocessing for synchronization (server)\"",
            "                        )",
            "                        _sync_manager = MultiprocessingSyncManager()",
            "    assert _sync_manager is not None"
          ]
        },
        "vulnerable_code": {
          "context": "tronbyt_server/sync.py",
          "class": null,
          "func": null,
          "lines": [
            "from abc import ABC, abstractmethod",
            "from multiprocessing import Manager",
            "from multiprocessing import current_process",
            "from typing import Any, cast",
            "",
            "import redis",
            "from threading import Lock",
            "",
            "class SyncManager(ABC):",
            "class AbstractSyncManager(ABC):",
            "    @abstractmethod",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "    @abstractmethod",
            "    def __enter__(self) -> \"SyncManager\":",
            "    def __enter__(self) -> \"AbstractSyncManager\":",
            "        with self._condition:",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "            notified = self._condition.wait(timeout=timeout)",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "",
            "class MultiprocessingSyncManager(SyncManager):",
            "class ServerSyncManager(SyncManager):",
            "",
            "    def __init__(self) -> None:",
            "        manager = Manager()",
            "        self._conditions: dict[str, \"ConditionType\"] = cast(",
            "            dict[str, \"ConditionType\"], manager.dict()",
            "        )",
            "        self._waiter_counts: dict[str, int] = cast(dict[str, int], manager.dict())",
            "        self._lock = manager.Lock()",
            "        self._manager = manager",
            "        self.shutdown_event = manager.Event()",
            "    _manager: \"ServerSyncManager\"",
            "        \"\"\"Notify waiters for a given device ID.\"\"\"",
            "        condition: \"ConditionType | None\" = None",
            "        condition: ConditionType | None = None",
            "",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "        self.shutdown_event.set()",
            "        with self._lock:",
            "            for condition in self._conditions.values():",
            "                with condition:",
            "                    condition.notify_all()",
            "        self._manager.shutdown()",
            "        if self._is_server:",
            "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "class RedisSyncManager(SyncManager):",
            "class RedisSyncManager(AbstractSyncManager):",
            "",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "_sync_manager: SyncManager | None = None",
            "_sync_manager: AbstractSyncManager | None = None",
            "",
            "def get_sync_manager() -> SyncManager:",
            "def get_sync_manager() -> AbstractSyncManager:",
            "                else:",
            "                    logger.info(\"Using multiprocessing for synchronization\")",
            "                    _sync_manager = MultiprocessingSyncManager()",
            "                    # Check env vars for multiprocessing client mode"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/tronbyt/server/commit/a9182d00866f8b6776c99bd640049ac954f8c541",
        "CWE": "CWE-833",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/tronbyt/server/pull/467"
      }
    ],
    "CWD-1131": [
      {
        "benign_code": {
          "context": "libs/core/kiln_ai/utils/pdf_utils.py",
          "class": null,
          "func": null,
          "lines": [
            "import asyncio",
            "import atexit",
            "import tempfile",
            "pdf_conversion_executor = ProcessPoolExecutor(max_workers=1)",
            "_pdf_conversion_executor: ProcessPoolExecutor | None = None",
            "",
            "",
            "# Lazy load for speed, singleton so dev-server reloading doesn't recreate the executor",
            "def get_pdf_conversion_executor() -> ProcessPoolExecutor:",
            "    global _pdf_conversion_executor",
            "    if _pdf_conversion_executor is None:",
            "        _pdf_conversion_executor = ProcessPoolExecutor(max_workers=1)",
            "    return _pdf_conversion_executor",
            "",
            "        pdf_conversion_executor,",
            "        get_pdf_conversion_executor(),",
            "        _convert_pdf_to_images_sync,",
            "    return result",
            "",
            "",
            "def _shutdown_pdf_conversion_executor():",
            "    \"\"\"Shutdown the PDF conversion executor process.\"\"\"",
            "    global _pdf_conversion_executor",
            "    if _pdf_conversion_executor is not None:",
            "        _pdf_conversion_executor.shutdown(wait=True)",
            "        _pdf_conversion_executor = None",
            "",
            "",
            "# Register shutdown function to ensure clean executor termination",
            "atexit.register(_shutdown_pdf_conversion_executor)"
          ]
        },
        "vulnerable_code": {
          "context": "libs/core/kiln_ai/utils/pdf_utils.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "# some of these operations are blocking, so we should run them outside of the event loop",
            "pdf_conversion_executor = ProcessPoolExecutor(max_workers=1)",
            "_pdf_conversion_executor: ProcessPoolExecutor | None = None",
            "    result = await loop.run_in_executor(",
            "        pdf_conversion_executor,",
            "        get_pdf_conversion_executor(),"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/Kiln-AI/Kiln/commit/581b22c4fd19deff3ba31f618d5bec0f0e7968a5",
        "CWE": "CWE-543",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/Kiln-AI/Kiln/pull/798"
      },
      {
        "benign_code": {
          "context": "packages/nvidia_nat_adk/src/nat/plugins/adk/adk_callback_handler.py",
          "class": null,
          "func": null,
          "lines": [
            "        super().__init__()",
            "    _instance: \"ADKProfilerHandler | None\" = None",
            "",
            "    def __new__(cls):",
            "        if cls._instance is None:",
            "            cls._instance = super().__new__(cls)",
            "",
            "        return cls._instance",
            "",
            "    def __init__(self):",
            "        self._lock = threading.Lock()",
            "        self.last_call_ts = time.time()",
            "        self.last_call_ts = 0.0",
            "        self.step_manager = Context.get().intermediate_step_manager",
            "        if getattr(self, \"_instrumented\", False):",
            "        if self._instrumented:",
            "            logger.debug(\"ADKProfilerHandler already instrumented; skipping.\")",
            "            return",
            "",
            "        try:",
            "            import litellm",
            "        except Exception as _e:",
            "            logger.exception(\"litellm import failed; skipping instrumentation\")",
            "            return",
            "        try:",
            "            FunctionTool.run_async = self._tool_use_monkey_patch()",
            "        self._original_tool_call = FunctionTool.run_async",
            "        self._original_llm_call = litellm.acompletion",
            "",
            "            litellm.acompletion = self._llm_call_monkey_patch()",
            "        FunctionTool.run_async = self._tool_use_monkey_patch()",
            "        litellm.acompletion = self._llm_call_monkey_patch()",
            "",
            "            if self._original_tool_call:",
            "            if self._original_tool_call is not None:",
            "                FunctionTool.run_async = self._original_tool_call",
            "            if self._original_llm_call:",
            "                self._original_tool_call = None",
            "",
            "            if self._original_llm_call is not None:",
            "                litellm.acompletion = self._original_llm_call",
            "                self._original_llm_call = None",
            "",
            "            self._instrumented = False",
            "            self.last_call_ts = 0.0",
            "            logger.debug(\"ADKProfilerHandler uninstrumented successfully.\")",
            "",
            "    def ensure_last_call_ts_initialized(self) -> float:",
            "        \"\"\" Ensure that last_call_ts is initialized to avoid issues in async calls. \"\"\"",
            "        if self.last_call_ts == 0.0:",
            "            with self._lock:",
            "                # Now that we have the lock, double-check",
            "                if self.last_call_ts == 0.0:",
            "                    self.last_call_ts = time.time()",
            "        return self.last_call_ts",
            "",
            "    def _tool_use_monkey_patch(self) -> Callable[..., Any]:",
            "            \"\"\"",
            "            self.ensure_last_call_ts_initialized()",
            "            now = time.time()",
            "            \"\"\"",
            "            self.ensure_last_call_ts_initialized()",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "packages/nvidia_nat_adk/src/nat/plugins/adk/adk_callback_handler.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    def __init__(self) -> None:",
            "        super().__init__()",
            "    _instance: \"ADKProfilerHandler | None\" = None",
            "        self._lock = threading.Lock()",
            "        self.last_call_ts = time.time()",
            "        self.last_call_ts = 0.0",
            "        \"\"\"",
            "        import litellm",
            "",
            "        if getattr(self, \"_instrumented\", False):",
            "        if self._instrumented:",
            "        # Save the originals",
            "        self._original_tool_call = getattr(FunctionTool, \"run_async\", None)",
            "        self._original_llm_call = getattr(litellm, \"acompletion\", None)",
            "",
            "        # Patch if available",
            "        if self._original_tool_call:",
            "            FunctionTool.run_async = self._tool_use_monkey_patch()",
            "        self._original_tool_call = FunctionTool.run_async",
            "",
            "        if self._original_llm_call:",
            "            litellm.acompletion = self._llm_call_monkey_patch()",
            "        FunctionTool.run_async = self._tool_use_monkey_patch()",
            "            from google.adk.tools.function_tool import FunctionTool",
            "            if self._original_tool_call:",
            "            if self._original_tool_call is not None:",
            "                FunctionTool.run_async = self._original_tool_call",
            "            if self._original_llm_call:",
            "                self._original_tool_call = None"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/NVIDIA/NeMo-Agent-Toolkit/commit/23e1e20d066e5815821869f2fdb054413aeb2c63",
        "CWE": "CWE-543",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/NVIDIA/NeMo-Agent-Toolkit/pull/1175"
      },
      {
        "benign_code": {
          "context": "tronbyt_server/sync.py",
          "class": null,
          "func": null,
          "lines": [
            "import logging",
            "import os",
            "import ast",
            "import base64",
            "import redis",
            "from abc import ABC, abstractmethod",
            "from multiprocessing import Manager",
            "from multiprocessing import current_process",
            "from multiprocessing.synchronize import Event as MPEvent",
            "from multiprocessing.synchronize import Lock as MPLock",
            "from multiprocessing.managers import (",
            "    SyncManager,",
            "    DictProxy,",
            ")",
            "from typing import Any, cast",
            "class SyncManager(ABC):",
            "class AbstractSyncManager(ABC):",
            "    \"\"\"Abstract base class for synchronization managers.\"\"\"",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "    def __enter__(self) -> \"SyncManager\":",
            "    def __enter__(self) -> \"AbstractSyncManager\":",
            "        \"\"\"Enter the context manager.\"\"\"",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "                return False",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "                return False",
            "class MultiprocessingSyncManager(SyncManager):",
            "class ServerSyncManager(SyncManager):",
            "    \"\"\"A custom SyncManager that creates and vends singleton sync primitives.\"\"\"",
            "",
            "    _init_lock = Lock()",
            "    _initialized = False",
            "",
            "    _conditions: DictProxy[Any, Any]",
            "    _waiter_counts: DictProxy[Any, Any]",
            "    _lock: MPLock",
            "    _shutdown_event: MPEvent",
            "",
            "    def _lazy_init(self) -> None:",
            "        \"\"\"",
            "        Initialize the shared primitives using double-checked locking to avoid",
            "        acquiring the lock on every call.",
            "        \"\"\"",
            "        if not self._initialized:",
            "            with self._init_lock:",
            "                if not self._initialized:",
            "                    self._conditions = self.dict()",
            "                    self._waiter_counts = self.dict()",
            "                    self._lock = cast(MPLock, self.Lock())",
            "                    self._shutdown_event = cast(MPEvent, self.Event())",
            "                    self._initialized = True",
            "",
            "    def get_conditions(self) -> DictProxy[Any, Any]:",
            "        self._lazy_init()",
            "        return self._conditions",
            "",
            "    def get_waiter_counts(self) -> DictProxy[Any, Any]:",
            "        self._lazy_init()",
            "        return self._waiter_counts",
            "",
            "    def get_lock(self) -> MPLock:",
            "        self._lazy_init()",
            "        return self._lock",
            "",
            "    def get_shutdown_event(self) -> MPEvent:",
            "        self._lazy_init()",
            "        return self._shutdown_event",
            "",
            "    def notify_all_and_clear(self) -> None:",
            "        \"\"\"Notify all waiting conditions and clear them to prevent new waiters.\"\"\"",
            "        self._lazy_init()",
            "        with self._lock:",
            "            conditions = list(self._conditions.values())",
            "            # Clear conditions to prevent new waiters",
            "            self._conditions.clear()",
            "            self._waiter_counts.clear()",
            "        for condition in conditions:",
            "            with condition:",
            "                condition.notify_all()",
            "",
            "",
            "class MultiprocessingSyncManager(AbstractSyncManager):",
            "    \"\"\"A synchronization manager that uses multiprocessing primitives.\"\"\"",
            "        self.shutdown_event = manager.Event()",
            "    _manager: \"ServerSyncManager\"",
            "",
            "    def __init__(self, address: Any = None, authkey: bytes | None = None) -> None:",
            "        if address:",
            "            # Client mode: connect to the server manager.",
            "            manager = ServerSyncManager(address=address, authkey=authkey)",
            "            manager.connect()",
            "            self._manager = manager",
            "            self._is_server = False",
            "        else:",
            "            # Server mode: create the manager that hosts the singletons.",
            "            manager = ServerSyncManager()",
            "            manager.start()",
            "            self._manager = manager",
            "            self._is_server = True",
            "            self._export_connection_details()",
            "",
            "        # Get proxies to the singleton objects.",
            "        self._conditions = self._manager.get_conditions()",
            "        self._waiter_counts = self._manager.get_waiter_counts()",
            "        self._lock = self._manager.get_lock()",
            "        self._shutdown_event = self._manager.get_shutdown_event()",
            "",
            "    def _export_connection_details(self) -> None:",
            "        \"\"\"Set environment variables for client processes to connect.\"\"\"",
            "        if not self._is_server:",
            "            return",
            "",
            "        address = self.address",
            "        authkey = current_process().authkey",
            "",
            "        if address and authkey:",
            "            os.environ[\"TRONBYT_MP_MANAGER_ADDR\"] = repr(address)",
            "            os.environ[\"TRONBYT_MP_MANAGER_AUTHKEY\"] = base64.b64encode(authkey).decode(",
            "                \"ascii\"",
            "            )",
            "",
            "    @property",
            "    def address(self) -> Any:",
            "        \"\"\"Get the address of the manager process (server mode only).\"\"\"",
            "        if not self._is_server:",
            "            return None",
            "        return self._manager.address",
            "",
            "    def is_shutdown(self) -> bool:",
            "        return self._shutdown_event.is_set()",
            "",
            "            if device_id not in self._conditions:",
            "                # We need to create the Condition object via the manager so it can",
            "                # be shared across processes.",
            "                self._conditions[device_id] = self._manager.Condition()",
            "            condition = self._conditions[device_id]",
            "",
            "        return MultiprocessingWaiter(condition, self, device_id)",
            "        condition: \"ConditionType | None\" = None",
            "        condition: ConditionType | None = None",
            "        with self._lock:",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "        self._manager.shutdown()",
            "        if self._is_server:",
            "            self._shutdown_event.set()",
            "            self._manager.notify_all_and_clear()",
            "            self._manager.shutdown()",
            "",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "class RedisSyncManager(SyncManager):",
            "class RedisSyncManager(AbstractSyncManager):",
            "    \"\"\"A synchronization manager that uses Redis Pub/Sub.\"\"\"",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "_sync_manager: SyncManager | None = None",
            "_sync_manager: AbstractSyncManager | None = None",
            "_sync_manager_lock = Lock()",
            "def get_sync_manager() -> SyncManager:",
            "def get_sync_manager() -> AbstractSyncManager:",
            "    \"\"\"Get the synchronization manager for the application.\"\"\"",
            "                    _sync_manager = MultiprocessingSyncManager()",
            "                    # Check env vars for multiprocessing client mode",
            "                    manager_address_str = os.environ.get(\"TRONBYT_MP_MANAGER_ADDR\")",
            "                    manager_authkey_b64 = os.environ.get(\"TRONBYT_MP_MANAGER_AUTHKEY\")",
            "",
            "                    if manager_address_str and manager_authkey_b64:",
            "                        # Client mode for worker processes",
            "",
            "                        logger.info(\"Connecting to parent sync manager...\")",
            "                        try:",
            "                            address = ast.literal_eval(manager_address_str)",
            "                            authkey = base64.b64decode(manager_authkey_b64)",
            "                            _sync_manager = MultiprocessingSyncManager(",
            "                                address=address, authkey=authkey",
            "                            )",
            "                        except Exception as e:",
            "                            logger.error(",
            "                                f\"Failed to connect to parent sync manager: {e}\"",
            "                            )",
            "                            raise",
            "                    else:",
            "                        # Server mode for the main process",
            "                        logger.info(",
            "                            \"Using multiprocessing for synchronization (server)\"",
            "                        )",
            "                        _sync_manager = MultiprocessingSyncManager()",
            "    assert _sync_manager is not None"
          ]
        },
        "vulnerable_code": {
          "context": "tronbyt_server/sync.py",
          "class": null,
          "func": null,
          "lines": [
            "from abc import ABC, abstractmethod",
            "from multiprocessing import Manager",
            "from multiprocessing import current_process",
            "from typing import Any, cast",
            "",
            "import redis",
            "from threading import Lock",
            "",
            "class SyncManager(ABC):",
            "class AbstractSyncManager(ABC):",
            "    @abstractmethod",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "    @abstractmethod",
            "    def __enter__(self) -> \"SyncManager\":",
            "    def __enter__(self) -> \"AbstractSyncManager\":",
            "        with self._condition:",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "            notified = self._condition.wait(timeout=timeout)",
            "            if self._manager.shutdown_event.is_set():",
            "            if self._manager.is_shutdown():",
            "",
            "class MultiprocessingSyncManager(SyncManager):",
            "class ServerSyncManager(SyncManager):",
            "",
            "    def __init__(self) -> None:",
            "        manager = Manager()",
            "        self._conditions: dict[str, \"ConditionType\"] = cast(",
            "            dict[str, \"ConditionType\"], manager.dict()",
            "        )",
            "        self._waiter_counts: dict[str, int] = cast(dict[str, int], manager.dict())",
            "        self._lock = manager.Lock()",
            "        self._manager = manager",
            "        self.shutdown_event = manager.Event()",
            "    _manager: \"ServerSyncManager\"",
            "        \"\"\"Notify waiters for a given device ID.\"\"\"",
            "        condition: \"ConditionType | None\" = None",
            "        condition: ConditionType | None = None",
            "",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "        \"\"\"Shut down the sync manager.\"\"\"",
            "        self.shutdown_event.set()",
            "        with self._lock:",
            "            for condition in self._conditions.values():",
            "                with condition:",
            "                    condition.notify_all()",
            "        self._manager.shutdown()",
            "        if self._is_server:",
            "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "class RedisSyncManager(SyncManager):",
            "class RedisSyncManager(AbstractSyncManager):",
            "",
            "    def shutdown(self) -> None:",
            "    def _shutdown(self) -> None:",
            "    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:",
            "        self.shutdown()",
            "        self._shutdown()",
            "",
            "_sync_manager: SyncManager | None = None",
            "_sync_manager: AbstractSyncManager | None = None",
            "",
            "def get_sync_manager() -> SyncManager:",
            "def get_sync_manager() -> AbstractSyncManager:",
            "                else:",
            "                    logger.info(\"Using multiprocessing for synchronization\")",
            "                    _sync_manager = MultiprocessingSyncManager()",
            "                    # Check env vars for multiprocessing client mode"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/tronbyt/server/commit/a9182d00866f8b6776c99bd640049ac954f8c541",
        "CWE": "CWE-543",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/tronbyt/server/pull/467"
      },
      {
        "benign_code": {
          "context": "tmt/steps/provision/mrack.py",
          "class": null,
          "func": null,
          "lines": [
            "import re",
            "import threading",
            "from contextlib import suppress",
            "_MRACK_IMPORTED: bool = False",
            "_MRACK_CONTEXT_INITIALIZED: bool = False",
            "",
            "def import_and_load_mrack_deps(workdir: Any, name: str, logger: tmt.log.Logger) -> None:",
            "# Thread-safe lock for mrack imports and initialization",
            "_MRACK_IMPORT_LOCK = threading.Lock()",
            "_MRACK_CONTEXT_LOCK = threading.Lock()",
            "_MRACK_CONTEXT_CONFIG_PATH: Optional[str] = None",
            "",
            "",
            "def import_and_load_mrack_deps(mrack_log: str, logger: tmt.log.Logger) -> None:",
            "    \"\"\"",
            "    Import mrack module only when needed",
            "    Import mrack module only when needed (thread-safe)",
            "    \"\"\"",
            "    global TmtBeakerTransformer",
            "    # Use lock to ensure thread-safe initialization",
            "    with _MRACK_IMPORT_LOCK:",
            "        if _MRACK_IMPORTED:",
            "            return",
            "",
            "        from mrack.transformers.beaker import BeakerTransformer",
            "        global MRACK_VERSION",
            "        global mrack",
            "        global providers",
            "        global ProvisioningError",
            "        global NotAuthenticatedError",
            "        global BEAKER",
            "        global BeakerProvider",
            "        global BeakerTransformer",
            "        global TmtBeakerTransformer",
            "",
            "        MRACK_VERSION = importlib.metadata.version('mrack')",
            "        try:",
            "            import mrack",
            "            from mrack.errors import NotAuthenticatedError, ProvisioningError",
            "            from mrack.providers import providers",
            "            from mrack.providers.beaker import PROVISIONER_KEY as BEAKER",
            "            from mrack.providers.beaker import BeakerProvider",
            "            from mrack.transformers.beaker import BeakerTransformer",
            "",
            "        mrack.logger.removeHandler(mrack.file_handler)",
            "            MRACK_VERSION = importlib.metadata.version('mrack')",
            "",
            "            os.remove(\"mrack.log\")",
            "            # hack: remove mrack stdout and move the logfile to /tmp",
            "            if hasattr(mrack, 'console_handler'):",
            "                mrack.logger.removeHandler(mrack.console_handler)",
            "            if hasattr(mrack, 'file_handler'):",
            "                mrack.logger.removeHandler(mrack.file_handler)",
            "",
            "        logging.FileHandler(str(f\"{workdir}/{name}-mrack.log\"))",
            "            with suppress(OSError):",
            "                if Path(\"mrack.log\").exists():",
            "                    Path(\"mrack.log\").unlink()",
            "            mrack.logger.addHandler(logging.FileHandler(mrack_log))",
            "",
            "        providers.register(BEAKER, BeakerProvider)",
            "            providers.register(BEAKER, BeakerProvider)",
            "",
            "        raise ProvisionError(\"Install 'tmt+provision-beaker' to provision using this method.\")",
            "        except ImportError:",
            "            raise ProvisionError(\"Install 'tmt+provision-beaker' to provision using this method.\")",
            "",
            "",
            "def init_mrack_global_context(config_path: str) -> None:",
            "    \"\"\"",
            "    Initialize mrack global context in a thread-safe manner",
            "    \"\"\"",
            "    global _MRACK_CONTEXT_INITIALIZED, _MRACK_CONTEXT_CONFIG_PATH",
            "",
            "    with _MRACK_CONTEXT_LOCK:",
            "        # If already initialized with the same config, return",
            "        if _MRACK_CONTEXT_INITIALIZED and config_path == _MRACK_CONTEXT_CONFIG_PATH:",
            "            return",
            "",
            "        # If initialized with different config, reinitialize",
            "        global_context = mrack.context.global_context",
            "        global_context.init(config_path)",
            "",
            "        _MRACK_CONTEXT_INITIALIZED = True",
            "        _MRACK_CONTEXT_CONFIG_PATH = config_path",
            "",
            "",
            "def async_run(func: Any) -> Any:",
            "            global_context.init(str(mrack_config))",
            "            init_mrack_global_context(str(mrack_config))",
            "        except mrack.errors.ConfigError as mrack_conf_err:",
            "            await self._mrack_transformer.init(global_context.PROV_CONFIG, {})",
            "",
            "        except NotAuthenticatedError as kinit_err:",
            "            ) from missing_conf_err",
            "        except Exception as e:",
            "            raise ProvisionError(\"Failed to initialize mrack transformer.\") from e",
            "",
            "        import_and_load_mrack_deps(self.parent.workdir, self.parent.name, self._logger)",
            "        self.mrack_log = f\"{self.parent.workdir}/{self.parent.name}-mrack.log\"",
            "        import_and_load_mrack_deps(self.mrack_log, self._logger)",
            "",
            "    # _thread_safe = True",
            "    _thread_safe = True",
            "",
            "",
            "    @property",
            "    def _preserved_workdir_members(self) -> set[str]:",
            "        \"\"\"",
            "        A set of members of the step workdir that should not be removed.",
            "        \"\"\"",
            "        assert self._guest",
            "",
            "        # Extract just the filename from the full path since",
            "        # prune_directory compares against member.name",
            "        mrack_log_filename = Path(self._guest.mrack_log).name",
            "        return {*super()._preserved_workdir_members, mrack_log_filename}",
            "",
            "    def go(self, *, logger: Optional[tmt.log.Logger] = None) -> None:"
          ]
        },
        "vulnerable_code": {
          "context": "tmt/steps/provision/mrack.py",
          "class": null,
          "func": null,
          "lines": [
            "import logging",
            "import os",
            "import re",
            "",
            "def import_and_load_mrack_deps(workdir: Any, name: str, logger: tmt.log.Logger) -> None:",
            "# Thread-safe lock for mrack imports and initialization",
            "    \"\"\"",
            "    Import mrack module only when needed",
            "    Import mrack module only when needed (thread-safe)",
            "",
            "    if _MRACK_IMPORTED:",
            "        return",
            "",
            "    global MRACK_VERSION",
            "    global mrack",
            "    global providers",
            "    global ProvisioningError",
            "    global NotAuthenticatedError",
            "    global BEAKER",
            "    global BeakerProvider",
            "    global BeakerTransformer",
            "    global TmtBeakerTransformer",
            "    # Use lock to ensure thread-safe initialization",
            "",
            "    try:",
            "        import mrack",
            "        from mrack.errors import NotAuthenticatedError, ProvisioningError",
            "        from mrack.providers import providers",
            "        from mrack.providers.beaker import PROVISIONER_KEY as BEAKER",
            "        from mrack.providers.beaker import BeakerProvider",
            "        from mrack.transformers.beaker import BeakerTransformer",
            "        global MRACK_VERSION",
            "",
            "        MRACK_VERSION = importlib.metadata.version('mrack')",
            "        try:",
            "",
            "        # hack: remove mrack stdout and move the logfile to /tmp",
            "        mrack.logger.removeHandler(mrack.console_handler)",
            "        mrack.logger.removeHandler(mrack.file_handler)",
            "            MRACK_VERSION = importlib.metadata.version('mrack')",
            "",
            "        with suppress(OSError):",
            "            os.remove(\"mrack.log\")",
            "            # hack: remove mrack stdout and move the logfile to /tmp",
            "",
            "        logging.FileHandler(str(f\"{workdir}/{name}-mrack.log\"))",
            "            with suppress(OSError):",
            "",
            "        providers.register(BEAKER, BeakerProvider)",
            "            providers.register(BEAKER, BeakerProvider)",
            "",
            "    except ImportError:",
            "        raise ProvisionError(\"Install 'tmt+provision-beaker' to provision using this method.\")",
            "        except ImportError:",
            "        try:",
            "            global_context.init(str(mrack_config))",
            "            init_mrack_global_context(str(mrack_config))",
            "        assert isinstance(self.parent, tmt.steps.provision.Provision)",
            "        import_and_load_mrack_deps(self.parent.workdir, self.parent.name, self._logger)",
            "        self.mrack_log = f\"{self.parent.workdir}/{self.parent.name}-mrack.log\"",
            "",
            "    # _thread_safe = True",
            "    _thread_safe = True"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/teemtee/tmt/commit/3ceb761054bd85f478bf4157093ac9e8dc9faa54",
        "CWE": "CWE-543",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/teemtee/tmt/pull/3941"
      },
      {
        "benign_code": {
          "context": "src/galois/_databases/_interface.py",
          "class": null,
          "func": null,
          "lines": [
            "from pathlib import Path",
            "from threading import Lock",
            "",
            "    singleton = None",
            "    _lock = Lock()",
            "    _singleton = None",
            "    file: Path",
            "        return cls.singleton",
            "        with cls._lock:",
            "            if cls._singleton is None:",
            "                cls._singleton = super().__new__(cls)",
            "                cls.conn = sqlite3.connect(cls.file, check_same_thread=False)",
            "                cls.cursor = cls.conn.cursor()",
            "        return cls._singleton",
            "",
            "        result = self.cursor.fetchone()",
            "        with self._lock:",
            "            self.cursor.execute(",
            "                \"\"\"",
            "                SELECT factors, multiplicities, composite",
            "                FROM factorizations",
            "                WHERE value=?",
            "                \"\"\",",
            "                (str(n),),",
            "            )",
            "            result = self.cursor.fetchone()",
            "",
            "        result = self.cursor.fetchone()",
            "        with self._lock:",
            "            self.cursor.execute(",
            "                \"\"\"",
            "                SELECT nonzero_degrees, nonzero_coeffs",
            "                FROM polys",
            "                WHERE characteristic=? AND degree=?\"\"\",",
            "                (characteristic, degree),",
            "            )",
            "            result = self.cursor.fetchone()",
            "",
            "        result = self.cursor.fetchone()",
            "        with self._lock:",
            "            self.cursor.execute(",
            "                \"\"\"",
            "                SELECT nonzero_degrees, nonzero_coeffs",
            "                FROM polys",
            "                WHERE characteristic=? AND degree=?",
            "                \"\"\",",
            "                (characteristic, degree),",
            "            )",
            "            result = self.cursor.fetchone()",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "src/galois/_databases/_interface.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    singleton = None",
            "    _lock = Lock()",
            "    def __new__(cls):",
            "        if cls.singleton is None:",
            "            cls.singleton = super().__new__(cls)",
            "            cls.conn = sqlite3.connect(cls.file)",
            "            cls.cursor = cls.conn.cursor()",
            "        return cls.singleton",
            "        with cls._lock:",
            "",
            "    singleton = None",
            "    file = Path(__file__).parent / \"prime_factors.db\"",
            "",
            "        self.cursor.execute(",
            "            \"\"\"",
            "            SELECT factors, multiplicities, composite",
            "            FROM factorizations",
            "            WHERE value=?",
            "            \"\"\",",
            "            (str(n),),",
            "        )",
            "        result = self.cursor.fetchone()",
            "        with self._lock:",
            "",
            "    singleton = None",
            "    file = Path(__file__).parent / \"irreducible_polys.db\"",
            "        \"\"\"",
            "        self.cursor.execute(",
            "            \"\"\"",
            "            SELECT nonzero_degrees, nonzero_coeffs",
            "            FROM polys",
            "            WHERE characteristic=? AND degree=?\"\"\",",
            "            (characteristic, degree),",
            "        )",
            "        result = self.cursor.fetchone()",
            "        with self._lock:",
            "",
            "    singleton = None",
            "    file = Path(__file__).parent / \"conway_polys.db\"",
            "        \"\"\"",
            "        self.cursor.execute(",
            "            \"\"\"",
            "            SELECT nonzero_degrees, nonzero_coeffs",
            "            FROM polys",
            "            WHERE characteristic=? AND degree=?",
            "            \"\"\",",
            "            (characteristic, degree),",
            "        )",
            "        result = self.cursor.fetchone()",
            "        with self._lock:"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/mhostetter/galois/commit/5bf5c79e9bfcc020b459c74073398f16cb028e8f",
        "CWE": "CWE-543",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/mhostetter/galois/pull/558"
      },
      {
        "benign_code": {
          "context": "dbt/adapters/athena/session.py",
          "class": null,
          "func": null,
          "lines": [
            "    return __BOTO3_SESSION__",
            "def get_boto3_session(connection: Connection) -> boto3.session.Session:",
            "    return boto3.session.Session(",
            "        region_name=connection.credentials.region_name,",
            "        profile_name=connection.credentials.aws_profile_name,",
            "    )"
          ]
        },
        "vulnerable_code": {
          "context": "dbt/adapters/athena/session.py",
          "class": null,
          "func": null,
          "lines": [
            "from typing import Optional",
            "",
            "import boto3.session",
            "",
            "__BOTO3_SESSION__: Optional[boto3.session.Session] = None",
            "",
            "",
            "def get_boto3_session(connection: Connection = None) -> boto3.session.Session:",
            "    def init_session():",
            "        global __BOTO3_SESSION__",
            "        __BOTO3_SESSION__ = boto3.session.Session(",
            "            region_name=connection.credentials.region_name,",
            "            profile_name=connection.credentials.aws_profile_name,",
            "        )",
            "",
            "    if not __BOTO3_SESSION__:",
            "        if connection is None:",
            "            raise RuntimeError(",
            "                \"A Connection object needs to be passed to initialize the boto3 session for the first time\"",
            "            )",
            "        init_session()",
            "",
            "    return __BOTO3_SESSION__",
            "def get_boto3_session(connection: Connection) -> boto3.session.Session:"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/dbt-labs/dbt-athena/commit/e11876d1744d84139f8db936f0329ea6f64be3d9",
        "CWE": "CWE-543",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/dbt-labs/dbt-athena/pull/45"
      }
    ],
    "CWD-1132": [
      {
        "benign_code": {
          "context": "superset/commands/report/execute.py",
          "class": null,
          "func": null,
          "lines": [
            "        db.session.commit()  # pylint: disable=consider-using-transaction",
            "        from sqlalchemy.orm.exc import StaleDataError",
            "",
            "        try:",
            "            log = ReportExecutionLog(",
            "                scheduled_dttm=self._scheduled_dttm,",
            "                start_dttm=self._start_dttm,",
            "                end_dttm=datetime.utcnow(),",
            "                value=self._report_schedule.last_value,",
            "                value_row_json=self._report_schedule.last_value_row_json,",
            "                state=self._report_schedule.last_state,",
            "                error_message=error_message,",
            "                report_schedule=self._report_schedule,",
            "                uuid=self._execution_id,",
            "            )",
            "            db.session.add(log)",
            "            db.session.commit()  # pylint: disable=consider-using-transaction",
            "        except StaleDataError as ex:",
            "            # Report schedule was modified or deleted by another process",
            "            db.session.rollback()",
            "            logger.warning(",
            "                \"Report schedule (execution %s) was modified or deleted \"",
            "                \"during execution. This can occur when a report is deleted \"",
            "                \"while running.\",",
            "                self._execution_id,",
            "            )",
            "            raise ReportScheduleUnexpectedError(",
            "                \"Report schedule was modified or deleted by another process \"",
            "                \"during execution\"",
            "            ) from ex",
            "",
            "    def next(self) -> None:",
            "    def next(self) -> None:  # noqa: C901",
            "        self.update_report_schedule_and_log(ReportState.WORKING)",
            "            )",
            "            try:",
            "                self.update_report_schedule_and_log(",
            "                    ReportState.ERROR, error_message=error_message",
            "                )",
            "            except ReportScheduleUnexpectedError as logging_ex:",
            "                # Logging failed (likely StaleDataError), but we still want to",
            "                # raise the original error so the root cause remains visible",
            "                logger.warning(",
            "                    \"Failed to log error for report schedule (execution %s) \"",
            "                    \"due to database issue\",",
            "                    self._execution_id,",
            "                    exc_info=True,",
            "                )",
            "                # Re-raise the original exception, not the logging failure",
            "                raise first_ex from logging_ex",
            "",
            "                    )",
            "                except ReportScheduleUnexpectedError:",
            "                    # send_error failed due to logging issue, log and continue",
            "                    # to raise the original error",
            "                    logger.warning(",
            "                        \"Failed to send error notification due to database issue\",",
            "                        exc_info=True,",
            "                    )",
            "                except Exception as second_ex:  # pylint: disable=broad-except",
            "                    )",
            "                    try:",
            "                        self.update_report_schedule_and_log(",
            "                            ReportState.ERROR, error_message=second_error_message",
            "                        )",
            "                    except ReportScheduleUnexpectedError:",
            "                        # Logging failed again, log it but don't let it hide first_ex",
            "                        logger.warning(",
            "                            \"Failed to log final error state due to database issue\",",
            "                            exc_info=True,",
            "                        )",
            "            raise",
            "            )",
            "            try:",
            "                self.update_report_schedule_and_log(",
            "                    ReportState.ERROR, error_message=str(ex)",
            "                )",
            "            except ReportScheduleUnexpectedError as logging_ex:",
            "                # Logging failed (likely StaleDataError), but we still want to",
            "                # raise the original error so the root cause remains visible",
            "                logger.warning(",
            "                    \"Failed to log error for report schedule (execution %s) \"",
            "                    \"due to database issue\",",
            "                    self._execution_id,",
            "                    exc_info=True,",
            "                )",
            "                # Re-raise the original exception, not the logging failure",
            "                raise ex from logging_ex",
            "            raise",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "superset/commands/report/execute.py",
          "class": null,
          "func": null,
          "lines": [
            "        \"\"\"",
            "        log = ReportExecutionLog(",
            "            scheduled_dttm=self._scheduled_dttm,",
            "            start_dttm=self._start_dttm,",
            "            end_dttm=datetime.utcnow(),",
            "            value=self._report_schedule.last_value,",
            "            value_row_json=self._report_schedule.last_value_row_json,",
            "            state=self._report_schedule.last_state,",
            "            error_message=error_message,",
            "            report_schedule=self._report_schedule,",
            "            uuid=self._execution_id,",
            "        )",
            "        db.session.add(log)",
            "        db.session.commit()  # pylint: disable=consider-using-transaction",
            "        from sqlalchemy.orm.exc import StaleDataError",
            "",
            "    def next(self) -> None:",
            "    def next(self) -> None:  # noqa: C901",
            "",
            "            self.update_report_schedule_and_log(",
            "                ReportState.ERROR, error_message=error_message",
            "            )",
            "            try:",
            "                finally:",
            "                    self.update_report_schedule_and_log(",
            "                        ReportState.ERROR, error_message=second_error_message",
            "                    )",
            "                    try:",
            "        except Exception as ex:  # pylint: disable=broad-except",
            "            self.update_report_schedule_and_log(",
            "                ReportState.ERROR, error_message=str(ex)",
            "            )",
            "            try:"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/apache/superset/commit/c42e3c68373f9a90b2b9f8e7f3341bf13e2fe37a",
        "CWE": "CWE-567",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/apache/superset/pull/35800"
      }
    ],
    "CWD-1133": [
      {
        "benign_code": {
          "context": "packages/nvidia_nat_adk/src/nat/plugins/adk/adk_callback_handler.py",
          "class": null,
          "func": null,
          "lines": [
            "        super().__init__()",
            "    _instance: \"ADKProfilerHandler | None\" = None",
            "",
            "    def __new__(cls):",
            "        if cls._instance is None:",
            "            cls._instance = super().__new__(cls)",
            "",
            "        return cls._instance",
            "",
            "    def __init__(self):",
            "        self._lock = threading.Lock()",
            "        self.last_call_ts = time.time()",
            "        self.last_call_ts = 0.0",
            "        self.step_manager = Context.get().intermediate_step_manager",
            "        if getattr(self, \"_instrumented\", False):",
            "        if self._instrumented:",
            "            logger.debug(\"ADKProfilerHandler already instrumented; skipping.\")",
            "            return",
            "",
            "        try:",
            "            import litellm",
            "        except Exception as _e:",
            "            logger.exception(\"litellm import failed; skipping instrumentation\")",
            "            return",
            "        try:",
            "            FunctionTool.run_async = self._tool_use_monkey_patch()",
            "        self._original_tool_call = FunctionTool.run_async",
            "        self._original_llm_call = litellm.acompletion",
            "",
            "            litellm.acompletion = self._llm_call_monkey_patch()",
            "        FunctionTool.run_async = self._tool_use_monkey_patch()",
            "        litellm.acompletion = self._llm_call_monkey_patch()",
            "",
            "            if self._original_tool_call:",
            "            if self._original_tool_call is not None:",
            "                FunctionTool.run_async = self._original_tool_call",
            "            if self._original_llm_call:",
            "                self._original_tool_call = None",
            "",
            "            if self._original_llm_call is not None:",
            "                litellm.acompletion = self._original_llm_call",
            "                self._original_llm_call = None",
            "",
            "            self._instrumented = False",
            "            self.last_call_ts = 0.0",
            "            logger.debug(\"ADKProfilerHandler uninstrumented successfully.\")",
            "",
            "    def ensure_last_call_ts_initialized(self) -> float:",
            "        \"\"\" Ensure that last_call_ts is initialized to avoid issues in async calls. \"\"\"",
            "        if self.last_call_ts == 0.0:",
            "            with self._lock:",
            "                # Now that we have the lock, double-check",
            "                if self.last_call_ts == 0.0:",
            "                    self.last_call_ts = time.time()",
            "        return self.last_call_ts",
            "",
            "    def _tool_use_monkey_patch(self) -> Callable[..., Any]:",
            "            \"\"\"",
            "            self.ensure_last_call_ts_initialized()",
            "            now = time.time()",
            "            \"\"\"",
            "            self.ensure_last_call_ts_initialized()",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "packages/nvidia_nat_adk/src/nat/plugins/adk/adk_callback_handler.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    def __init__(self) -> None:",
            "        super().__init__()",
            "    _instance: \"ADKProfilerHandler | None\" = None",
            "        self._lock = threading.Lock()",
            "        self.last_call_ts = time.time()",
            "        self.last_call_ts = 0.0",
            "        \"\"\"",
            "        import litellm",
            "",
            "        if getattr(self, \"_instrumented\", False):",
            "        if self._instrumented:",
            "        # Save the originals",
            "        self._original_tool_call = getattr(FunctionTool, \"run_async\", None)",
            "        self._original_llm_call = getattr(litellm, \"acompletion\", None)",
            "",
            "        # Patch if available",
            "        if self._original_tool_call:",
            "            FunctionTool.run_async = self._tool_use_monkey_patch()",
            "        self._original_tool_call = FunctionTool.run_async",
            "",
            "        if self._original_llm_call:",
            "            litellm.acompletion = self._llm_call_monkey_patch()",
            "        FunctionTool.run_async = self._tool_use_monkey_patch()",
            "            from google.adk.tools.function_tool import FunctionTool",
            "            if self._original_tool_call:",
            "            if self._original_tool_call is not None:",
            "                FunctionTool.run_async = self._original_tool_call",
            "            if self._original_llm_call:",
            "                self._original_tool_call = None"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/NVIDIA/NeMo-Agent-Toolkit/commit/23e1e20d066e5815821869f2fdb054413aeb2c63",
        "CWE": "CWE-1096",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/NVIDIA/NeMo-Agent-Toolkit/pull/1175"
      },
      {
        "benign_code": {
          "context": "lmcache/integration/vllm/utils.py",
          "class": null,
          "func": null,
          "lines": [
            "import os",
            "import threading",
            "",
            "",
            "# Thread-safe singleton storage",
            "_config_instance: Union[Config, V1Config, None] = None",
            "_config_lock = threading.Lock()",
            "",
            "",
            "def lmcache_get_config() -> Union[Config, V1Config]:",
            "def lmcache_get_or_create_config() -> Union[Config, V1Config]:",
            "    \"\"\"Get the LMCache configuration from the environment variable",
            "    return config",
            "    This function is thread-safe and implements singleton pattern,",
            "    ensuring the configuration is loaded only once.",
            "    \"\"\"",
            "    global _config_instance",
            "",
            "    # Double-checked locking for thread-safe singleton",
            "    if _config_instance is None:",
            "        with _config_lock:",
            "            if _config_instance is None:  # Check again within lock",
            "                if is_false(os.getenv(\"LMCACHE_USE_EXPERIMENTAL\", \"True\")):",
            "                    logger.warning(",
            "                        \"Detected LMCACHE_USE_EXPERIMENTAL is set to False. \"",
            "                        \"Using legacy configuration is deprecated and will \"",
            "                        \"be remove soon! Please set LMCACHE_USE_EXPERIMENTAL \"",
            "                        \"to True.\"",
            "                    )",
            "                    LMCacheEngineConfig = Config  # type: ignore[assignment]",
            "                else:",
            "                    LMCacheEngineConfig = V1Config  # type: ignore[assignment]",
            "",
            "                if \"LMCACHE_CONFIG_FILE\" not in os.environ:",
            "                    logger.warning(",
            "                        \"No LMCache configuration file is set. Trying to read\"",
            "                        \" configurations from the environment variables.\"",
            "                    )",
            "                    logger.warning(",
            "                        \"You can set the configuration file through \"",
            "                        \"the environment variable: LMCACHE_CONFIG_FILE\"",
            "                    )",
            "                    _config_instance = LMCacheEngineConfig.from_env()",
            "                else:",
            "                    config_file = os.environ[\"LMCACHE_CONFIG_FILE\"]",
            "                    logger.info(f\"Loading LMCache config file {config_file}\")",
            "                    _config_instance = LMCacheEngineConfig.from_file(config_file)",
            "                    # Update config from environment variables",
            "                    _config_instance.update_config_from_env()",
            "    return _config_instance",
            "",
            "    config = lmcache_get_config()",
            "    config = lmcache_get_or_create_config()",
            "    # Support both vllm_config object and individual config parameters"
          ]
        },
        "vulnerable_code": {
          "context": "lmcache/integration/vllm/utils.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "def lmcache_get_config() -> Union[Config, V1Config]:",
            "def lmcache_get_or_create_config() -> Union[Config, V1Config]:",
            "    function will return the default configuration.",
            "    \"\"\"",
            "",
            "    if is_false(os.getenv(\"LMCACHE_USE_EXPERIMENTAL\", \"True\")):",
            "        logger.warning(",
            "            \"Detected LMCACHE_USE_EXPERIMENTAL is set to False. \"",
            "            \"Using legacy configuration is deprecated and will \"",
            "            \"be remove soon! Please set LMCACHE_USE_EXPERIMENTAL \"",
            "            \"to True.\"",
            "        )",
            "        LMCacheEngineConfig = Config  # type: ignore[assignment]",
            "    else:",
            "        LMCacheEngineConfig = V1Config  # type: ignore[assignment]",
            "",
            "    if \"LMCACHE_CONFIG_FILE\" not in os.environ:",
            "        logger.warn(",
            "            \"No LMCache configuration file is set. Trying to read\"",
            "            \" configurations from the environment variables.\"",
            "        )",
            "        logger.warn(",
            "            \"You can set the configuration file through \"",
            "            \"the environment variable: LMCACHE_CONFIG_FILE\"",
            "        )",
            "        config = LMCacheEngineConfig.from_env()",
            "    else:",
            "        config_file = os.environ[\"LMCACHE_CONFIG_FILE\"]",
            "        logger.info(f\"Loading LMCache config file {config_file}\")",
            "        config = LMCacheEngineConfig.from_file(config_file)",
            "        # Update config from environment variables",
            "        config.update_config_from_env()",
            "    return config",
            "    This function is thread-safe and implements singleton pattern,",
            "",
            "    config = lmcache_get_config()",
            "    config = lmcache_get_or_create_config()"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/LMCache/LMCache/commit/269c36b79d6b3a09b9b1bcf562880bce10f77ed5",
        "CWE": "CWE-1096",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/LMCache/LMCache/pull/1586"
      },
      {
        "benign_code": {
          "context": "python/ray/_private/telemetry/open_telemetry_metric_recorder.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    _metrics_initialized = False",
            "    _metrics_initialized_lock = threading.Lock()",
            "",
            "    def __init__(self):",
            "        metrics.set_meter_provider(provider)",
            "        self._init_metrics()",
            "        self.meter = metrics.get_meter(__name__)",
            "",
            "    def _init_metrics(self):",
            "        # Initialize the global metrics provider and meter. We only do this once on",
            "        # the first initialization of the class, because re-setting the meter provider",
            "        # can result in loss of metrics.",
            "        with self._metrics_initialized_lock:",
            "            if self._metrics_initialized:",
            "                return",
            "            prometheus_reader = PrometheusMetricReader()",
            "            provider = MeterProvider(metric_readers=[prometheus_reader])",
            "            metrics.set_meter_provider(provider)",
            "            self._metrics_initialized = True",
            "",
            "    def register_gauge_metric(self, name: str, description: str) -> None:"
          ]
        },
        "vulnerable_code": {
          "context": "python/ray/_private/telemetry/open_telemetry_metric_recorder.py",
          "class": null,
          "func": null,
          "lines": [
            "        self._histogram_bucket_midpoints = defaultdict(list)",
            "",
            "        prometheus_reader = PrometheusMetricReader()",
            "        provider = MeterProvider(metric_readers=[prometheus_reader])",
            "        metrics.set_meter_provider(provider)",
            "        self._init_metrics()"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/ray-project/ray/commit/f8d32c35b03fbfe7f425fbbdc4534f0bda45483b",
        "CWE": "CWE-1096",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/ray-project/ray/pull/56347"
      },
      {
        "benign_code": {
          "context": "backend/apps/nest/clients/kms.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "from threading import Lock",
            "",
            "import boto3",
            "",
            "    _lock = Lock()",
            "",
            "    def __new__(cls):",
            "        \"\"\"Create a new instance of KmsClient.\"\"\"",
            "        if not hasattr(cls, \"instance\"):",
            "            with cls._lock:",
            "                if not hasattr(cls, \"instance\"):",
            "                    cls.instance = super().__new__(cls)",
            "        return cls.instance",
            "",
            "    def __init__(self):",
            "        \"\"\"Initialize the KMS client.\"\"\"",
            "        if getattr(self, \"client\", None) is not None:",
            "            return",
            "        self.client = boto3.client("
          ]
        },
        "vulnerable_code": {
          "context": "backend/apps/nest/clients/kms.py",
          "class": null,
          "func": null,
          "lines": [
            "        )[\"CiphertextBlob\"]",
            "",
            "",
            "kms_client = KmsClient()"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/OWASP/Nest/commit/210e207bb467c8520a91a8768485b3a4dd1108c3",
        "CWE": "CWE-1096",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/OWASP/Nest/pull/2158"
      }
    ],
    "CWD-1135": [
      {
        "benign_code": {
          "context": "tests/integration/responses/fixtures/fixtures.py",
          "class": null,
          "func": null,
          "lines": [
            "    return OpenAI(",
            "    client = OpenAI(",
            "        base_url=base_url,",
            "        api_key=api_key,",
            "        max_retries=0,",
            "        timeout=30.0,",
            "    )",
            "    yield client",
            "    # Cleanup: close HTTP connections",
            "    try:",
            "        client.close()",
            "    except Exception:",
            "        pass"
          ]
        },
        "vulnerable_code": {
          "context": "tests/integration/responses/fixtures/fixtures.py",
          "class": null,
          "func": null,
          "lines": [
            "",
            "    return OpenAI(",
            "    client = OpenAI("
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/llamastack/llama-stack/commit/37853ca5581a832ef7db9a130b2064ae705bcce3",
        "CWE": "CWE-1088",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/llamastack/llama-stack/pull/4119"
      }
    ],
    "CWD-1137": [
      {
        "benign_code": {
          "context": "tidal_dl_ng/download.py",
          "class": null,
          "func": null,
          "lines": [
            "from concurrent import futures",
            "from dataclasses import dataclass",
            "from threading import Event",
            "",
            "@dataclass",
            "class TrackStreamInfo:",
            "    \"\"\"Container for track stream information.\"\"\"",
            "",
            "    stream_manifest: StreamManifest | None",
            "    file_extension: str",
            "    requires_flac_extraction: bool",
            "    media_stream: Stream | None",
            "",
            "",
            "# TODO: Set appropriate client string and use it for video download.",
            "                - atmos_session_context(): Dolby Atmos credential switching",
            "                - switch_to_atmos_session(): Dolby Atmos credential switching",
            "                - restore_normal_session(): Restore original session credentials",
            "            path_base (str): Base path for downloads.",
            "            tuple[StreamManifest | None, str, bool, Stream | None]: Stream info.",
            "        tuple[StreamManifest | None, str, bool, Stream | None]: Stream info.",
            "        \"\"\"",
            "        if isinstance(media, Track):",
            "        file_extension: str = \"\"",
            "",
            "        # CRITICAL: This lock is intentionally broad and serializes all",
            "        # stream-fetching (Phase 1) to prevent a critical race condition.",
            "        #",
            "        # THE PROBLEM:",
            "        # The single, shared session (self.tidal.session) must change its",
            "        # credentials to switch between Atmos and Hi-Res/Normal streams.",
            "        #",
            "        # THE RACE CONDITION IT FIXES:",
            "        # If this lock is released *before* get_stream() is called,",
            "        # another thread could change the session (e.g., back to \"Normal\")",
            "        # right after this thread switched it to \"Atmos\". This would",
            "        # cause this thread to call get_stream() with the wrong credentials,",
            "        # resulting in the API returning AAC 320 instead of Atmos.",
            "        #",
            "        # THE TRADEOFF:",
            "        # This creates a \"tollbooth\" bottleneck, serializing the get_stream()",
            "        # calls. However, the *actual* segment downloads (Phase 2)",
            "        # still run in parallel, governed by `downloads_concurrent_max`.",
            "        #",
            "        # DO NOT \"OPTIMIZE\" THIS by making the lock more granular.",
            "        # Correctness > Performance.",
            "",
            "        with self.tidal.stream_lock:",
            "            try:",
            "                    media_stream = media.get_stream()",
            "                if isinstance(media, Track):",
            "                    track_info = self._get_track_stream_info(media)",
            "",
            "                    if track_info.stream_manifest is None:",
            "                        return None, \"\", False, None",
            "",
            "                stream_manifest = media_stream.get_stream_manifest()",
            "                    stream_manifest = track_info.stream_manifest",
            "                    file_extension = track_info.file_extension",
            "                    do_flac_extract = track_info.requires_flac_extraction",
            "                    media_stream = track_info.media_stream",
            "",
            "                elif isinstance(media, Video):",
            "                    # Videos always require the normal session",
            "                    if not self.tidal.restore_normal_session():",
            "                        self.fn_logger.error(f\"Failed to restore normal session for video: {media.id}\")",
            "                        return None, \"\", False, None",
            "",
            "                    file_extension = AudioExtensions.MP4 if self.settings.data.video_convert_mp4 else VideoExtensions.TS",
            "",
            "                    stream_manifest = None",
            "                    media_stream = None",
            "                    do_flac_extract = False",
            "",
            "                else:",
            "                    self.fn_logger.error(f\"Unknown media type for stream info: {type(media)}\")",
            "                    return None, \"\", False, None",
            "",
            "                return None, \"\", False, None",
            "",
            "            except Exception:",
            "            file_extension = stream_manifest.file_extension",
            "        return stream_manifest, file_extension, do_flac_extract, media_stream",
            "",
            "            file_extension = AudioExtensions.MP4 if self.settings.data.video_convert_mp4 else VideoExtensions.TS",
            "    def _get_track_stream_info(self, media: Track) -> TrackStreamInfo:",
            "        \"\"\"",
            "        Gets stream info for a Track, handling Atmos/Normal session switching.",
            "",
            "        return stream_manifest, file_extension, do_flac_extract, media_stream",
            "        Args:",
            "            media: The track to get stream information for.",
            "",
            "        Returns:",
            "            TrackStreamInfo: Container with stream manifest, file extension,",
            "                            FLAC extraction flag, and media stream object.",
            "                            Returns TrackStreamInfo with None/empty values if fails.",
            "        \"\"\"",
            "        want_atmos = (",
            "            self.settings.data.download_dolby_atmos",
            "            and hasattr(media, \"audio_modes\")",
            "            and AudioMode.dolby_atmos.value in media.audio_modes",
            "        )",
            "",
            "        if want_atmos:",
            "            if not self.tidal.switch_to_atmos_session():",
            "                self.fn_logger.error(f\"Failed to switch to Atmos session for track: {media.id}\")",
            "                return TrackStreamInfo(None, \"\", False, None)",
            "        else:",
            "            if not self.tidal.restore_normal_session():",
            "                self.fn_logger.error(f\"Failed to restore normal session for track: {media.id}\")",
            "                return TrackStreamInfo(None, \"\", False, None)",
            "",
            "        media_stream = self.session.track(media.id).get_stream() if want_atmos else media.get_stream()",
            "",
            "        stream_manifest = media_stream.get_stream_manifest()",
            "        file_extension = stream_manifest.file_extension",
            "        requires_flac_extraction = False",
            "",
            "        if self.settings.data.extract_flac and (",
            "            stream_manifest.codecs.upper() == Codec.FLAC and file_extension != AudioExtensions.FLAC",
            "        ):",
            "            file_extension = AudioExtensions.FLAC",
            "            requires_flac_extraction = True",
            "",
            "        return TrackStreamInfo(",
            "            stream_manifest=stream_manifest,",
            "            file_extension=file_extension,",
            "            requires_flac_extraction=requires_flac_extraction,",
            "            media_stream=media_stream,",
            "        )",
            ""
          ]
        },
        "vulnerable_code": {
          "context": "tidal_dl_ng/download.py",
          "class": null,
          "func": null,
          "lines": [
            "                - session: Main TIDAL API session",
            "                - atmos_session_context(): Dolby Atmos credential switching",
            "                - switch_to_atmos_session(): Dolby Atmos credential switching",
            "        Returns:",
            "            tuple[StreamManifest | None, str, bool, Stream | None]: Stream info.",
            "        tuple[StreamManifest | None, str, bool, Stream | None]: Stream info.",
            "        do_flac_extract: bool = False",
            "",
            "        if isinstance(media, Track):",
            "        file_extension: str = \"\"",
            "            try:",
            "                if (",
            "                    self.settings.data.download_dolby_atmos",
            "                    and hasattr(media, \"audio_modes\")",
            "                    and AudioMode.dolby_atmos.value in media.audio_modes",
            "                ):",
            "                    with self.tidal.atmos_session_context():",
            "                        atmos_track = self.session.track(media.id)",
            "                        media_stream = atmos_track.get_stream()",
            "                else:",
            "                    media_stream = media.get_stream()",
            "                if isinstance(media, Track):",
            "",
            "                stream_manifest = media_stream.get_stream_manifest()",
            "                    stream_manifest = track_info.stream_manifest",
            "                )",
            "",
            "                return None, \"\", False, None",
            "                self.fn_logger.exception(f\"Something went wrong. Skipping '{name_builder_item(media)}'.\")",
            "",
            "                return None, \"\", False, None",
            "",
            "            file_extension = stream_manifest.file_extension",
            "        return stream_manifest, file_extension, do_flac_extract, media_stream",
            "",
            "            if self.settings.data.extract_flac and (",
            "                stream_manifest.codecs.upper() == Codec.FLAC and file_extension != AudioExtensions.FLAC",
            "            ):",
            "                file_extension = AudioExtensions.FLAC",
            "                do_flac_extract = True",
            "        elif isinstance(media, Video):",
            "            file_extension = AudioExtensions.MP4 if self.settings.data.video_convert_mp4 else VideoExtensions.TS",
            "    def _get_track_stream_info(self, media: Track) -> TrackStreamInfo:",
            "",
            "        return stream_manifest, file_extension, do_flac_extract, media_stream",
            "        Args:"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/exislow/tidal-dl-ng/commit/0939bc0742cbc45f9cd2a2a21c335b238a288737",
        "CWE": "CWE-663",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/exislow/tidal-dl-ng/pull/626"
      },
      {
        "benign_code": {
          "context": "astrbot/core/pipeline/process_stage/method/llm_request.py",
          "class": null,
          "func": null,
          "lines": [
            "from astrbot.core.utils.metrics import Metric",
            "from astrbot.core.utils.session_lock import session_lock_manager",
            "",
            "    async def process(",
            "    async def _apply_kb_context(",
            "        self,",
            "        # ",
            "        req: ProviderRequest,",
            "    ):",
            "        \"\"\"\"\"\"",
            "        try:",
            "            return",
            "    def _truncate_contexts(",
            "        self,",
            "        contexts: list[dict],",
            "    ) -> list[dict]:",
            "        \"\"\"\"\"\"",
            "        if self.max_context_length == -1:",
            "            return contexts",
            "",
            "            req.contexts = json.loads(req.contexts)",
            "        if len(contexts) // 2 <= self.max_context_length:",
            "            return contexts",
            "",
            "            req.session_id = event.unified_msg_origin",
            "        truncated_contexts = contexts[",
            "            -(self.max_context_length - self.dequeue_context_length + 1) * 2 :",
            "        ]",
            "        # role  user ",
            "        index = next(",
            "            (",
            "                i",
            "                for i, item in enumerate(truncated_contexts)",
            "                if item.get(\"role\") == \"user\"",
            "            ),",
            "            None,",
            "        )",
            "        if index is not None and index > 0:",
            "            truncated_contexts = truncated_contexts[index:]",
            "",
            "        req.contexts = self.fix_messages(req.contexts)",
            "        return truncated_contexts",
            "",
            "        # //",
            "    def _modalities_fix(",
            "        self,",
            "        provider: Provider,",
            "        req: ProviderRequest,",
            "    ):",
            "        \"\"\"\"\"\"",
            "        if req.image_urls:",
            "        # ",
            "",
            "    def _plugin_tool_fix(",
            "        self,",
            "        event: AstrMessageEvent,",
            "        req: ProviderRequest,",
            "    ):",
            "        \"\"\"\"\"\"",
            "        if event.plugins_name is not None and req.func_tool:",
            "    def fix_messages(self, messages: list[dict]) -> list[dict]:",
            "    def _fix_messages(self, messages: list[dict]) -> list[dict]:",
            "        \"\"\"\"\"\"",
            "        return fixed_messages",
            "",
            "    async def process(",
            "        self,",
            "        event: AstrMessageEvent,",
            "        _nested: bool = False,",
            "    ) -> None | AsyncGenerator[None, None]:",
            "        req: ProviderRequest | None = None",
            "",
            "        if not self.ctx.astrbot_config[\"provider_settings\"][\"enable\"]:",
            "            logger.debug(\" LLM \")",
            "            return",
            "",
            "        # LLM",
            "        if not SessionServiceManager.should_process_llm_request(event):",
            "            logger.debug(f\" {event.unified_msg_origin}  LLM\")",
            "            return",
            "",
            "        provider = self._select_provider(event)",
            "        if provider is None:",
            "            return",
            "        if not isinstance(provider, Provider):",
            "            logger.error(f\"({type(provider)}) LLM \")",
            "            return",
            "",
            "        streaming_response = self.streaming_response",
            "        if (enable_streaming := event.get_extra(\"enable_streaming\")) is not None:",
            "            streaming_response = bool(enable_streaming)",
            "",
            "        logger.debug(\"ready to request llm provider\")",
            "        async with session_lock_manager.acquire_lock(event.unified_msg_origin):",
            "            logger.debug(\"acquired session lock for llm request\")",
            "            if event.get_extra(\"provider_request\"):",
            "                req = event.get_extra(\"provider_request\")",
            "                assert isinstance(req, ProviderRequest), (",
            "                    \"provider_request  ProviderRequest \"",
            "                )",
            "",
            "                if req.conversation:",
            "                    req.contexts = json.loads(req.conversation.history)",
            "",
            "            else:",
            "                req = ProviderRequest(prompt=\"\", image_urls=[])",
            "                if sel_model := event.get_extra(\"selected_model\"):",
            "                    req.model = sel_model",
            "                if self.provider_wake_prefix and not event.message_str.startswith(",
            "                    self.provider_wake_prefix",
            "                ):",
            "                    return",
            "",
            "                req.prompt = event.message_str[len(self.provider_wake_prefix) :]",
            "                # func_tool selection  packages/astrbot ",
            "                # req.func_tool = self.ctx.plugin_manager.context.get_llm_tool_manager()",
            "                for comp in event.message_obj.message:",
            "                    if isinstance(comp, Image):",
            "                        image_path = await comp.convert_to_file_path()",
            "                        req.image_urls.append(image_path)",
            "",
            "                conversation = await self._get_session_conv(event)",
            "                req.conversation = conversation",
            "                req.contexts = json.loads(conversation.history)",
            "",
            "                event.set_extra(\"provider_request\", req)",
            "",
            "            if not req.prompt and not req.image_urls:",
            "                return",
            "",
            "            # apply knowledge base context",
            "            await self._apply_kb_context(event, req)",
            "",
            "            # call event hook",
            "            if await call_event_hook(event, EventType.OnLLMRequestEvent, req):",
            "                return",
            "",
            "            # fix contexts json str",
            "            if isinstance(req.contexts, str):",
            "                req.contexts = json.loads(req.contexts)",
            "",
            "            # truncate contexts to fit max length",
            "            req.contexts = self._truncate_contexts(req.contexts)",
            "",
            "            # session_id",
            "            if not req.session_id:",
            "                req.session_id = event.unified_msg_origin",
            "",
            "            # fix messages",
            "            req.contexts = self._fix_messages(req.contexts)",
            "",
            "            # check provider modalities, if provider does not support image/tool_use, clear them in request.",
            "            self._modalities_fix(provider, req)",
            "",
            "            # filter tools, only keep tools from this pipeline's selected plugins",
            "            self._plugin_tool_fix(event, req)",
            "",
            "            stream_to_general = (",
            "                self.unsupported_streaming_strategy == \"turn_off\"",
            "                and not event.platform_meta.support_streaming_message",
            "            )",
            "            #  req.contexts",
            "            backup_contexts = copy.deepcopy(req.contexts)",
            "",
            "            # run agent",
            "            agent_runner = AgentRunner()",
            "            logger.debug(",
            "                f\"handle provider[id: {provider.provider_config['id']}] request: {req}\",",
            "            )",
            "            astr_agent_ctx = AstrAgentContext(",
            "                provider=provider,",
            "                first_provider_request=req,",
            "                curr_provider_request=req,",
            "                streaming=streaming_response,",
            "                event=event,",
            "            )",
            "            await agent_runner.reset(",
            "                provider=provider,",
            "                request=req,",
            "                run_context=AgentContextWrapper(",
            "                    context=astr_agent_ctx,",
            "                    tool_call_timeout=self.tool_call_timeout,",
            "                ),",
            "                tool_executor=FunctionToolExecutor(),",
            "                agent_hooks=MAIN_AGENT_HOOKS,",
            "                streaming=streaming_response,",
            "            )",
            "",
            "            if streaming_response and not stream_to_general:",
            "                # ",
            "                event.set_result(",
            "                    MessageEventResult()",
            "                    .set_result_content_type(ResultContentType.STREAMING_RESULT)",
            "                    .set_async_stream(",
            "                        run_agent(agent_runner, self.max_step, self.show_tool_use),",
            "                    ),",
            "                )",
            "                yield",
            "                if agent_runner.done():",
            "                    if final_llm_resp := agent_runner.get_final_llm_resp():",
            "                        if final_llm_resp.completion_text:",
            "                            chain = (",
            "                                MessageChain()",
            "                                .message(final_llm_resp.completion_text)",
            "                                .chain",
            "                            )",
            "                        elif final_llm_resp.result_chain:",
            "                            chain = final_llm_resp.result_chain.chain",
            "                        else:",
            "                            chain = MessageChain().chain",
            "                        event.set_result(",
            "                            MessageEventResult(",
            "                                chain=chain,",
            "                                result_content_type=ResultContentType.STREAMING_FINISH,",
            "                            ),",
            "                        )",
            "            else:",
            "                async for _ in run_agent(",
            "                    agent_runner, self.max_step, self.show_tool_use, stream_to_general",
            "                ):",
            "                    yield",
            "",
            "            #  contexts",
            "            req.contexts = backup_contexts",
            "",
            "            await self._save_to_history(event, req, agent_runner.get_final_llm_resp())",
            "",
            "        #  WebChat ",
            "        if event.get_platform_name() == \"webchat\":",
            "            asyncio.create_task(self._handle_webchat(event, req, provider))",
            "",
            "        asyncio.create_task(",
            "            Metric.upload(",
            "                llm_tick=1,",
            "                model_name=agent_runner.provider.get_model(),",
            "                provider_type=agent_runner.provider.meta().type,",
            "            ),",
            "        )"
          ]
        },
        "vulnerable_code": {
          "context": "astrbot/core/pipeline/process_stage/method/llm_request.py",
          "class": null,
          "func": null,
          "lines": [
            "            if \"call\" in ty.__dict__ and ty.__dict__[\"call\"] is not FunctionTool.call:",
            "                logger.debug(f\"Found call in: {ty}\")",
            "                is_override_call = True",
            "",
            "    async def process(",
            "    async def _apply_kb_context(",
            "        event: AstrMessageEvent,",
            "        _nested: bool = False,",
            "    ) -> None | AsyncGenerator[None, None]:",
            "        req: ProviderRequest | None = None",
            "",
            "        if not self.ctx.astrbot_config[\"provider_settings\"][\"enable\"]:",
            "            logger.debug(\" LLM \")",
            "            return",
            "",
            "        # LLM",
            "        if not SessionServiceManager.should_process_llm_request(event):",
            "            logger.debug(f\" {event.unified_msg_origin}  LLM\")",
            "            return",
            "",
            "        provider = self._select_provider(event)",
            "        if provider is None:",
            "            return",
            "        if not isinstance(provider, Provider):",
            "            logger.error(f\"({type(provider)}) LLM \")",
            "            return",
            "",
            "        streaming_response = self.streaming_response",
            "        if (enable_streaming := event.get_extra(\"enable_streaming\")) is not None:",
            "            streaming_response = bool(enable_streaming)",
            "",
            "        if event.get_extra(\"provider_request\"):",
            "            req = event.get_extra(\"provider_request\")",
            "            assert isinstance(req, ProviderRequest), (",
            "                \"provider_request  ProviderRequest \"",
            "            )",
            "",
            "            if req.conversation:",
            "                req.contexts = json.loads(req.conversation.history)",
            "",
            "        else:",
            "            req = ProviderRequest(prompt=\"\", image_urls=[])",
            "            if sel_model := event.get_extra(\"selected_model\"):",
            "                req.model = sel_model",
            "            if self.provider_wake_prefix:",
            "                if not event.message_str.startswith(self.provider_wake_prefix):",
            "                    return",
            "            req.prompt = event.message_str[len(self.provider_wake_prefix) :]",
            "            # func_tool selection  packages/astrbot ",
            "            # req.func_tool = self.ctx.plugin_manager.context.get_llm_tool_manager()",
            "            for comp in event.message_obj.message:",
            "                if isinstance(comp, Image):",
            "                    image_path = await comp.convert_to_file_path()",
            "                    req.image_urls.append(image_path)",
            "",
            "            conversation = await self._get_session_conv(event)",
            "            req.conversation = conversation",
            "            req.contexts = json.loads(conversation.history)",
            "",
            "            event.set_extra(\"provider_request\", req)",
            "",
            "        if not req.prompt and not req.image_urls:",
            "            return",
            "",
            "        # ",
            "        req: ProviderRequest,",
            "",
            "        #  LLM ",
            "        if await call_event_hook(event, EventType.OnLLMRequestEvent, req):",
            "            return",
            "    def _truncate_contexts(",
            "",
            "        if isinstance(req.contexts, str):",
            "            req.contexts = json.loads(req.contexts)",
            "        if len(contexts) // 2 <= self.max_context_length:",
            "",
            "        # max context length",
            "        if (",
            "            self.max_context_length != -1  # -1 ",
            "            and len(req.contexts) // 2 > self.max_context_length",
            "        ):",
            "            logger.debug(\"\")",
            "            req.contexts = req.contexts[",
            "                -(self.max_context_length - self.dequeue_context_length + 1) * 2 :",
            "            ]",
            "            # role  user ",
            "            index = next(",
            "                (",
            "                    i",
            "                    for i, item in enumerate(req.contexts)",
            "                    if item.get(\"role\") == \"user\"",
            "                ),",
            "                None,",
            "            )",
            "            if index is not None and index > 0:",
            "                req.contexts = req.contexts[index:]",
            "",
            "        # session_id",
            "        if not req.session_id:",
            "            req.session_id = event.unified_msg_origin",
            "        truncated_contexts = contexts[",
            "",
            "        # fix messages",
            "        req.contexts = self.fix_messages(req.contexts)",
            "        return truncated_contexts",
            "",
            "        # check provider modalities",
            "        # //",
            "    def _modalities_fix(",
            "                req.func_tool = None",
            "        # ",
            "",
            "",
            "        stream_to_general = (",
            "            self.unsupported_streaming_strategy == \"turn_off\"",
            "            and not event.platform_meta.support_streaming_message",
            "        )",
            "        #  req.contexts",
            "        backup_contexts = copy.deepcopy(req.contexts)",
            "",
            "        # run agent",
            "        agent_runner = AgentRunner()",
            "        logger.debug(",
            "            f\"handle provider[id: {provider.provider_config['id']}] request: {req}\",",
            "        )",
            "        astr_agent_ctx = AstrAgentContext(",
            "            provider=provider,",
            "            first_provider_request=req,",
            "            curr_provider_request=req,",
            "            streaming=streaming_response,",
            "            event=event,",
            "        )",
            "        await agent_runner.reset(",
            "            provider=provider,",
            "            request=req,",
            "            run_context=AgentContextWrapper(",
            "                context=astr_agent_ctx,",
            "                tool_call_timeout=self.tool_call_timeout,",
            "            ),",
            "            tool_executor=FunctionToolExecutor(),",
            "            agent_hooks=MAIN_AGENT_HOOKS,",
            "            streaming=streaming_response,",
            "        )",
            "",
            "        if streaming_response and not stream_to_general:",
            "            # ",
            "            event.set_result(",
            "                MessageEventResult()",
            "                .set_result_content_type(ResultContentType.STREAMING_RESULT)",
            "                .set_async_stream(",
            "                    run_agent(agent_runner, self.max_step, self.show_tool_use),",
            "                ),",
            "            )",
            "            yield",
            "            if agent_runner.done():",
            "                if final_llm_resp := agent_runner.get_final_llm_resp():",
            "                    if final_llm_resp.completion_text:",
            "                        chain = (",
            "                            MessageChain().message(final_llm_resp.completion_text).chain",
            "                        )",
            "                    elif final_llm_resp.result_chain:",
            "                        chain = final_llm_resp.result_chain.chain",
            "                    else:",
            "                        chain = MessageChain().chain",
            "                    event.set_result(",
            "                        MessageEventResult(",
            "                            chain=chain,",
            "                            result_content_type=ResultContentType.STREAMING_FINISH,",
            "                        ),",
            "                    )",
            "        else:",
            "            async for _ in run_agent(",
            "                agent_runner, self.max_step, self.show_tool_use, stream_to_general",
            "            ):",
            "                yield",
            "",
            "        #  contexts",
            "        req.contexts = backup_contexts",
            "",
            "        await self._save_to_history(event, req, agent_runner.get_final_llm_resp())",
            "",
            "        #  WebChat ",
            "        if event.get_platform_name() == \"webchat\":",
            "            asyncio.create_task(self._handle_webchat(event, req, provider))",
            "",
            "        asyncio.create_task(",
            "            Metric.upload(",
            "                llm_tick=1,",
            "                model_name=agent_runner.provider.get_model(),",
            "                provider_type=agent_runner.provider.meta().type,",
            "            ),",
            "        )",
            "",
            "    async def _handle_webchat(",
            "            if llm_resp and llm_resp.completion_text:",
            "                logger.debug(",
            "                    f\"WebChat : {llm_resp.completion_text.strip()}\",",
            "                )",
            "                title = llm_resp.completion_text.strip()",
            "",
            "    def fix_messages(self, messages: list[dict]) -> list[dict]:",
            "    def _fix_messages(self, messages: list[dict]) -> list[dict]:"
          ]
        },
        "source": "github",
        "commit_url": "https://github.com/AstrBotDevs/AstrBot/commit/395786187881f5cc56cb4e58addb271627ed4419",
        "CWE": "CWE-663",
        "other_CWEs": [],
        "other_CWDs": [],
        "issue_url": "https://github.com/AstrBotDevs/AstrBot/pull/3607"
      }
    ]
  }
}